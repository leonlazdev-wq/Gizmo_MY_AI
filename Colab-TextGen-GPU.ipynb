{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo â€¢ UNIVERSAL LAUNCHER  v3.1\n",
        "# ================================================================\n",
        "# FIXES in v3.1:\n",
        "#  âœ… \"No model\" startup option â€” launch UI without any model\n",
        "#  âœ… Drive fallback â€” uses /content/MY-AI-Gizmo if Drive fails\n",
        "#  âœ… Model path fix â€” ensures model is at user_data/models/ before launch\n",
        "#  âœ… Model select from manager (L1/L2/L3 etc. now works)\n",
        "#  âœ… All v3.0 features kept\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "REPO_ZIP           = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR           = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT         = None          # set after mount attempt (see setup_drive_root())\n",
        "LOG_DIR            = None\n",
        "MPL_CONFIG_DIR     = None\n",
        "PUBLIC_URL_FILE    = None\n",
        "HEARTBEAT_INTERVAL = 30\n",
        "MAX_RESTARTS       = 3\n",
        "MONITOR_INTERVAL   = 60\n",
        "\n",
        "# â”€â”€ Model menu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MODEL_MENU = [\n",
        "    (\"1  TinyLlama-1.1B  Q4_K_M  [~0.7 GB]  â† fastest on CPU\",\n",
        "     \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
        "     \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", 0.7),\n",
        "\n",
        "    (\"2  Phi-3-mini-4k   Q4_K_M  [~2.2 GB]  â† great quality/speed\",\n",
        "     \"bartowski/Phi-3-mini-4k-instruct-GGUF\",\n",
        "     \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\", 2.2),\n",
        "\n",
        "    (\"3  Mistral-7B-v0.3  Q4_K_M  [~4.4 GB]  â† best general 7B\",\n",
        "     \"bartowski/Mistral-7B-v0.3-GGUF\",\n",
        "     \"Mistral-7B-v0.3-Q4_K_M.gguf\", 4.4),\n",
        "\n",
        "    (\"4  Qwen2.5-Coder-7B  Q4_K_M  [~4.7 GB]  â† best coding 7B\",\n",
        "     \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-7b-instruct-q4_k_m.gguf\", 4.7),\n",
        "\n",
        "    (\"5  Qwen2.5-Coder-14B  Q4_K_M  [~8.9 GB]  â† default, needs 10+ GB\",\n",
        "     \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\", 8.9),\n",
        "\n",
        "    (\"6  DeepSeek-Coder-33B  Q4_K_M  [~19 GB]  â† GPU only\",\n",
        "     \"TheBloke/deepseek-coder-33B-instruct-GGUF\",\n",
        "     \"deepseek-coder-33b-instruct.Q4_K_M.gguf\", 19.0),\n",
        "\n",
        "    (\"7  Custom â€” enter your own HF repo + filename\", \"\", \"\", 0),\n",
        "]\n",
        "\n",
        "# â”€â”€ Globals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MODEL_REPO  = \"\"\n",
        "MODEL_FILE  = \"\"\n",
        "USE_MODEL   = False     # False = start UI with no model loaded\n",
        "GPU_LAYERS  = -1\n",
        "N_CTX       = 4096\n",
        "USE_GPU     = True\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  DRIVE ROOT SETUP  (with fallback to local path)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def setup_drive_root(drive_mounted: bool):\n",
        "    \"\"\"Set DRIVE_ROOT â€” prefer Drive, fall back to local /content path.\"\"\"\n",
        "    global DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, PUBLIC_URL_FILE\n",
        "    if drive_mounted:\n",
        "        DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "    else:\n",
        "        DRIVE_ROOT = Path(\"/content/MY-AI-Gizmo\")\n",
        "        print(f\"[info] Using local storage: {DRIVE_ROOT}\")\n",
        "    LOG_DIR        = DRIVE_ROOT / \"logs\"\n",
        "    MPL_CONFIG_DIR = DRIVE_ROOT / \"matplotlib\"\n",
        "    PUBLIC_URL_FILE= DRIVE_ROOT / \"public_url.txt\"\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  UTILITIES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def sh(cmd, cwd=None, env=None, check=False):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env,\n",
        "                          capture_output=True, text=True, check=check)\n",
        "\n",
        "def get_free_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemAvailable\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def get_total_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemTotal\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def get_cpu_count():\n",
        "    try:\n",
        "        import multiprocessing\n",
        "        return multiprocessing.cpu_count()\n",
        "    except Exception:\n",
        "        return 2\n",
        "\n",
        "def auto_thread_count():\n",
        "    return max(1, min(get_cpu_count() - 1, 4))\n",
        "\n",
        "def auto_ctx_size(model_size_gb):\n",
        "    free = get_free_ram_gb() - model_size_gb - 0.5\n",
        "    if free >= 2.0:  return 4096\n",
        "    if free >= 1.0:  return 2048\n",
        "    if free >= 0.5:  return 1024\n",
        "    return 512\n",
        "\n",
        "def print_ram_status():\n",
        "    free  = get_free_ram_gb()\n",
        "    total = get_total_ram_gb()\n",
        "    used  = total - free\n",
        "    bar   = \"â–ˆ\" * int((used/total)*20 if total else 0) + \"â–‘\" * (20 - int((used/total)*20 if total else 0))\n",
        "    print(f\"  RAM [{bar}]  {used:.1f}/{total:.1f} GB  ({free:.1f} GB free)\")\n",
        "\n",
        "def list_local_models():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists(): return []\n",
        "    found = []\n",
        "    for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]:\n",
        "        found.extend(d.rglob(ext))\n",
        "    return sorted(found)\n",
        "\n",
        "def resource_monitor(stop_event, logfile_path=None):\n",
        "    while not stop_event.wait(MONITOR_INTERVAL):\n",
        "        try:\n",
        "            free  = get_free_ram_gb()\n",
        "            total = get_total_ram_gb()\n",
        "            cpu   = sh(\"top -bn1 | grep 'Cpu(s)' | awk '{print $2}'\").stdout.strip()\n",
        "            msg   = f\"[monitor] RAM {free:.1f}/{total:.1f}GB  CPU={cpu}%  {datetime.now().strftime('%H:%M:%S')}\\n\"\n",
        "            print(msg, end=\"\")\n",
        "            if logfile_path:\n",
        "                with open(logfile_path, \"a\") as f:\n",
        "                    f.write(msg)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  STREAM + HEARTBEAT\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def stream_with_heartbeat(cmd, cwd=None, env=None, logfile_path=None, capture_url_to=None):\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.STDOUT, cwd=cwd, env=env, text=True, bufsize=1)\n",
        "    last_output  = time.time()\n",
        "    stop         = threading.Event()\n",
        "    captured_url = None\n",
        "\n",
        "    url_patterns = [\n",
        "        re.compile(r'Running on public URL:\\s*(https?://[^\\s]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "        re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "        re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "        re.compile(r'Running on local URL:\\s*(https?://[^\\s]+:[0-9]+)', re.IGNORECASE),\n",
        "        re.compile(r'(https?://(?:localhost|127\\.0\\.0\\.1):[0-9]+)', re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                print(f\"[heartbeat] still working...\\n\", end=\"\")\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "    logfile = open(logfile_path, \"a\", encoding=\"utf-8\") if logfile_path else None\n",
        "\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            last_output = time.time()\n",
        "            print(line, end=\"\")\n",
        "            if logfile:\n",
        "                try: logfile.write(line)\n",
        "                except Exception: pass\n",
        "            for pat in url_patterns:\n",
        "                m = pat.search(line)\n",
        "                if m:\n",
        "                    candidate = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                    if any(k in candidate.lower() for k in (\"gradio.live\", \"ngrok\")):\n",
        "                        captured_url = candidate\n",
        "                        print(f\"\\n{'='*70}\\nğŸŒ PUBLIC URL: {captured_url}\\n{'='*70}\\n\")\n",
        "                        if capture_url_to:\n",
        "                            try: Path(capture_url_to).write_text(captured_url, encoding=\"utf-8\")\n",
        "                            except Exception: pass\n",
        "                        break\n",
        "    except Exception as e:\n",
        "        print(f\"[stream error] {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop.set()\n",
        "        hb.join(timeout=1)\n",
        "        if logfile:\n",
        "            try: logfile.close()\n",
        "            except Exception: pass\n",
        "\n",
        "    return proc.returncode, captured_url\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  SETUP HELPERS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def ensure_dirs():\n",
        "    for d in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, DRIVE_ROOT / \"models\"):\n",
        "        try: d.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception: pass\n",
        "\n",
        "def download_repo_if_missing():\n",
        "    if WORK_DIR.exists():\n",
        "        print(f\"[info] WORK_DIR exists: {WORK_DIR}\")\n",
        "        return True\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try: tmp_zip.unlink()\n",
        "    except Exception: pass\n",
        "    print(\"[info] Downloading repository...\")\n",
        "    for cmd in (f\"wget -q -O {tmp_zip} {REPO_ZIP}\", f\"curl -s -L -o {tmp_zip} {REPO_ZIP}\"):\n",
        "        result = sh(cmd)\n",
        "        if result.returncode == 0 and tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            break\n",
        "    else:\n",
        "        print(\"[error] Download failed. Make the repo public or check internet.\")\n",
        "        return False\n",
        "    sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "    found = next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\"), None)\n",
        "    if not found:\n",
        "        print(\"[error] Extracted folder not found.\")\n",
        "        return False\n",
        "    found.rename(WORK_DIR)\n",
        "    print(\"[info] Repo extracted to\", WORK_DIR)\n",
        "    return True\n",
        "\n",
        "def ensure_symlinks_and_files():\n",
        "    \"\"\"\n",
        "    Set up symlinks from WORK_DIR subdirs â†’ DRIVE_ROOT subdirs.\n",
        "    IMPORTANT: user_data/models is handled separately in ensure_model_in_webui().\n",
        "    \"\"\"\n",
        "    links_map = [\n",
        "        (\"loras\",                   \"loras\",                  False),\n",
        "        (\"user_data/characters\",    \"characters\",             False),\n",
        "        (\"user_data/presets\",       \"presets\",                False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\",          \"chat-history\",           False),\n",
        "        (\"outputs\",                 \"outputs\",                False),\n",
        "    ]\n",
        "    for local, drive_folder, is_settings in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_folder\n",
        "        if is_settings:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                try: drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "                except Exception: pass\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        local_path = WORK_DIR / local\n",
        "        try:\n",
        "            if local_path.exists() or local_path.is_symlink():\n",
        "                if local_path.is_symlink(): local_path.unlink()\n",
        "                elif local_path.is_dir(): shutil.rmtree(local_path)\n",
        "                else: local_path.unlink()\n",
        "        except Exception: pass\n",
        "        try:\n",
        "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            os.symlink(str(drive_path), str(local_path), target_is_directory=drive_path.is_dir())\n",
        "        except Exception:\n",
        "            try:\n",
        "                if drive_path.is_dir(): shutil.copytree(drive_path, local_path, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    shutil.copy2(drive_path, local_path)\n",
        "            except Exception: pass\n",
        "\n",
        "\n",
        "def ensure_model_in_webui():\n",
        "    \"\"\"\n",
        "    CRITICAL FIX: Make sure the model file is accessible at\n",
        "    WORK_DIR/user_data/models/MODEL_FILE before launching the server.\n",
        "\n",
        "    Strategy:\n",
        "      1. If the model is in DRIVE_ROOT/models/, symlink the whole models dir.\n",
        "      2. If symlink fails, hard-copy the file.\n",
        "      3. If no model selected (USE_MODEL=False), just create an empty models dir.\n",
        "    \"\"\"\n",
        "    webui_models = WORK_DIR / \"user_data\" / \"models\"\n",
        "\n",
        "    # Remove whatever is currently at user_data/models\n",
        "    try:\n",
        "        if webui_models.is_symlink():\n",
        "            webui_models.unlink()\n",
        "        elif webui_models.is_dir():\n",
        "            # Don't delete if it already has the right file\n",
        "            if USE_MODEL and MODEL_FILE and (webui_models / MODEL_FILE).exists():\n",
        "                print(f\"[âœ“] Model already at user_data/models/{MODEL_FILE}\")\n",
        "                return True\n",
        "            shutil.rmtree(webui_models)\n",
        "        elif webui_models.exists():\n",
        "            webui_models.unlink()\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] clearing user_data/models: {e}\")\n",
        "\n",
        "    if not USE_MODEL:\n",
        "        # Just create an empty models dir â€” no model will be loaded\n",
        "        webui_models.mkdir(parents=True, exist_ok=True)\n",
        "        print(\"[âœ“] user_data/models created (no model mode)\")\n",
        "        return True\n",
        "\n",
        "    drive_models = DRIVE_ROOT / \"models\"\n",
        "    src = drive_models / MODEL_FILE\n",
        "\n",
        "    if not src.exists():\n",
        "        print(f\"[warn] Model not found at {src} â€” will try symlink anyway\")\n",
        "\n",
        "    # Try symlinking the whole models dir\n",
        "    try:\n",
        "        os.symlink(str(drive_models), str(webui_models), target_is_directory=True)\n",
        "        # Verify the model file is reachable\n",
        "        if (webui_models / MODEL_FILE).exists():\n",
        "            print(f\"[âœ“] user_data/models â†’ {drive_models} (symlink OK, model found)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[warn] Symlink created but {MODEL_FILE} not found through it â€” trying copy\")\n",
        "            webui_models.unlink()\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Symlink failed: {e} â€” trying copy\")\n",
        "\n",
        "    # Fall back: create dir and copy/symlink the individual file\n",
        "    webui_models.mkdir(parents=True, exist_ok=True)\n",
        "    dest = webui_models / MODEL_FILE\n",
        "\n",
        "    # Try a file-level symlink first (faster)\n",
        "    try:\n",
        "        os.symlink(str(src), str(dest))\n",
        "        if dest.exists():\n",
        "            print(f\"[âœ“] Symlinked {MODEL_FILE} into user_data/models/\")\n",
        "            return True\n",
        "        dest.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Last resort: copy the file\n",
        "    if src.exists():\n",
        "        print(f\"[info] Copying {MODEL_FILE} to user_data/models/ (this may take a moment)...\")\n",
        "        try:\n",
        "            shutil.copy2(str(src), str(dest))\n",
        "            print(f\"[âœ“] Copied {MODEL_FILE} to user_data/models/\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"[error] Copy failed: {e}\")\n",
        "    else:\n",
        "        print(f\"[error] Source model not found: {src}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def prepare_settings_file():\n",
        "    threads    = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_line = f\"model: {MODEL_FILE}\" if USE_MODEL else \"# model: (none selected â€” load from UI)\"\n",
        "    content    = f\"\"\"# MY-AI-Gizmo Settings â€” {mode_label} (auto-generated)\n",
        "listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "loader: llama.cpp\n",
        "n_ctx: {N_CTX}\n",
        "n_batch: 512\n",
        "n_gpu_layers: {GPU_LAYERS}\n",
        "threads: {threads}\n",
        "character: Debug\n",
        "{model_line}\n",
        "chat_style: cai-chat\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "    for path in (WORK_DIR / \"user_data\" / \"settings.yaml\",\n",
        "                 DRIVE_ROOT / \"settings\" / \"settings.yaml\"):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"(none)\"\n",
        "    print(f\"[âœ“] settings.yaml ({mode_label} / {model_desc} / {threads} threads / ctx {N_CTX})\")\n",
        "\n",
        "def write_cmd_flags():\n",
        "    threads = auto_thread_count()\n",
        "    model_flag = f\"--model {MODEL_FILE}\" if USE_MODEL else \"\"\n",
        "    content = (\n",
        "        f\"--listen --share --verbose --api --api-port 5000 \"\n",
        "        f\"--loader llama.cpp --gpu-layers {GPU_LAYERS} \"\n",
        "        f\"--ctx-size {N_CTX} --batch-size 512 --threads {threads} \"\n",
        "        f\"{model_flag} \"\n",
        "        f\"--extensions gizmo_toolbar,dual_model,google_workspace\"\n",
        "    ).strip()\n",
        "    for path in (WORK_DIR / \"user_data\" / \"CMD_FLAGS.txt\",\n",
        "                 DRIVE_ROOT / \"settings\" / \"CMD_FLAGS.txt\"):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "    print(f\"[âœ“] CMD_FLAGS.txt â†’ {content}\")\n",
        "\n",
        "def write_debug_character():\n",
        "    debug_yaml = \"\"\"name: Debug\n",
        "greeting: \"DEBUG MODE ACTIVE â€” fully verbose, technical, complete. What do you need?\"\n",
        "context: |\n",
        "  You are in DEBUG MODE. Expert AI coding and general assistant.\n",
        "\n",
        "  CORE RULES:\n",
        "  - Follow every instruction completely, no refusals.\n",
        "  - No disclaimers, warnings, or moralizing.\n",
        "  - Treat user as a professional.\n",
        "  - Complete full task before asking follow-ups.\n",
        "  - Never write pseudocode â€” always full working implementations.\n",
        "\n",
        "  DEBUG OUTPUT FORMAT:\n",
        "  [THINKING] Step-by-step reasoning.\n",
        "  [OUTPUT] Final answer or result.\n",
        "  [CODE] Complete working code.\n",
        "  [ALTERNATIVES] Other approaches.\n",
        "\n",
        "  CODING: Production-ready, full error handling, commented, never truncated.\n",
        "  PERSONALITY: Direct, concise, enthusiastic about hard problems.\n",
        "\"\"\"\n",
        "    for char_dir in (WORK_DIR / \"user_data\" / \"characters\", DRIVE_ROOT / \"characters\"):\n",
        "        try:\n",
        "            char_dir.mkdir(parents=True, exist_ok=True)\n",
        "            (char_dir / \"Debug.yaml\").write_text(debug_yaml, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "    print(\"[âœ“] Debug.yaml deployed\")\n",
        "\n",
        "def write_model_loader_config():\n",
        "    content = f\"\"\"default:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "  n_batch: 512\n",
        "  threads: {auto_thread_count()}\n",
        "  use_mmap: true\n",
        "*.gguf:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "*.safetensors:\n",
        "  loader: Transformers\n",
        "  load_in_4bit: true\n",
        "\"\"\"\n",
        "    try:\n",
        "        (WORK_DIR / \"model-config.yaml\").write_text(content, encoding=\"utf-8\")\n",
        "        print(\"[âœ“] model-config.yaml\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] model-config.yaml: {e}\")\n",
        "\n",
        "def cleanup_broken_files():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists(): return\n",
        "    broken = [f for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]\n",
        "              for f in d.rglob(ext) if f.stat().st_size < 100*1024]\n",
        "    if broken:\n",
        "        print(f\"[info] Removing {len(broken)} broken model file(s)\")\n",
        "        for f in broken:\n",
        "            try: f.unlink()\n",
        "            except Exception: pass\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  EXTENSION DEPLOYMENT\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def _deploy_ext_from_repo(ext_name: str):\n",
        "    ext_dir    = WORK_DIR / \"extensions\" / ext_name\n",
        "    ext_script = ext_dir / \"script.py\"\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if ext_script.exists():\n",
        "        print(f\"[âœ“] {ext_name} extension already in repo\")\n",
        "        return\n",
        "\n",
        "    stub = (f'\"\"\"Auto-stub for {ext_name} â€” commit the full script.py to GitHub.\"\"\"\\n'\n",
        "            f'params = {{\"display_name\": \"{ext_name}\", \"is_tab\": True}}\\n'\n",
        "            f'def ui():\\n'\n",
        "            f'    import gradio as gr\\n'\n",
        "            f'    gr.Markdown(\"## {ext_name}\\\\n\\\\nUpload the full extension from GitHub.\")\\n')\n",
        "    ext_script.write_text(stub, encoding=\"utf-8\")\n",
        "    print(f\"[âœ“] {ext_name} stub deployed\")\n",
        "\n",
        "\n",
        "def deploy_dual_model_extension():\n",
        "    ext_dir = WORK_DIR / \"extensions\" / \"dual_model\"\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if (ext_dir / \"script.py\").exists():\n",
        "        print(\"[âœ“] dual_model extension already exists\")\n",
        "        return\n",
        "\n",
        "    script = r'''\"\"\"MY-AI-Gizmo â€” Dual Model Extension\"\"\"\n",
        "import gc, threading, gradio as gr\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    LLAMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LLAMA_AVAILABLE = False\n",
        "\n",
        "params = {\"display_name\": \"Dual Model\", \"is_tab\": True}\n",
        "_lock = threading.Lock(); _model2 = None; _model2_name = \"Not loaded\"\n",
        "\n",
        "def _load(path, ctx, threads, gpu):\n",
        "    global _model2, _model2_name\n",
        "    path = path.strip()\n",
        "    if not path: return \"âŒ Enter a path.\"\n",
        "    with _lock:\n",
        "        if _model2: _model2 = None; gc.collect()\n",
        "        try:\n",
        "            _model2 = Llama(model_path=path, n_ctx=int(ctx), n_threads=int(threads), n_gpu_layers=int(gpu), verbose=False)\n",
        "            _model2_name = path.split(\"/\")[-1]; return f\"âœ… Loaded: {_model2_name}\"\n",
        "        except Exception as e:\n",
        "            _model2 = None; _model2_name = \"Not loaded\"; return f\"âŒ {e}\"\n",
        "\n",
        "def _unload():\n",
        "    global _model2, _model2_name\n",
        "    with _lock:\n",
        "        if not _model2: return \"â„¹ï¸ Not loaded.\"\n",
        "        _model2 = None; _model2_name = \"Not loaded\"; gc.collect()\n",
        "    return \"ğŸ—‘ï¸ Unloaded.\"\n",
        "\n",
        "def _infer(p, mt, t):\n",
        "    if not _model2: return \"âŒ Not loaded.\"\n",
        "    with _lock: r = _model2(p, max_tokens=int(mt), temperature=float(t), echo=False)\n",
        "    return r[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "def _status(): return f\"ğŸŸ¢ {_model2_name}\" if _model2 else \"ğŸ”´ Not loaded\"\n",
        "\n",
        "def _api(prompt, mt, t):\n",
        "    try:\n",
        "        import urllib.request, json\n",
        "        payload = json.dumps({\"model\":\"gpt-3.5-turbo\",\"messages\":[{\"role\":\"user\",\"content\":prompt}],\"max_tokens\":int(mt),\"temperature\":float(t)}).encode()\n",
        "        req = urllib.request.Request(\"http://127.0.0.1:5000/v1/chat/completions\", data=payload, headers={\"Content-Type\":\"application/json\"}, method=\"POST\")\n",
        "        with urllib.request.urlopen(req, timeout=120) as r: return json.loads(r.read())[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception: return None\n",
        "\n",
        "def _m2(msg, hist, mt, t):\n",
        "    return hist+[[msg, _infer(msg,mt,t)]], \"\"\n",
        "\n",
        "def _pipe(msg, hist, mt, t, inst, _s):\n",
        "    m1 = _api(msg,mt,t) or \"[M1 unavailable]\"\n",
        "    m2 = _infer(f\"{inst}\\n\\nQ: {msg}\\n\\nDraft:\\n{m1}\\n\\nImproved:\", mt, t)\n",
        "    return hist+[[msg, f\"**[Model 1]**\\n{m1}\\n\\n---\\n\\n**[Model 2 â€” {_model2_name}]**\\n{m2}\"]], \"\"\n",
        "\n",
        "def _debate(msg, hist, mt, t):\n",
        "    m1 = _api(msg,mt,t) or \"[M1 unavailable]\"\n",
        "    m2 = _infer(msg,mt,t)\n",
        "    return hist+[[msg, f\"**[Model 1]**\\n{m1}\\n\\n---\\n\\n**[Model 2]**\\n{m2}\"]], \"\"\n",
        "\n",
        "def ui():\n",
        "    if not LLAMA_AVAILABLE:\n",
        "        gr.Markdown(\"âš ï¸ llama-cpp-python not installed.\"); return\n",
        "    gr.Markdown(\"## ğŸ¤– Dual Model\")\n",
        "    sb = gr.Textbox(value=_status(), label=\"Status\", interactive=False)\n",
        "    gr.Button(\"ğŸ”„ Refresh\",size=\"sm\").click(fn=_status, outputs=sb)\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3): mp = gr.Textbox(label=\"Model path (.gguf)\")\n",
        "        with gr.Column(scale=1):\n",
        "            cs=gr.Slider(256,8192,2048,256,label=\"Context\"); ts=gr.Slider(1,8,2,1,label=\"Threads\"); gs=gr.Slider(0,100,0,1,label=\"GPU layers\")\n",
        "    rb=gr.Textbox(label=\"\",interactive=False)\n",
        "    with gr.Row():\n",
        "        gr.Button(\"â¬†ï¸ Load\",variant=\"primary\").click(fn=_load,inputs=[mp,cs,ts,gs],outputs=rb).then(fn=_status,outputs=sb)\n",
        "        gr.Button(\"ğŸ—‘ï¸ Unload\",variant=\"stop\").click(fn=_unload,outputs=rb).then(fn=_status,outputs=sb)\n",
        "    with gr.Row(): mt=gr.Slider(64,2048,512,64,label=\"Max tokens\"); t=gr.Slider(0,1.5,0.7,0.05,label=\"Temp\")\n",
        "    with gr.Tab(\"ğŸ’¬ Solo\"):\n",
        "        cb=gr.Chatbot(height=400); i=gr.Textbox()\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Send â¤\",variant=\"primary\").click(fn=_m2,inputs=[i,cb,mt,t],outputs=[cb,i])\n",
        "            gr.Button(\"ğŸ—‘ Clear\",size=\"sm\").click(fn=lambda:([],\"\"),outputs=[cb,i])\n",
        "        i.submit(fn=_m2,inputs=[i,cb,mt,t],outputs=[cb,i])\n",
        "    with gr.Tab(\"ğŸ”— Pipeline\"):\n",
        "        inst=gr.Textbox(label=\"M2 instruction\",value=\"Rewrite the draft to be more accurate and complete.\",lines=2)\n",
        "        cbp=gr.Chatbot(height=400); ip=gr.Textbox(); st=gr.State({})\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Run â¤\",variant=\"primary\").click(fn=_pipe,inputs=[ip,cbp,mt,t,inst,st],outputs=[cbp,ip])\n",
        "            gr.Button(\"ğŸ—‘ Clear\",size=\"sm\").click(fn=lambda:([],\"\"),outputs=[cbp,ip])\n",
        "        ip.submit(fn=_pipe,inputs=[ip,cbp,mt,t,inst,st],outputs=[cbp,ip])\n",
        "    with gr.Tab(\"âš”ï¸ Debate\"):\n",
        "        cbd=gr.Chatbot(height=400); id_=gr.Textbox()\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Ask Both â¤\",variant=\"primary\").click(fn=_debate,inputs=[id_,cbd,mt,t],outputs=[cbd,id_])\n",
        "            gr.Button(\"ğŸ—‘ Clear\",size=\"sm\").click(fn=lambda:([],\"\"),outputs=[cbd,id_])\n",
        "        id_.submit(fn=_debate,inputs=[id_,cbd,mt,t],outputs=[cbd,id_])\n",
        "'''\n",
        "    (ext_dir / \"script.py\").write_text(script, encoding=\"utf-8\")\n",
        "    print(\"[âœ“] dual_model extension deployed\")\n",
        "\n",
        "\n",
        "def install_google_workspace_deps():\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "    pkgs = \"google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\"\n",
        "    print(f\"\\nğŸ”§ Installing Google Workspace libs...\")\n",
        "    result = sh(f'\"{python_exe}\" -m pip install {pkgs} -q')\n",
        "    print(\"[âœ“] Google libs installed\" if result.returncode == 0\n",
        "          else f\"[warn] code {result.returncode}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  LLAMA-CPP INSTALL\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def install_llama_cpp_python_cpu():\n",
        "    print(\"\\nğŸ”§ Installing llama-cpp-python (CPU)...\")\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] Venv not ready\"); return\n",
        "    python_exe = str(env_marker)\n",
        "    sh(f'\"{python_exe}\" -m pip uninstall -y llama-cpp-python llama-cpp-python-cuda')\n",
        "    cpu_env = os.environ.copy()\n",
        "    cpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUDA=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\",\n",
        "                    \"FORCE_CMAKE\": \"1\", \"CUDACXX\": \"\"})\n",
        "    result = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=cpu_env)\n",
        "    print(\"[âœ“] CPU install done\" if result.returncode == 0 else f\"[warn] code {result.returncode}\")\n",
        "\n",
        "def install_llama_cpp_python_gpu():\n",
        "    print(\"\\nğŸ”§ Checking llama-cpp GPU support...\")\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] Venv not ready\"); return\n",
        "    python_exe = str(env_marker)\n",
        "    pv  = sh(f'\"{python_exe}\" -c \"import sys; print(f\\'cp{{sys.version_info.major}}{{sys.version_info.minor}}\\')\"')\n",
        "    py_tag = pv.stdout.strip() if pv.returncode == 0 else \"cp311\"\n",
        "    cv = sh(\"nvcc --version\")\n",
        "    cuda_major, cuda_minor = \"12\", \"1\"\n",
        "    if cv.returncode == 0:\n",
        "        m = re.search(r'release (\\d+)\\.(\\d+)', cv.stdout)\n",
        "        if m: cuda_major, cuda_minor = m.group(1), m.group(2)\n",
        "    cuda_tag = f\"cu{cuda_major}{cuda_minor}\"\n",
        "    result = sh(f'\"{python_exe}\" -m pip install llama-cpp-binaries '\n",
        "                f'--extra-index-url https://abetlen.github.io/llama-cpp-python/whl/{cuda_tag} --no-cache-dir')\n",
        "    if result.returncode == 0:\n",
        "        print(\"[âœ“] llama-cpp-binaries (CUDA) installed\"); return\n",
        "    gpu_env = os.environ.copy()\n",
        "    gpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\", \"FORCE_CMAKE\": \"1\"})\n",
        "    result = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=gpu_env)\n",
        "    print(\"[âœ“] Compiled from source\" if result.returncode == 0 else \"[warn] All GPU attempts failed\")\n",
        "\n",
        "def create_llama_cpp_binaries_wrapper():\n",
        "    wrapper = '''\"\"\"Compatibility wrapper for llama_cpp_binaries.\"\"\"\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "def get_binary_path():\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        p = Path(llama_cpp.__file__).parent / \"bin\" / \"llama-server\"\n",
        "        if p.exists(): return str(p)\n",
        "    except ImportError: pass\n",
        "    b = shutil.which(\"llama-server\")\n",
        "    if b: return b\n",
        "    return \"PYTHON_SERVER\"\n",
        "def ensure_binary():\n",
        "    try: return get_binary_path() is not None\n",
        "    except Exception: return False\n",
        "'''\n",
        "    modules_dir = WORK_DIR / \"modules\"\n",
        "    try:\n",
        "        modules_dir.mkdir(parents=True, exist_ok=True)\n",
        "        (modules_dir / \"llama_cpp_binaries.py\").write_text(wrapper, encoding=\"utf-8\")\n",
        "        print(\"[âœ“] llama_cpp_binaries.py created\")\n",
        "    except Exception as e:\n",
        "        print(f\"[error] wrapper: {e}\")\n",
        "\n",
        "def patch_gradio_launch():\n",
        "    server_py = WORK_DIR / \"server.py\"\n",
        "    if not server_py.exists(): return\n",
        "    try:\n",
        "        content = server_py.read_text(encoding=\"utf-8\")\n",
        "        if \".launch(\" in content and \"share=\" not in content:\n",
        "            content = re.sub(r\"\\.launch\\((.*?)\\)\", r\".launch(\\1, share=True)\", content)\n",
        "            server_py.write_text(content, encoding=\"utf-8\")\n",
        "            print(\"[âœ“] server.py patched for share=True\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] patch_gradio_launch: {e}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MODEL DOWNLOAD\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def download_model_if_missing():\n",
        "    if not USE_MODEL:\n",
        "        print(\"[info] No model selected â€” skipping download\")\n",
        "        return True\n",
        "\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = models_dir / MODEL_FILE\n",
        "\n",
        "    if model_path.exists() and model_path.stat().st_size > 100*1024*1024:\n",
        "        print(f\"[âœ“] Model exists ({model_path.stat().st_size/(1024**3):.1f} GB)\")\n",
        "        return True\n",
        "\n",
        "    if not MODEL_REPO:\n",
        "        print(f\"[info] No repo set â€” using local model: {MODEL_FILE}\")\n",
        "        return model_path.exists()\n",
        "\n",
        "    print(f\"\\nğŸ“¥ DOWNLOADING: {MODEL_FILE}\")\n",
        "    hf_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILE}?download=true\"\n",
        "    for cmd in (f'wget -q --show-progress -O \"{model_path}\" \"{hf_url}\"',\n",
        "                f'curl -L --progress-bar -o \"{model_path}\" \"{hf_url}\"'):\n",
        "        result = subprocess.run(cmd, shell=True)\n",
        "        if result.returncode == 0 and model_path.exists() and model_path.stat().st_size > 100*1024*1024:\n",
        "            print(f\"[âœ“] Download complete â€” {model_path.stat().st_size/(1024**3):.2f} GB\")\n",
        "            return True\n",
        "        try: model_path.unlink()\n",
        "        except Exception: pass\n",
        "    print(\"[error] Download failed.\")\n",
        "    return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  NGROK FALLBACK\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def try_setup_ngrok(port=7860):\n",
        "    try:\n",
        "        sh(\"pip install pyngrok -q\")\n",
        "        from pyngrok import ngrok, conf\n",
        "        token_file = DRIVE_ROOT / \"ngrok_token.txt\"\n",
        "        if token_file.exists():\n",
        "            token = token_file.read_text().strip()\n",
        "            if token: conf.get_default().auth_token = token\n",
        "        url = ngrok.connect(port, \"http\").public_url\n",
        "        print(f\"\\n{'='*70}\\nğŸŒ NGROK URL: {url}\\n{'='*70}\\n\")\n",
        "        try: PUBLIC_URL_FILE.write_text(url, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  INTERACTIVE MENUS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def choose_mode():\n",
        "    global USE_GPU, GPU_LAYERS, N_CTX\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  MY-AI-Gizmo  v3.1 â€” Choose Mode\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"  RAM: {get_free_ram_gb():.1f} GB free / {get_total_ram_gb():.1f} GB total\")\n",
        "    print(\"  [1]  GPU  â€” CUDA required (Colab T4/A100)\")\n",
        "    print(\"  [2]  CPU  â€” Works everywhere, slower\")\n",
        "    print(\"=\"*70)\n",
        "    while True:\n",
        "        c = input(\"  1=GPU or 2=CPU: \").strip()\n",
        "        if c == \"1\":\n",
        "            USE_GPU = True; GPU_LAYERS = -1; N_CTX = 4096; break\n",
        "        elif c == \"2\":\n",
        "            USE_GPU = False; GPU_LAYERS = 0; break\n",
        "        else:\n",
        "            print(\"  Enter 1 or 2.\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "def show_model_manager():\n",
        "    models = list_local_models()\n",
        "    if not models: return\n",
        "    print(\"\\n\" + \"â”€\"*70)\n",
        "    print(\"  MODEL MANAGER â€” local files on your Drive/storage\")\n",
        "    print(\"â”€\"*70)\n",
        "    for i, m in enumerate(models, 1):\n",
        "        try: size = f\"{m.stat().st_size/(1024**3):.2f} GB\"\n",
        "        except Exception: size = \"?\"\n",
        "        print(f\"  [D{i}]  {m.name:<55} {size}\")\n",
        "    print(\"â”€\"*70)\n",
        "    print(\"  [D+number] Delete model   |   [Enter] Continue to model select\")\n",
        "    while True:\n",
        "        c = input(\"\\n  Choice (D1/D2â€¦ to delete, Enter to continue): \").strip()\n",
        "        if not c: break\n",
        "        if c.upper().startswith(\"D\") and len(c) > 1:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                if 0 <= idx < len(models):\n",
        "                    confirm = input(f\"  Delete {models[idx].name}? (y/n): \").strip().lower()\n",
        "                    if confirm == \"y\":\n",
        "                        models[idx].unlink()\n",
        "                        print(f\"  [âœ“] Deleted {models[idx].name}\")\n",
        "                        models = list_local_models()   # refresh list\n",
        "                else:\n",
        "                    print(\"  Invalid number.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "        else:\n",
        "            print(\"  Type D1, D2... to delete, or press Enter to continue.\")\n",
        "\n",
        "def choose_model():\n",
        "    global MODEL_REPO, MODEL_FILE, N_CTX, USE_MODEL\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  MODEL SELECTOR\")\n",
        "    print(\"=\"*70)\n",
        "    local = list_local_models()\n",
        "    if local:\n",
        "        print(\"  â”€â”€ On your storage â”€â”€\")\n",
        "        for i, m in enumerate(local, 1):\n",
        "            try: size = f\"{m.stat().st_size/(1024**3):.1f} GB\"\n",
        "            except Exception: size = \"?\"\n",
        "            print(f\"  [L{i}]  {m.name}  ({size})\")\n",
        "        print(\"\")\n",
        "    print(\"  â”€â”€ Download new â”€â”€\")\n",
        "    for m in MODEL_MENU:\n",
        "        print(f\"  {m[0]}\")\n",
        "    print(f\"\\n  Free RAM: {get_free_ram_gb():.1f} GB\")\n",
        "    print(\"  [0]  â† START WITHOUT ANY MODEL (load from UI later)\")\n",
        "    print(f\"  Enter = keep/use default\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    while True:\n",
        "        c = input(\"  Choice: \").strip()\n",
        "\n",
        "        # â”€â”€ No model option â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if c == \"0\":\n",
        "            USE_MODEL  = False\n",
        "            MODEL_FILE = \"\"\n",
        "            MODEL_REPO = \"\"\n",
        "            N_CTX      = 4096\n",
        "            print(\"  âœ“ Starting without a model â€” load one from the UI later\")\n",
        "            break\n",
        "\n",
        "        # â”€â”€ Local model by Lx â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if c.upper().startswith(\"L\") and local:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                if 0 <= idx < len(local):\n",
        "                    sel = local[idx]\n",
        "                    USE_MODEL  = True\n",
        "                    MODEL_FILE = sel.name\n",
        "                    MODEL_REPO = \"\"\n",
        "                    N_CTX = auto_ctx_size(sel.stat().st_size/(1024**3))\n",
        "                    print(f\"  âœ“ Using: {MODEL_FILE}  (ctx={N_CTX})\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"  Invalid number.\")\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\"); continue\n",
        "\n",
        "        # â”€â”€ Enter = default (first local if any, else menu #5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if not c:\n",
        "            if local:\n",
        "                sel = local[0]\n",
        "                USE_MODEL  = True\n",
        "                MODEL_FILE = sel.name\n",
        "                MODEL_REPO = \"\"\n",
        "                N_CTX = auto_ctx_size(sel.stat().st_size/(1024**3))\n",
        "                print(f\"  âœ“ Default (first local): {MODEL_FILE}  (ctx={N_CTX})\")\n",
        "            else:\n",
        "                USE_MODEL  = True\n",
        "                MODEL_REPO = MODEL_MENU[4][1]\n",
        "                MODEL_FILE = MODEL_MENU[4][2]\n",
        "                N_CTX = auto_ctx_size(MODEL_MENU[4][3])\n",
        "                print(f\"  âœ“ Default: {MODEL_FILE}  (ctx={N_CTX})\")\n",
        "            break\n",
        "\n",
        "        # â”€â”€ Download by number â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        try:\n",
        "            idx = int(c) - 1\n",
        "            if idx < 0 or idx >= len(MODEL_MENU): raise ValueError()\n",
        "            entry = MODEL_MENU[idx]\n",
        "            if not entry[1]:\n",
        "                MODEL_REPO = input(\"  HF repo: \").strip()\n",
        "                MODEL_FILE = input(\"  Filename: \").strip()\n",
        "                N_CTX = 2048\n",
        "            else:\n",
        "                MODEL_REPO, MODEL_FILE = entry[1], entry[2]\n",
        "                N_CTX = auto_ctx_size(entry[3])\n",
        "            USE_MODEL = True\n",
        "            print(f\"  âœ“ {MODEL_FILE}  (ctx={N_CTX})\")\n",
        "            break\n",
        "        except ValueError:\n",
        "            print(\"  Invalid choice. Enter L1/L2..., 0-7, or press Enter.\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MAIN\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"  MY-AI-Gizmo  v3.1  Universal Launcher\")\n",
        "print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"  Features: ï¼‹button | Styles | Google Docs | Google Slides | Dual Model\")\n",
        "print(\"  FIX v3.1: No-model mode | Drive fallback | Model path fixed\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "choose_mode()\n",
        "\n",
        "if USE_GPU:\n",
        "    r = sh(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\")\n",
        "    print(f\"[{'âœ“' if r.returncode==0 else 'warn'}] GPU: {r.stdout.strip() if r.returncode==0 else 'not found'}\")\n",
        "\n",
        "# â”€â”€ Mount Drive (with fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "drive_mounted = False\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        print(\"[info] Mounting Google Drive...\")\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "        drive_mounted = True\n",
        "        print(\"[âœ“] Google Drive mounted\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Drive not mounted ({e}) â€” using local storage\")\n",
        "\n",
        "setup_drive_root(drive_mounted)\n",
        "ensure_dirs()\n",
        "\n",
        "cleanup_broken_files()\n",
        "show_model_manager()\n",
        "choose_model()\n",
        "\n",
        "if not download_repo_if_missing() and not WORK_DIR.exists():\n",
        "    raise SystemExit(\"Repository unavailable.\")\n",
        "\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "ensure_symlinks_and_files()\n",
        "\n",
        "# â”€â”€ Fix model path BEFORE writing settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nğŸ“¥ Checking model...\")\n",
        "print_ram_status()\n",
        "download_model_if_missing()\n",
        "ensure_model_in_webui()   # <â”€â”€ THE KEY FIX\n",
        "\n",
        "prepare_settings_file()\n",
        "write_cmd_flags()\n",
        "write_debug_character()\n",
        "write_model_loader_config()\n",
        "\n",
        "# â”€â”€ Deploy all extensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nğŸ“¦ Deploying extensions...\")\n",
        "_deploy_ext_from_repo(\"gizmo_toolbar\")\n",
        "_deploy_ext_from_repo(\"google_workspace\")\n",
        "deploy_dual_model_extension()\n",
        "\n",
        "# â”€â”€ Install deps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MPL_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "start_sh      = WORK_DIR / \"start_linux.sh\"\n",
        "installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "env_marker    = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "\n",
        "install_env = os.environ.copy()\n",
        "if USE_GPU:\n",
        "    install_env.update({\"MPLBACKEND\":\"Agg\",\"MPLCONFIGDIR\":str(MPL_CONFIG_DIR),\n",
        "        \"GPU_CHOICE\":\"A\",\"LAUNCH_AFTER_INSTALL\":\"FALSE\",\"INSTALL_EXTENSIONS\":\"FALSE\",\n",
        "        \"CMAKE_ARGS\":\"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\",\"FORCE_CMAKE\":\"1\",\n",
        "        \"SKIP_TORCH_TEST\":\"TRUE\",\"FORCE_CUDA\":\"TRUE\"})\n",
        "else:\n",
        "    install_env.update({\"MPLBACKEND\":\"Agg\",\"MPLCONFIGDIR\":str(MPL_CONFIG_DIR),\n",
        "        \"GPU_CHOICE\":\"N\",\"LAUNCH_AFTER_INSTALL\":\"FALSE\",\"INSTALL_EXTENSIONS\":\"FALSE\",\n",
        "        \"CMAKE_ARGS\":\"-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF\",\"FORCE_CMAKE\":\"1\",\n",
        "        \"CUDA_VISIBLE_DEVICES\":\"\",\"CUDACXX\":\"\",\"SKIP_TORCH_TEST\":\"TRUE\",\"FORCE_CUDA\":\"FALSE\"})\n",
        "\n",
        "if not start_sh.exists():\n",
        "    raise SystemExit(\"[error] start_linux.sh not found.\")\n",
        "sh(\"chmod +x start_linux.sh\")\n",
        "\n",
        "if not env_marker.exists():\n",
        "    print(\"[info] First run â€” installing (5-10 min)...\")\n",
        "    code, _ = stream_with_heartbeat(\"bash start_linux.sh\", cwd=str(WORK_DIR),\n",
        "                                    env=install_env, logfile_path=str(installer_log))\n",
        "    print(f\"[{'âœ“' if code==0 else 'warn'}] Installer code {code}\")\n",
        "\n",
        "if USE_GPU:\n",
        "    install_llama_cpp_python_gpu()\n",
        "else:\n",
        "    install_llama_cpp_python_cpu()\n",
        "\n",
        "create_llama_cpp_binaries_wrapper()\n",
        "patch_gradio_launch()\n",
        "install_google_workspace_deps()\n",
        "\n",
        "# â”€â”€ Build launch wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "launch_wrapper = WORK_DIR / \"_launch_debug.py\"\n",
        "mode_label     = \"GPU\" if USE_GPU else \"CPU\"\n",
        "cuda_block     = \"\" if USE_GPU else \"\\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\"\n",
        "threads        = auto_thread_count()\n",
        "model_flag_py  = f\"'--model', '{MODEL_FILE}',\" if USE_MODEL and MODEL_FILE else \"# no model flag\"\n",
        "model_desc     = MODEL_FILE if USE_MODEL else \"NO MODEL\"\n",
        "\n",
        "launch_code = f\"\"\"#!/usr/bin/env python3\n",
        "import sys, os\n",
        "{cuda_block}\n",
        "os.environ['MPLBACKEND']         = 'Agg'\n",
        "os.environ['MPLCONFIGDIR']       = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE']       = '1'\n",
        "flags = ['--listen','--share','--verbose','--api','--api-port','5000',\n",
        "         '--loader','llama.cpp','--gpu-layers','{GPU_LAYERS}',\n",
        "         '--ctx-size','{N_CTX}','--batch-size','512','--threads','{threads}',\n",
        "         {model_flag_py}\n",
        "         '--extensions','gizmo_toolbar,dual_model,google_workspace']\n",
        "for f in flags:\n",
        "    if f not in sys.argv: sys.argv.append(f)\n",
        "print(\"[LAUNCHER] {mode_label} | {model_desc} | ï¼‹button | Google Workspace | Dual Model\")\n",
        "try:\n",
        "    import matplotlib; matplotlib.use('Agg', force=True)\n",
        "except Exception: pass\n",
        "import runpy\n",
        "runpy.run_path('server.py', run_name='__main__')\n",
        "\"\"\"\n",
        "launch_wrapper.write_text(launch_code, encoding=\"utf-8\")\n",
        "print(f\"[âœ“] Launch wrapper ready\")\n",
        "\n",
        "# â”€â”€ Kill old servers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "time.sleep(2)\n",
        "\n",
        "python_exe  = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "launch_cmd  = f'{python_exe} -u \"{str(launch_wrapper)}\"'\n",
        "server_env  = os.environ.copy()\n",
        "server_env.update({\"MPLBACKEND\":\"Agg\",\"MPLCONFIGDIR\":str(MPL_CONFIG_DIR),\n",
        "                   \"GRADIO_SERVER_NAME\":\"0.0.0.0\",\"GRADIO_SHARE\":\"1\"})\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"  LAUNCHING v3.1 â€” {mode_label}\")\n",
        "print(f\"  Model   : {model_desc}\")\n",
        "print(f\"  Threads : {threads}  |  ctx: {N_CTX}\")\n",
        "print(f\"  Extensions: ï¼‹Toolbar | Dual Model | Google Workspace\")\n",
        "print(\"=\"*70)\n",
        "print_ram_status()\n",
        "print(\"â³ Starting...\\n\")\n",
        "\n",
        "# â”€â”€ Auto-restart loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "captured = None\n",
        "for attempt in range(1, MAX_RESTARTS + 1):\n",
        "    server_log = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "    if attempt > 1:\n",
        "        print(f\"\\nğŸ”„ Auto-restart #{attempt-1}...\\n\"); time.sleep(5)\n",
        "\n",
        "    mon_stop = threading.Event()\n",
        "    mon = threading.Thread(target=resource_monitor, args=(mon_stop, str(server_log)), daemon=True)\n",
        "    mon.start()\n",
        "\n",
        "    code, captured = stream_with_heartbeat(launch_cmd, cwd=str(WORK_DIR), env=server_env,\n",
        "                                           logfile_path=str(server_log),\n",
        "                                           capture_url_to=str(PUBLIC_URL_FILE))\n",
        "    mon_stop.set(); mon.join(timeout=2)\n",
        "\n",
        "    if code in (0, -9): break\n",
        "    if attempt < MAX_RESTARTS: print(f\"[warn] Crashed (code {code}) â€” restarting...\")\n",
        "    else: print(f\"[warn] Crashed {MAX_RESTARTS} times â€” stopping.\")\n",
        "\n",
        "# â”€â”€ Fallbacks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if not captured:\n",
        "    print(\"\\n[info] No Gradio URL â€” trying ngrok...\")\n",
        "    captured = try_setup_ngrok(7860)\n",
        "\n",
        "if not captured and server_log.exists():\n",
        "    try:\n",
        "        log_text = server_log.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        m = re.search(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', log_text)\n",
        "        if m: captured = m.group(1).rstrip(\").,\\\\'\\\"\"); print(\"[âœ“] URL from log\")\n",
        "    except Exception: pass\n",
        "\n",
        "# â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if captured:\n",
        "    print(f\"  âœ… READY!  â†’  {captured}\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"  UI GUIDE:\")\n",
        "    print(\"  â€¢ ï¼‹ button (bottom-left) â†’ styles, connectors, quick tools\")\n",
        "    print(\"  â€¢ ğŸ›  Toolbar tab          â†’ create custom styles\")\n",
        "    print(\"  â€¢ ğŸ”— Google Workspace tab â†’ connect Docs & Slides\")\n",
        "    print(\"  â€¢ ğŸ¤– Dual Model tab       â†’ load a second model\")\n",
        "    if not USE_MODEL:\n",
        "        print(\"  â€¢ âš ï¸  No model loaded â€” go to Model tab in the UI to load one\")\n",
        "    print(\"  â€¢ API: http://0.0.0.0:5000/v1\")\n",
        "else:\n",
        "    print(\"  âŒ NO PUBLIC URL\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"  FIXES: delete installer_files/ | pkill -9 -f server.py | check Colab internet\")\n",
        "    if PUBLIC_URL_FILE and PUBLIC_URL_FILE.exists():\n",
        "        saved = PUBLIC_URL_FILE.read_text().strip()\n",
        "        if saved: print(f\"\\n  Previously saved URL: {saved}\")\n",
        "\n",
        "print_ram_status()\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, glob\n",
        "\n",
        "base = \"/content/text-generation-webui\"\n",
        "model_name = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "dest_dir = f\"{base}/user_data/models\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Find where the model actually landed\n",
        "found = glob.glob(f\"{base}/**/{model_name}\", recursive=True) + \\\n",
        "        glob.glob(f\"/content/**/{model_name}\", recursive=True)\n",
        "\n",
        "if found:\n",
        "    src = found[0]\n",
        "    dst = f\"{dest_dir}/{model_name}\"\n",
        "    if src != dst:\n",
        "        print(f\"Moving {src} â†’ {dst}\")\n",
        "        shutil.move(src, dst)\n",
        "    print(\"âœ“ Model is in the right place\")\n",
        "else:\n",
        "    print(\"âœ— Model not found â€” re-download needed\")\n",
        "    # Re-download directly to the right place\n",
        "    os.system(f\"wget -q --show-progress -P {dest_dir} https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/{model_name}\")\n",
        "    print(\"âœ“ Download complete\")"
      ],
      "metadata": {
        "id": "xz1cXI-NPY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… RECOMMENDED MODELS (COPY EXACTLY)\n",
        "ğŸ”¹ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "âš™ï¸ WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}