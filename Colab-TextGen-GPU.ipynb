{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE LLAMA CHAT - NO ERRORS - CPU ONLY\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- MODEL CHOICE ----------\n",
        "# ü¶ô OPTION 1: Llama 3.2 3B (DEFAULT - FAST)\n",
        "MODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\n",
        "SPECIFIC_FILE = \"Llama-3.2-3B-Instruct-Q5_K_M.gguf\"\n",
        "\n",
        "# ü¶ô OPTION 2: Llama 3.1 8B (BETTER QUALITY)\n",
        "# MODEL_REPO = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"\n",
        "# SPECIFIC_FILE = \"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n",
        "\n",
        "# üíª OPTION 3: Qwen2.5-Coder 7B (GOOD AT CODING)\n",
        "# MODEL_REPO = \"bartowski/Qwen2.5-Coder-7B-Instruct-GGUF\"\n",
        "# SPECIFIC_FILE = \"Qwen2.5-Coder-7B-Instruct-Q5_K_M.gguf\"\n",
        "\n",
        "# üöÄ OPTION 4: Phi-3 Mini (VERY FAST)\n",
        "# MODEL_REPO = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
        "# SPECIFIC_FILE = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
        "# ------------------------------------\n",
        "\n",
        "# Setup\n",
        "models_dir = Path(\"/content/models\") / MODEL_REPO.replace(\"/\", \"_\")\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "model_path = models_dir / SPECIFIC_FILE\n",
        "\n",
        "# Force CPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    \"\"\"Run command and show output\"\"\"\n",
        "    print(f\"\\n>>> {cmd}\\n\")\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=False, text=True)\n",
        "    return result.returncode\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"ü§ñ SIMPLE LLAMA CHAT - CPU MODE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üì¶ Model: {MODEL_REPO}\")\n",
        "print(f\"üìÑ File: {SPECIFIC_FILE}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1) Download model if needed\n",
        "if model_path.exists() and model_path.stat().st_size > 1_000_000:\n",
        "    print(f\"\\n‚úì Model already exists: {model_path}\")\n",
        "else:\n",
        "    print(\"\\nüì• Downloading model...\")\n",
        "    run_cmd(\"pip install -q huggingface_hub\")\n",
        "\n",
        "    try:\n",
        "        from huggingface_hub import hf_hub_download\n",
        "        print(f\"Downloading {SPECIFIC_FILE}...\")\n",
        "        downloaded = hf_hub_download(\n",
        "            repo_id=MODEL_REPO,\n",
        "            filename=SPECIFIC_FILE,\n",
        "            local_dir=str(models_dir),\n",
        "            resume_download=True\n",
        "        )\n",
        "        print(f\"‚úì Downloaded to: {downloaded}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download failed: {e}\")\n",
        "        print(f\"\\nManual download:\")\n",
        "        print(f\"1. Go to: https://huggingface.co/{MODEL_REPO}/tree/main\")\n",
        "        print(f\"2. Download: {SPECIFIC_FILE}\")\n",
        "        print(f\"3. Upload to: {models_dir}\")\n",
        "        raise SystemExit(1)\n",
        "\n",
        "# 2) Install llama-cpp-python (CPU version)\n",
        "print(\"\\nüì¶ Installing llama-cpp-python (CPU)...\")\n",
        "run_cmd(\"pip install -q llama-cpp-python\")\n",
        "\n",
        "# 3) Install Gradio for web interface\n",
        "print(\"\\nüì¶ Installing Gradio...\")\n",
        "run_cmd(\"pip install -q gradio\")\n",
        "\n",
        "# 4) Create and run the chat interface\n",
        "print(\"\\nüöÄ Starting chat interface...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create the Python script for the chat interface\n",
        "chat_script = f'''\n",
        "import gradio as gr\n",
        "from llama_cpp import Llama\n",
        "\n",
        "print(\"Loading model... (this may take 1-2 minutes)\")\n",
        "llm = Llama(\n",
        "    model_path=\"{model_path}\",\n",
        "    n_ctx=4096,          # Context window\n",
        "    n_threads={os.cpu_count() or 2},  # Use all CPU cores\n",
        "    n_gpu_layers=0,      # CPU only\n",
        "    verbose=False\n",
        ")\n",
        "print(\"‚úì Model loaded!\")\n",
        "\n",
        "def chat(message, history):\n",
        "    \"\"\"Generate response\"\"\"\n",
        "    # Build conversation\n",
        "    conversation = []\n",
        "    for h in history:\n",
        "        conversation.append({{\"role\": \"user\", \"content\": h[0]}})\n",
        "        conversation.append({{\"role\": \"assistant\", \"content\": h[1]}})\n",
        "    conversation.append({{\"role\": \"user\", \"content\": message}})\n",
        "\n",
        "    # Generate\n",
        "    response = llm.create_chat_completion(\n",
        "        messages=conversation,\n",
        "        max_tokens=1024,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    # Stream output\n",
        "    partial = \"\"\n",
        "    for chunk in response:\n",
        "        if \"choices\" in chunk:\n",
        "            delta = chunk[\"choices\"][0].get(\"delta\", {{}})\n",
        "            if \"content\" in delta:\n",
        "                partial += delta[\"content\"]\n",
        "                yield partial\n",
        "\n",
        "# Create interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat,\n",
        "    title=\"ü¶ô Llama Chat (CPU Mode)\",\n",
        "    description=f\"Model: {MODEL_REPO} | Running on CPU\",\n",
        "    examples=[\n",
        "        \"What is the capital of France?\",\n",
        "        \"Write a Python function to calculate fibonacci numbers\",\n",
        "        \"Explain quantum computing in simple terms\"\n",
        "    ],\n",
        "    retry_btn=None,\n",
        "    undo_btn=\"Delete Previous\",\n",
        "    clear_btn=\"Clear\"\n",
        ")\n",
        "\n",
        "# Launch with public URL\n",
        "demo.launch(share=True, server_name=\"0.0.0.0\")\n",
        "'''\n",
        "\n",
        "# Save the script\n",
        "script_path = Path(\"/content/chat_app.py\")\n",
        "script_path.write_text(chat_script)\n",
        "\n",
        "# Run it\n",
        "print(\"\\nüåê Starting server... Look for the public URL below!\\n\")\n",
        "print(\"=\" * 70)\n",
        "os.system(f\"python {script_path}\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}