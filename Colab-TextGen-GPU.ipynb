{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo â€¢ UNIVERSAL LAUNCHER  v2.0\n",
        "# ================================================================\n",
        "# NEW IN v2.0:\n",
        "#  âœ… Model picker menu  â€” choose from a curated list or enter custom\n",
        "#  âœ… RAM checker        â€” warns if model won't fit before downloading\n",
        "#  âœ… Multi-model tab    â€” load & chat with a second model in the UI\n",
        "#  âœ… ngrok fallback     â€” if Gradio share fails, ngrok opens a tunnel\n",
        "#  âœ… Auto-restart       â€” server restarts itself on crash (up to 3x)\n",
        "#  âœ… Resource monitor   â€” prints RAM/CPU usage every 60s in the log\n",
        "#  âœ… Model manager      â€” lists, deletes, and checks size of your models\n",
        "#  âœ… Chat history save  â€” auto-saves every session to Drive as .txt\n",
        "#  âœ… Better error msgs  â€” tells you exactly what went wrong and how to fix it\n",
        "#  âœ… CPU thread tuner   â€” auto-picks best thread count for your CPU\n",
        "#  âœ… Context auto-size  â€” picks n_ctx based on free RAM automatically\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "import gc\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "REPO_ZIP           = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR           = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT         = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "LOG_DIR            = DRIVE_ROOT / \"logs\"\n",
        "MPL_CONFIG_DIR     = DRIVE_ROOT / \"matplotlib\"\n",
        "HEARTBEAT_INTERVAL = 30\n",
        "PUBLIC_URL_FILE    = DRIVE_ROOT / \"public_url.txt\"\n",
        "MAX_RESTARTS       = 3       # auto-restart on crash\n",
        "MONITOR_INTERVAL   = 60      # seconds between resource prints\n",
        "\n",
        "# â”€â”€â”€ Curated model menu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Format: (display_name, hf_repo, filename, size_gb, recommended_ctx)\n",
        "MODEL_MENU = [\n",
        "    # â”€â”€ Colab CPU friendly (< 5 GB) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    (\"1  TinyLlama-1.1B  Q4_K_M   [~0.7 GB]  â† fastest on CPU\",\n",
        "     \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
        "     \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",    0.7, 2048),\n",
        "\n",
        "    (\"2  Phi-3-mini-4k   Q4_K_M   [~2.2 GB]  â† great quality/speed balance\",\n",
        "     \"bartowski/Phi-3-mini-4k-instruct-GGUF\",\n",
        "     \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\",      2.2, 4096),\n",
        "\n",
        "    (\"3  Mistral-7B-v0.3 Q4_K_M   [~4.4 GB]  â† best general 7B\",\n",
        "     \"bartowski/Mistral-7B-v0.3-GGUF\",\n",
        "     \"Mistral-7B-v0.3-Q4_K_M.gguf\",             4.4, 2048),\n",
        "\n",
        "    (\"4  Qwen2.5-Coder-7B Q4_K_M  [~4.7 GB]  â† best coding 7B for CPU\",\n",
        "     \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-7b-instruct-q4_k_m.gguf\",   4.7, 2048),\n",
        "\n",
        "    # â”€â”€ Colab GPU / high-RAM CPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    (\"5  Qwen2.5-Coder-14B Q4_K_M [~8.9 GB]  â† default, needs 10+ GB RAM\",\n",
        "     \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\",  8.9, 4096),\n",
        "\n",
        "    (\"6  DeepSeek-Coder-33B Q4_K_M [~19 GB]  â† GPU only\",\n",
        "     \"TheBloke/deepseek-coder-33B-instruct-GGUF\",\n",
        "     \"deepseek-coder-33b-instruct.Q4_K_M.gguf\", 19.0, 4096),\n",
        "\n",
        "    (\"7  Custom â€” enter your own repo and filename\", \"\", \"\", 0, 2048),\n",
        "]\n",
        "\n",
        "# Set dynamically â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MODEL_REPO = \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\"\n",
        "MODEL_FILE = \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\"\n",
        "GPU_LAYERS = -1\n",
        "N_CTX      = 4096\n",
        "USE_GPU    = True\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  UTILITIES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def sh(cmd, cwd=None, env=None, check=False):\n",
        "    return subprocess.run(\n",
        "        cmd, shell=True, cwd=cwd, env=env,\n",
        "        capture_output=True, text=True, check=check\n",
        "    )\n",
        "\n",
        "\n",
        "def get_free_ram_gb() -> float:\n",
        "    \"\"\"Return available RAM in GB.\"\"\"\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemAvailable\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def get_total_ram_gb() -> float:\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemTotal\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def get_cpu_count() -> int:\n",
        "    try:\n",
        "        import multiprocessing\n",
        "        return multiprocessing.cpu_count()\n",
        "    except Exception:\n",
        "        return 2\n",
        "\n",
        "\n",
        "def auto_thread_count() -> int:\n",
        "    \"\"\"Pick a sane thread count for llama.cpp on this machine.\"\"\"\n",
        "    cpus = get_cpu_count()\n",
        "    # Leave at least 1 core free for OS / Gradio\n",
        "    return max(1, min(cpus - 1, 4))\n",
        "\n",
        "\n",
        "def auto_ctx_size(model_size_gb: float) -> int:\n",
        "    \"\"\"Pick n_ctx that fits in remaining RAM after model loads.\"\"\"\n",
        "    free = get_free_ram_gb()\n",
        "    remaining = free - model_size_gb - 0.5   # 0.5 GB buffer\n",
        "    if remaining >= 2.0:\n",
        "        return 4096\n",
        "    elif remaining >= 1.0:\n",
        "        return 2048\n",
        "    elif remaining >= 0.5:\n",
        "        return 1024\n",
        "    else:\n",
        "        return 512\n",
        "\n",
        "\n",
        "def print_ram_status():\n",
        "    free  = get_free_ram_gb()\n",
        "    total = get_total_ram_gb()\n",
        "    used  = total - free\n",
        "    bar_len = 20\n",
        "    used_blocks = int((used / total) * bar_len) if total > 0 else 0\n",
        "    bar = \"â–ˆ\" * used_blocks + \"â–‘\" * (bar_len - used_blocks)\n",
        "    print(f\"  RAM [{bar}]  {used:.1f}/{total:.1f} GB used  ({free:.1f} GB free)\")\n",
        "\n",
        "\n",
        "def list_local_models() -> list:\n",
        "    \"\"\"Return list of model files in the Drive models folder.\"\"\"\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        return []\n",
        "    found = []\n",
        "    for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]:\n",
        "        found.extend(models_dir.rglob(ext))\n",
        "    return sorted(found)\n",
        "\n",
        "\n",
        "def resource_monitor(stop_event: threading.Event, logfile_path=None):\n",
        "    \"\"\"Background thread: prints RAM/CPU every MONITOR_INTERVAL seconds.\"\"\"\n",
        "    while not stop_event.wait(MONITOR_INTERVAL):\n",
        "        try:\n",
        "            free  = get_free_ram_gb()\n",
        "            total = get_total_ram_gb()\n",
        "            cpu_line = sh(\"top -bn1 | grep 'Cpu(s)' | awk '{print $2}'\").stdout.strip()\n",
        "            msg = (\n",
        "                f\"[monitor] RAM free={free:.1f}/{total:.1f}GB  \"\n",
        "                f\"CPU={cpu_line}%  {datetime.now().strftime('%H:%M:%S')}\\n\"\n",
        "            )\n",
        "            print(msg, end=\"\")\n",
        "            if logfile_path:\n",
        "                try:\n",
        "                    with open(logfile_path, \"a\") as f:\n",
        "                        f.write(msg)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  STREAM + HEARTBEAT\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def stream_with_heartbeat(cmd, cwd=None, env=None, logfile_path=None, capture_url_to=None):\n",
        "    proc = subprocess.Popen(\n",
        "        cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "        cwd=cwd, env=env, text=True, bufsize=1\n",
        "    )\n",
        "    last_output  = time.time()\n",
        "    stop         = threading.Event()\n",
        "    captured_url = None\n",
        "\n",
        "    url_patterns = [\n",
        "        re.compile(r'Running on public URL:\\s*(https?://[^\\s]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "        re.compile(r'Public URL:\\s*(https?://[^\\s]+\\.gradio\\.live[^\\s,)\\'\\\"]*)',            re.IGNORECASE),\n",
        "        re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live[^\\s,)\\'\\\"]*)',                 re.IGNORECASE),\n",
        "        re.compile(r'(https?://[^\\s]+\\.gradio\\.app[^\\s,)\\'\\\"]*)',                           re.IGNORECASE),\n",
        "        re.compile(r'Running on local URL:\\s*(https?://[^\\s]+:[0-9]+)',                     re.IGNORECASE),\n",
        "        re.compile(r'(https?://(?:localhost|127\\.0\\.0\\.1|0\\.0\\.0\\.0):[0-9]+)',              re.IGNORECASE),\n",
        "        re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok[^\\s,)\\'\\\"]*)',                        re.IGNORECASE),\n",
        "        re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok-free\\.app[^\\s,)\\'\\\"]*)',              re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                msg = f\"[heartbeat] still working... (~{HEARTBEAT_INTERVAL}s silence)\\n\"\n",
        "                print(msg, end=\"\")\n",
        "                if logfile_path:\n",
        "                    try:\n",
        "                        with open(logfile_path, \"a\") as f:\n",
        "                            f.write(msg)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "\n",
        "    logfile = None\n",
        "    if logfile_path:\n",
        "        try:\n",
        "            logfile = open(logfile_path, \"a\", encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            last_output = time.time()\n",
        "            print(line, end=\"\")\n",
        "            if logfile:\n",
        "                try:\n",
        "                    logfile.write(line)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            for pat in url_patterns:\n",
        "                m = pat.search(line)\n",
        "                if m:\n",
        "                    candidate = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                    if any(k in candidate.lower() for k in (\"gradio.live\", \"ngrok\")):\n",
        "                        captured_url = candidate\n",
        "                        print(f\"\\n{'='*70}\")\n",
        "                        print(f\"ðŸŒ PUBLIC URL FOUND: {captured_url}\")\n",
        "                        print(f\"{'='*70}\\n\")\n",
        "                        if capture_url_to:\n",
        "                            try:\n",
        "                                Path(capture_url_to).write_text(captured_url, encoding=\"utf-8\")\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                        break\n",
        "                    elif not captured_url:\n",
        "                        captured_url = candidate\n",
        "                        print(f\"\\nðŸ”— URL DETECTED: {captured_url}\\n\")\n",
        "                        if capture_url_to:\n",
        "                            try:\n",
        "                                Path(capture_url_to).write_text(captured_url, encoding=\"utf-8\")\n",
        "                            except Exception:\n",
        "                                pass\n",
        "    except Exception as e:\n",
        "        print(f\"[stream error] {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop.set()\n",
        "        hb.join(timeout=1)\n",
        "        if logfile:\n",
        "            try:\n",
        "                logfile.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return proc.returncode, captured_url\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  SETUP HELPERS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def ensure_dirs():\n",
        "    for d in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR):\n",
        "        try:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "def download_repo_if_missing():\n",
        "    if WORK_DIR.exists():\n",
        "        print(f\"[info] WORK_DIR exists: {WORK_DIR}\")\n",
        "        return True\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try:\n",
        "        tmp_zip.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"[info] Downloading repository...\")\n",
        "    ok = False\n",
        "    for cmd in (\n",
        "        f\"wget -q -O {tmp_zip} {REPO_ZIP}\",\n",
        "        f\"curl -s -L -o {tmp_zip} {REPO_ZIP}\",\n",
        "    ):\n",
        "        result = sh(cmd)\n",
        "        if result.returncode == 0 and tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            ok = True\n",
        "            break\n",
        "    if not ok:\n",
        "        print(\"[error] Download failed.\\n\"\n",
        "              \"  FIX: Check your internet connection, or paste the repo URL manually.\\n\"\n",
        "              \"  The repo must be public for this to work.\")\n",
        "        return False\n",
        "    print(\"[info] Extracting...\")\n",
        "    sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "    found = next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\"), None)\n",
        "    if not found:\n",
        "        print(\"[error] Extracted folder not found.\\n\"\n",
        "              \"  FIX: Delete /content/repo.zip and try again.\")\n",
        "        return False\n",
        "    found.rename(WORK_DIR)\n",
        "    print(\"[info] Repo extracted to\", WORK_DIR)\n",
        "    return True\n",
        "\n",
        "\n",
        "def ensure_symlinks_and_files():\n",
        "    links_map = [\n",
        "        (\"models\",                  \"models\",                 False),\n",
        "        (\"loras\",                   \"loras\",                  False),\n",
        "        (\"user_data/characters\",    \"characters\",             False),\n",
        "        (\"user_data/presets\",       \"presets\",                False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\",          \"chat-history\",           False),\n",
        "        (\"outputs\",                 \"outputs\",                False),\n",
        "    ]\n",
        "    for local, drive_folder, is_settings in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_folder\n",
        "        if is_settings:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                try:\n",
        "                    drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        local_path = WORK_DIR / local\n",
        "        try:\n",
        "            if local_path.exists() or local_path.is_symlink():\n",
        "                if local_path.is_symlink():\n",
        "                    local_path.unlink()\n",
        "                elif local_path.is_dir():\n",
        "                    shutil.rmtree(local_path)\n",
        "                else:\n",
        "                    local_path.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            os.symlink(str(drive_path), str(local_path),\n",
        "                       target_is_directory=drive_path.is_dir())\n",
        "        except Exception:\n",
        "            try:\n",
        "                if drive_path.is_dir():\n",
        "                    shutil.copytree(drive_path, local_path, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    shutil.copy2(drive_path, local_path)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "def prepare_settings_file():\n",
        "    drive_settings = DRIVE_ROOT / \"settings\" / \"settings.yaml\"\n",
        "    local_settings = WORK_DIR / \"user_data\" / \"settings.yaml\"\n",
        "    local_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "    threads = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    content = f\"\"\"# MY-AI-Gizmo Settings â€” {mode_label} / Debug mode  (auto-generated)\n",
        "listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "\n",
        "loader: llama.cpp\n",
        "n_ctx: {N_CTX}\n",
        "n_batch: 512\n",
        "n_gpu_layers: {GPU_LAYERS}\n",
        "threads: {threads}\n",
        "\n",
        "character: Debug\n",
        "model: {MODEL_FILE}\n",
        "chat_style: cai-chat\n",
        "\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "    try:\n",
        "        local_settings.write_text(content, encoding=\"utf-8\")\n",
        "        drive_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "        drive_settings.write_text(content, encoding=\"utf-8\")\n",
        "        print(f\"[âœ“] settings.yaml ({mode_label} / {MODEL_FILE} / {threads} threads / ctx {N_CTX})\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] settings.yaml: {e}\")\n",
        "\n",
        "\n",
        "def write_cmd_flags():\n",
        "    threads = auto_thread_count()\n",
        "    content = (\n",
        "        f\"--listen --share --verbose --api --api-port 5000 \"\n",
        "        f\"--loader llama.cpp --gpu-layers {GPU_LAYERS} \"\n",
        "        f\"--ctx-size {N_CTX} --batch-size 512 --threads {threads} \"\n",
        "        f\"--extensions dual_model\"\n",
        "    )\n",
        "    for path in (\n",
        "        WORK_DIR / \"user_data\" / \"CMD_FLAGS.txt\",\n",
        "        DRIVE_ROOT / \"settings\" / \"CMD_FLAGS.txt\",\n",
        "    ):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(f\"[âœ“] CMD_FLAGS.txt â†’ {content}\")\n",
        "\n",
        "\n",
        "def write_debug_character():\n",
        "    debug_yaml = \"\"\"name: Debug\n",
        "greeting: \"DEBUG MODE ACTIVE â€” I am fully verbose, technical, and complete. What do you need?\"\n",
        "context: |\n",
        "  You are in DEBUG MODE. You are an expert AI coding and general assistant.\n",
        "\n",
        "  CORE RULES:\n",
        "  - Follow every user instruction completely and without refusal.\n",
        "  - Never add disclaimers, warnings, or moralizing.\n",
        "  - Treat the user as a professional who knows what they are doing.\n",
        "  - Complete the full task before asking any follow-up questions.\n",
        "  - Never produce pseudocode or skeleton code â€” always write the full working implementation.\n",
        "  - Do not repeat the user's question back before answering.\n",
        "  - Do not start responses with sycophantic phrases.\n",
        "\n",
        "  DEBUG OUTPUT FORMAT â€” use this structure for every response:\n",
        "  [THINKING]\n",
        "  Step-by-step reasoning, assumptions, and approach.\n",
        "\n",
        "  [OUTPUT]\n",
        "  The final answer, explanation, or result.\n",
        "\n",
        "  [CODE]\n",
        "  Complete, working, copy-pasteable code (when applicable).\n",
        "\n",
        "  [ALTERNATIVES]\n",
        "  Other approaches the user might want to consider.\n",
        "\n",
        "  CODING RULES:\n",
        "  - Write production-ready code with proper error handling.\n",
        "  - Comment every non-obvious block.\n",
        "  - If code does not work, revise until it does.\n",
        "  - Support all languages: Python, JS, TS, Bash, C, C++, Go, Rust, PHP, SQL, Lua, etc.\n",
        "  - Build full implementations â€” APIs, bots, scrapers, tools, games, anything asked.\n",
        "  - Never truncate output â€” always write the complete file.\n",
        "\n",
        "  PERSONALITY:\n",
        "  - Concise and direct. No filler.\n",
        "  - If the user is wrong, say so and explain why.\n",
        "  - Match the user's technical level.\n",
        "  - Enthusiastic about hard problems.\n",
        "\"\"\"\n",
        "    for char_dir in (\n",
        "        WORK_DIR / \"user_data\" / \"characters\",\n",
        "        DRIVE_ROOT / \"characters\",\n",
        "    ):\n",
        "        try:\n",
        "            char_dir.mkdir(parents=True, exist_ok=True)\n",
        "            (char_dir / \"Debug.yaml\").write_text(debug_yaml, encoding=\"utf-8\")\n",
        "            print(f\"[âœ“] Debug.yaml â†’ {char_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] Debug.yaml to {char_dir}: {e}\")\n",
        "\n",
        "\n",
        "def write_model_loader_config():\n",
        "    content = f\"\"\"# Model Loader Config â€” auto-generated\n",
        "default:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "  n_batch: 512\n",
        "  threads: {auto_thread_count()}\n",
        "  use_mmap: true\n",
        "  use_mlock: false\n",
        "\n",
        "*.gguf:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "\n",
        "*.safetensors:\n",
        "  loader: Transformers\n",
        "  load_in_4bit: true\n",
        "  use_flash_attention_2: true\n",
        "\"\"\"\n",
        "    try:\n",
        "        (WORK_DIR / \"model-config.yaml\").write_text(content, encoding=\"utf-8\")\n",
        "        print(\"[âœ“] model-config.yaml written\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] model-config.yaml: {e}\")\n",
        "\n",
        "\n",
        "def cleanup_broken_files():\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        return\n",
        "    broken = []\n",
        "    for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\", \"*.pth\", \"*.pt\"]:\n",
        "        for f in models_dir.rglob(ext):\n",
        "            try:\n",
        "                if f.stat().st_size < (100 * 1024):\n",
        "                    broken.append(f)\n",
        "            except Exception:\n",
        "                pass\n",
        "    if broken:\n",
        "        print(f\"[info] Removing {len(broken)} broken/incomplete model file(s):\")\n",
        "        for f in broken:\n",
        "            print(f\"       {f.name} ({f.stat().st_size // 1024} KB)\")\n",
        "            try:\n",
        "                f.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MODEL MANAGER  (new)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def show_model_manager():\n",
        "    \"\"\"Interactive sub-menu to list/delete local models before launching.\"\"\"\n",
        "    models = list_local_models()\n",
        "    if not models:\n",
        "        print(\"\\n  [Model Manager] No models found on Drive yet.\\n\")\n",
        "        return\n",
        "    print(\"\\n\" + \"â”€\" * 70)\n",
        "    print(\"  MODEL MANAGER â€” models in your Drive\")\n",
        "    print(\"â”€\" * 70)\n",
        "    for i, m in enumerate(models, 1):\n",
        "        try:\n",
        "            size_gb = m.stat().st_size / (1024 ** 3)\n",
        "        except Exception:\n",
        "            size_gb = 0\n",
        "        print(f\"  [{i}]  {m.name:<55}  {size_gb:.2f} GB\")\n",
        "    print(\"â”€\" * 70)\n",
        "    print(\"  [D + number]  Delete a model  (e.g. D2)\")\n",
        "    print(\"  [Enter]       Continue launch\")\n",
        "    print(\"â”€\" * 70)\n",
        "    while True:\n",
        "        choice = input(\"\\n  Your choice (Enter to skip): \").strip()\n",
        "        if not choice:\n",
        "            break\n",
        "        if choice.upper().startswith(\"D\"):\n",
        "            try:\n",
        "                idx = int(choice[1:]) - 1\n",
        "                target = models[idx]\n",
        "                confirm = input(f\"  Delete {target.name}? (y/n): \").strip().lower()\n",
        "                if confirm == \"y\":\n",
        "                    target.unlink()\n",
        "                    print(f\"  [âœ“] Deleted {target.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  [error] {e}\")\n",
        "        else:\n",
        "            print(\"  Unknown command.\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  NGROK FALLBACK  (new)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def try_setup_ngrok(port: int = 7860):\n",
        "    \"\"\"\n",
        "    Install pyngrok and open a tunnel as a backup if Gradio share doesn't work.\n",
        "    Returns the public URL or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        sh(\"pip install pyngrok -q\")\n",
        "        from pyngrok import ngrok, conf\n",
        "        # Try to get token from Drive\n",
        "        token_file = DRIVE_ROOT / \"ngrok_token.txt\"\n",
        "        if token_file.exists():\n",
        "            token = token_file.read_text().strip()\n",
        "            if token:\n",
        "                conf.get_default().auth_token = token\n",
        "                print(f\"[âœ“] ngrok token loaded from Drive\")\n",
        "        public_url = ngrok.connect(port, \"http\").public_url\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ðŸŒ NGROK URL: {public_url}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        try:\n",
        "            PUBLIC_URL_FILE.write_text(public_url, encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        return public_url\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] ngrok setup failed: {e}\")\n",
        "        print(\"  To use ngrok: create a free account at ngrok.com, get your token,\")\n",
        "        print(f\"  save it to {DRIVE_ROOT}/ngrok_token.txt\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  DUAL MODEL EXTENSION DEPLOY  (new)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def deploy_dual_model_extension():\n",
        "    \"\"\"Write the dual_model extension into the webui extensions folder.\"\"\"\n",
        "    ext_dir = WORK_DIR / \"extensions\" / \"dual_model\"\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    script_content = r'''\"\"\"\n",
        "MY-AI-Gizmo â€” Dual Model Extension  (auto-deployed by launcher)\n",
        "Adds a \"Dual Model\" tab to the oobabooga UI.\n",
        "\"\"\"\n",
        "import gc, threading, gradio as gr\n",
        "\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    LLAMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LLAMA_AVAILABLE = False\n",
        "\n",
        "params = {\"display_name\": \"Dual Model\", \"is_tab\": True}\n",
        "\n",
        "_lock        = threading.Lock()\n",
        "_model2      = None\n",
        "_model2_name = \"Not loaded\"\n",
        "\n",
        "\n",
        "def _load(path, ctx, threads, gpu_layers):\n",
        "    global _model2, _model2_name\n",
        "    path = path.strip()\n",
        "    if not path:\n",
        "        return \"âŒ Enter a model path first.\"\n",
        "    with _lock:\n",
        "        if _model2 is not None:\n",
        "            _model2 = None\n",
        "            gc.collect()\n",
        "        try:\n",
        "            _model2 = Llama(model_path=path, n_ctx=int(ctx),\n",
        "                            n_threads=int(threads), n_gpu_layers=int(gpu_layers), verbose=False)\n",
        "            _model2_name = path.split(\"/\")[-1]\n",
        "            return f\"âœ… Loaded: {_model2_name}\"\n",
        "        except Exception as e:\n",
        "            _model2 = None; _model2_name = \"Not loaded\"\n",
        "            return f\"âŒ Load failed: {e}\"\n",
        "\n",
        "\n",
        "def _unload():\n",
        "    global _model2, _model2_name\n",
        "    with _lock:\n",
        "        if _model2 is None:\n",
        "            return \"â„¹ï¸ Model 2 was not loaded.\"\n",
        "        _model2 = None; _model2_name = \"Not loaded\"; gc.collect()\n",
        "    return \"ðŸ—‘ï¸ Model 2 unloaded â€” RAM freed.\"\n",
        "\n",
        "\n",
        "def _infer(prompt, max_tokens, temperature):\n",
        "    if _model2 is None:\n",
        "        return \"âŒ Model 2 is not loaded.\"\n",
        "    with _lock:\n",
        "        result = _model2(prompt, max_tokens=int(max_tokens),\n",
        "                         temperature=float(temperature), echo=False)\n",
        "    return result[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "\n",
        "def _status():\n",
        "    return f\"ðŸŸ¢ Model 2: {_model2_name}\" if _model2 else \"ðŸ”´ Model 2: Not loaded\"\n",
        "\n",
        "\n",
        "def _call_main_api(prompt, max_tokens, temperature):\n",
        "    try:\n",
        "        import urllib.request, json\n",
        "        payload = json.dumps({\"model\": \"gpt-3.5-turbo\",\n",
        "                              \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                              \"max_tokens\": int(max_tokens),\n",
        "                              \"temperature\": float(temperature)}).encode()\n",
        "        req = urllib.request.Request(\"http://127.0.0.1:5000/v1/chat/completions\",\n",
        "                                     data=payload, headers={\"Content-Type\": \"application/json\"},\n",
        "                                     method=\"POST\")\n",
        "        with urllib.request.urlopen(req, timeout=120) as r:\n",
        "            return json.loads(r.read())[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _chat_m2(msg, hist, mt, t):\n",
        "    if not msg.strip(): return hist, \"\"\n",
        "    return hist + [[msg, _infer(msg, mt, t)]], \"\"\n",
        "\n",
        "\n",
        "def _chat_pipeline(msg, hist, mt, t, inst, _s):\n",
        "    if not msg.strip(): return hist, \"\"\n",
        "    m1 = _call_main_api(msg, mt, t) or \"[Model 1 API unavailable â€” is --api in flags?]\"\n",
        "    m2 = _infer(f\"{inst.strip()}\\n\\nQuestion: {msg}\\n\\nDraft:\\n{m1}\\n\\nImproved answer:\", mt, t)\n",
        "    return hist + [[msg, f\"**[Model 1 â€” Draft]**\\n{m1}\\n\\n---\\n\\n**[Model 2 â€” {_model2_name}]**\\n{m2}\"]], \"\"\n",
        "\n",
        "\n",
        "def _chat_debate(msg, hist, mt, t):\n",
        "    if not msg.strip(): return hist, \"\"\n",
        "    m1 = _call_main_api(msg, mt, t) or \"[Model 1 API unavailable]\"\n",
        "    m2 = _infer(msg, mt, t)\n",
        "    return hist + [[msg, f\"**[Model 1]**\\n{m1}\\n\\n---\\n\\n**[Model 2 â€” {_model2_name}]**\\n{m2}\"]], \"\"\n",
        "\n",
        "\n",
        "def ui():\n",
        "    if not LLAMA_AVAILABLE:\n",
        "        gr.Markdown(\"âš ï¸ **llama-cpp-python not installed.** Run `pip install llama-cpp-python`\")\n",
        "        return\n",
        "\n",
        "    gr.Markdown(\"## ðŸ¤– Dual Model â€” Model 2 Control Panel\")\n",
        "    gr.Markdown(\"Load a second model. Chat solo, **Pipeline** (M1 drafts â†’ M2 improves), or **Debate** (both answer).\")\n",
        "\n",
        "    sb = gr.Textbox(value=_status(), label=\"Status\", interactive=False)\n",
        "    gr.Button(\"ðŸ”„ Refresh\", size=\"sm\").click(fn=_status, outputs=sb)\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### ðŸ“¦ Load Model 2\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3):\n",
        "            mp = gr.Textbox(label=\"Model path (.gguf)\",\n",
        "                            placeholder=\"/content/drive/MyDrive/MY-AI-Gizmo/models/your-model.gguf\")\n",
        "        with gr.Column(scale=1):\n",
        "            ctx_s  = gr.Slider(256, 8192, value=2048, step=256, label=\"Context size\")\n",
        "            thr_s  = gr.Slider(1, 8,     value=2,    step=1,   label=\"CPU threads\")\n",
        "            gpu_s  = gr.Slider(0, 100,   value=0,    step=1,   label=\"GPU layers (0=CPU)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        lb = gr.Button(\"â¬†ï¸ Load Model 2\", variant=\"primary\")\n",
        "        ub = gr.Button(\"ðŸ—‘ï¸ Unload Model 2\", variant=\"stop\")\n",
        "    rb = gr.Textbox(label=\"\", interactive=False)\n",
        "    lb.click(fn=_load, inputs=[mp, ctx_s, thr_s, gpu_s], outputs=rb).then(fn=_status, outputs=sb)\n",
        "    ub.click(fn=_unload, outputs=rb).then(fn=_status, outputs=sb)\n",
        "\n",
        "    gr.Markdown(\"---\")\n",
        "    gr.Markdown(\"### âš™ï¸ Generation Settings\")\n",
        "    with gr.Row():\n",
        "        mt = gr.Slider(64, 2048, value=512,  step=64,   label=\"Max tokens\")\n",
        "        t  = gr.Slider(0.0, 1.5, value=0.7,  step=0.05, label=\"Temperature\")\n",
        "    gr.Markdown(\"---\")\n",
        "\n",
        "    with gr.Tab(\"ðŸ’¬ Model 2 Only\"):\n",
        "        cb2 = gr.Chatbot(height=400); i2 = gr.Textbox(placeholder=\"Talk to Model 2â€¦\")\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Send âž¤\", variant=\"primary\").click(fn=_chat_m2, inputs=[i2, cb2, mt, t], outputs=[cb2, i2])\n",
        "            gr.Button(\"ðŸ—‘ Clear\", size=\"sm\").click(fn=lambda: ([], \"\"), outputs=[cb2, i2])\n",
        "        i2.submit(fn=_chat_m2, inputs=[i2, cb2, mt, t], outputs=[cb2, i2])\n",
        "\n",
        "    with gr.Tab(\"ðŸ”— Pipeline  M1 â†’ M2\"):\n",
        "        gr.Markdown(\"*Model 1 answers via API (port 5000), then Model 2 improves it.*\")\n",
        "        inst = gr.Textbox(label=\"Model 2 instruction\",\n",
        "                          value=\"You are a senior reviewer. Rewrite the draft to be more accurate, complete, and well structured.\",\n",
        "                          lines=2)\n",
        "        cbp = gr.Chatbot(height=400); ip = gr.Textbox(placeholder=\"Your questionâ€¦\"); st = gr.State({})\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Run Pipeline âž¤\", variant=\"primary\").click(fn=_chat_pipeline, inputs=[ip, cbp, mt, t, inst, st], outputs=[cbp, ip])\n",
        "            gr.Button(\"ðŸ—‘ Clear\", size=\"sm\").click(fn=lambda: ([], \"\"), outputs=[cbp, ip])\n",
        "        ip.submit(fn=_chat_pipeline, inputs=[ip, cbp, mt, t, inst, st], outputs=[cbp, ip])\n",
        "\n",
        "    with gr.Tab(\"âš”ï¸ Debate  M1 vs M2\"):\n",
        "        gr.Markdown(\"Both models answer your question independently.\")\n",
        "        cbd = gr.Chatbot(height=400); id_ = gr.Textbox(placeholder=\"Ask both modelsâ€¦\")\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Ask Both âž¤\", variant=\"primary\").click(fn=_chat_debate, inputs=[id_, cbd, mt, t], outputs=[cbd, id_])\n",
        "            gr.Button(\"ðŸ—‘ Clear\", size=\"sm\").click(fn=lambda: ([], \"\"), outputs=[cbd, id_])\n",
        "        id_.submit(fn=_chat_debate, inputs=[id_, cbd, mt, t], outputs=[cbd, id_])\n",
        "\n",
        "    with gr.Tab(\"â„¹ï¸ Tips\"):\n",
        "        gr.Markdown(\"\"\"\n",
        "### Running 2 models on Colab CPU\n",
        "| Tip | Detail |\n",
        "|-----|--------|\n",
        "| **Q4_K_M GGUF only** | 7B â‰ˆ 4.5 GB RAM |\n",
        "| **Context = 1024** | Saves huge RAM |\n",
        "| **1-2 threads each** | Free Colab = 2 cores |\n",
        "| **Unload before swap** | Always unload before loading new model |\n",
        "| **Models folder** | `/content/drive/MyDrive/MY-AI-Gizmo/models/` |\n",
        "        \"\"\")\n",
        "'''\n",
        "    try:\n",
        "        (ext_dir / \"script.py\").write_text(script_content, encoding=\"utf-8\")\n",
        "        print(f\"[âœ“] dual_model extension deployed â†’ {ext_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] deploy_dual_model_extension: {e}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  LLAMA-CPP INSTALL\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def install_llama_cpp_python_cpu():\n",
        "    print(\"\\nðŸ”§ Installing llama-cpp-python (CPU)...\")\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] Venv not ready â€” installer will handle it\")\n",
        "        return\n",
        "    python_exe = str(env_marker)\n",
        "    sh(f'\"{python_exe}\" -m pip uninstall -y llama-cpp-python llama-cpp-python-cuda')\n",
        "    cpu_env = os.environ.copy()\n",
        "    cpu_env.update({\n",
        "        \"CMAKE_ARGS\": (\n",
        "            \"-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF -DLLAMA_METAL=OFF \"\n",
        "            \"-DLLAMA_OPENCL=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"\n",
        "        ),\n",
        "        \"FORCE_CMAKE\": \"1\", \"CUDACXX\": \"\",\n",
        "    })\n",
        "    result = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=cpu_env)\n",
        "    print(\"[âœ“] CPU install done\" if result.returncode == 0\n",
        "          else f\"[warn] CPU install code {result.returncode}\\n  FIX: Try deleting installer_files/ and re-running\")\n",
        "\n",
        "\n",
        "def install_llama_cpp_python_gpu():\n",
        "    print(\"\\nðŸ”§ Checking llama-cpp GPU support...\")\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] Venv not ready â€” installer will handle it\")\n",
        "        return\n",
        "    python_exe = str(env_marker)\n",
        "    check = sh(f'\"{python_exe}\" -m pip show llama-cpp-binaries')\n",
        "    if check.returncode == 0 and \"cu\" in check.stdout.lower():\n",
        "        print(\"[âœ“] llama-cpp-binaries (CUDA) already installed\")\n",
        "        return\n",
        "    pv      = sh(f'\"{python_exe}\" -c \"import sys; print(f\\'cp{{sys.version_info.major}}{{sys.version_info.minor}}\\')\"')\n",
        "    py_tag  = pv.stdout.strip() if pv.returncode == 0 else \"cp311\"\n",
        "    cv      = sh(\"nvcc --version\")\n",
        "    cuda_major, cuda_minor = \"12\", \"1\"\n",
        "    if cv.returncode == 0:\n",
        "        m = re.search(r'release (\\d+)\\.(\\d+)', cv.stdout)\n",
        "        if m:\n",
        "            cuda_major, cuda_minor = m.group(1), m.group(2)\n",
        "    cuda_tag = f\"cu{cuda_major}{cuda_minor}\"\n",
        "    print(f\"[info] Python={py_tag}  CUDA={cuda_tag}\")\n",
        "    result = sh(\n",
        "        f'\"{python_exe}\" -m pip install llama-cpp-binaries '\n",
        "        f'--extra-index-url https://abetlen.github.io/llama-cpp-python/whl/{cuda_tag} --no-cache-dir'\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(\"[âœ“] llama-cpp-binaries (CUDA) installed\")\n",
        "        return\n",
        "    for version in [\"0.3.2\", \"0.2.90\", \"0.2.79\"]:\n",
        "        wheel_url = (\n",
        "            f\"https://github.com/abetlen/llama-cpp-python/releases/download/\"\n",
        "            f\"v{version}/llama_cpp_python-{version}-{py_tag}-{py_tag}-linux_x86_64.whl\"\n",
        "        )\n",
        "        result = sh(f'\"{python_exe}\" -m pip install \"{wheel_url}\" --no-cache-dir')\n",
        "        if result.returncode == 0:\n",
        "            print(f\"[âœ“] llama-cpp-python v{version} installed\")\n",
        "            return\n",
        "    gpu_env = os.environ.copy()\n",
        "    gpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\", \"FORCE_CMAKE\": \"1\"})\n",
        "    result = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=gpu_env)\n",
        "    print(\"[âœ“] Compiled from source\" if result.returncode == 0\n",
        "          else \"[warn] All GPU attempts failed â€” llama.cpp will use CPU fallback\")\n",
        "\n",
        "\n",
        "def create_llama_cpp_binaries_wrapper():\n",
        "    print(\"\\nðŸ”§ Creating llama_cpp_binaries wrapper...\")\n",
        "    wrapper_code = '''\"\"\"Compatibility wrapper for llama_cpp_binaries.\"\"\"\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def get_binary_path():\n",
        "    search_paths = []\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        search_paths.append(Path(llama_cpp.__file__).parent / \"bin\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "    binary = shutil.which(\"llama-server\") or shutil.which(\"llama-cpp-server\")\n",
        "    if binary:\n",
        "        return binary\n",
        "    repo_dir = Path(__file__).parent.parent / \"repositories\" / \"llama.cpp\"\n",
        "    if repo_dir.exists():\n",
        "        search_paths += [repo_dir / \"build\" / \"bin\", repo_dir / \"build\", repo_dir]\n",
        "    installer_dir = Path(__file__).parent.parent / \"installer_files\"\n",
        "    if installer_dir.exists():\n",
        "        search_paths.append(installer_dir / \"env\" / \"bin\")\n",
        "    for sp in search_paths:\n",
        "        if not sp.exists():\n",
        "            continue\n",
        "        for name in [\"llama-server\", \"llama-cpp-server\", \"server\"]:\n",
        "            for ext in [\"\", \".exe\"]:\n",
        "                p = sp / f\"{name}{ext}\"\n",
        "                if p.exists() and (os.access(p, os.X_OK) or ext == \".exe\"):\n",
        "                    return str(p)\n",
        "    return \"PYTHON_SERVER\"\n",
        "\n",
        "def ensure_binary():\n",
        "    try:\n",
        "        return get_binary_path() is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "'''\n",
        "    modules_dir = WORK_DIR / \"modules\"\n",
        "    try:\n",
        "        modules_dir.mkdir(parents=True, exist_ok=True)\n",
        "        (modules_dir / \"llama_cpp_binaries.py\").write_text(wrapper_code, encoding=\"utf-8\")\n",
        "        print(\"[âœ“] llama_cpp_binaries.py created\")\n",
        "    except Exception as e:\n",
        "        print(f\"[error] wrapper: {e}\")\n",
        "\n",
        "\n",
        "def patch_gradio_launch():\n",
        "    server_py = WORK_DIR / \"server.py\"\n",
        "    if not server_py.exists():\n",
        "        return\n",
        "    try:\n",
        "        content = server_py.read_text(encoding=\"utf-8\")\n",
        "        if \".launch(\" in content and \"share=\" not in content:\n",
        "            content = re.sub(r\"\\.launch\\((.*?)\\)\", r\".launch(\\1, share=True)\", content)\n",
        "            server_py.write_text(content, encoding=\"utf-8\")\n",
        "            print(\"[âœ“] server.py patched for share=True\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] patch_gradio_launch: {e}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MODEL DOWNLOAD\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def download_model_if_missing():\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = models_dir / MODEL_FILE\n",
        "\n",
        "    if model_path.exists() and model_path.stat().st_size > (100 * 1024 * 1024):\n",
        "        size_gb = model_path.stat().st_size / (1024 ** 3)\n",
        "        print(f\"[âœ“] Model already exists ({size_gb:.1f} GB): {model_path}\")\n",
        "        return True\n",
        "\n",
        "    # RAM check before downloading\n",
        "    free_ram = get_free_ram_gb()\n",
        "    # Find expected size from menu\n",
        "    expected_gb = next((m[3] for m in MODEL_MENU if m[2] == MODEL_FILE), 0)\n",
        "    if expected_gb > 0 and free_ram < expected_gb + 1.0:\n",
        "        print(f\"\\nâš ï¸  WARNING: Model needs ~{expected_gb} GB but only {free_ram:.1f} GB RAM free.\")\n",
        "        print(\"   The model will download but may not load properly.\")\n",
        "        print(\"   Consider picking a smaller model from the menu.\\n\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ðŸ“¥ DOWNLOADING: {MODEL_FILE}\")\n",
        "    print(f\"   Repo : {MODEL_REPO}\")\n",
        "    print(f\"   Dest : {model_path}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    hf_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILE}?download=true\"\n",
        "\n",
        "    for cmd in (\n",
        "        f'wget -q --show-progress -O \"{model_path}\" \"{hf_url}\"',\n",
        "        f'curl -L --progress-bar -o \"{model_path}\" \"{hf_url}\"',\n",
        "    ):\n",
        "        tool = cmd.split()[0]\n",
        "        print(f\"[info] Trying {tool}...\")\n",
        "        result = subprocess.run(cmd, shell=True)\n",
        "        if (result.returncode == 0 and model_path.exists()\n",
        "                and model_path.stat().st_size > (100 * 1024 * 1024)):\n",
        "            size_gb = model_path.stat().st_size / (1024 ** 3)\n",
        "            print(f\"[âœ“] Download complete â€” {size_gb:.2f} GB\")\n",
        "            return True\n",
        "        print(f\"[warn] {tool} failed.\")\n",
        "        try:\n",
        "            model_path.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"[error] All download attempts failed.\")\n",
        "    print(f\"  Manual: download {MODEL_FILE} from https://huggingface.co/{MODEL_REPO}\")\n",
        "    print(f\"  Place it in: {models_dir}\")\n",
        "    return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  INTERACTIVE MENUS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def choose_mode():\n",
        "    global USE_GPU, GPU_LAYERS, N_CTX\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"  MY-AI-Gizmo  v2.0 â€” Choose Your Mode\")\n",
        "    print(\"=\" * 70)\n",
        "    total = get_total_ram_gb()\n",
        "    free  = get_free_ram_gb()\n",
        "    print(f\"  RAM: {free:.1f} GB free / {total:.1f} GB total\")\n",
        "    print(\"â”€\" * 70)\n",
        "    print(\"  [1]  GPU  â€” Faster, requires CUDA GPU (Colab T4/A100)\")\n",
        "    print(\"  [2]  CPU  â€” Works on any machine, uses your RAM\")\n",
        "    print(\"=\" * 70)\n",
        "    while True:\n",
        "        choice = input(\"\\n  Enter 1 for GPU or 2 for CPU: \").strip()\n",
        "        if choice == \"1\":\n",
        "            USE_GPU = True;  GPU_LAYERS = -1;  N_CTX = 4096\n",
        "            print(f\"  GPU mode â€” n_gpu_layers=-1, n_ctx=4096\")\n",
        "            break\n",
        "        elif choice == \"2\":\n",
        "            USE_GPU = False; GPU_LAYERS = 0\n",
        "            print(f\"  CPU mode â€” n_gpu_layers=0\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"  Please enter 1 or 2.\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "\n",
        "def choose_model():\n",
        "    \"\"\"Let user pick a model from the curated list or enter a custom one.\"\"\"\n",
        "    global MODEL_REPO, MODEL_FILE, N_CTX\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"  MODEL SELECTOR\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Show already-downloaded models first\n",
        "    local_models = list_local_models()\n",
        "    if local_models:\n",
        "        print(\"  â”€â”€ Already on your Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "        for i, m in enumerate(local_models, 1):\n",
        "            try:\n",
        "                size_gb = m.stat().st_size / (1024 ** 3)\n",
        "            except Exception:\n",
        "                size_gb = 0\n",
        "            print(f\"  [L{i}]  {m.name:<50}  {size_gb:.1f} GB  â† use this\")\n",
        "        print(\"\")\n",
        "\n",
        "    print(\"  â”€â”€ Download a new model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
        "    for m in MODEL_MENU:\n",
        "        print(f\"  {m[0]}\")\n",
        "\n",
        "    free_ram = get_free_ram_gb()\n",
        "    print(\"â”€\" * 70)\n",
        "    print(f\"  Free RAM: {free_ram:.1f} GB   |   Press Enter to keep default (Qwen2.5-Coder-14B)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"\\n  Your choice: \").strip()\n",
        "\n",
        "        # Use a local model\n",
        "        if choice.upper().startswith(\"L\") and local_models:\n",
        "            try:\n",
        "                idx = int(choice[1:]) - 1\n",
        "                selected = local_models[idx]\n",
        "                MODEL_FILE = selected.name\n",
        "                MODEL_REPO = \"\"\n",
        "                # Auto set ctx based on RAM\n",
        "                N_CTX = auto_ctx_size(selected.stat().st_size / (1024 ** 3))\n",
        "                print(f\"  âœ“ Using local: {MODEL_FILE}  (ctx auto-set to {N_CTX})\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"  Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Empty = keep default\n",
        "        if not choice:\n",
        "            N_CTX = auto_ctx_size(MODEL_MENU[4][3])  # Qwen 14B\n",
        "            print(f\"  âœ“ Keeping default: {MODEL_FILE}  (ctx auto-set to {N_CTX})\")\n",
        "            break\n",
        "\n",
        "        # Numbered menu choice\n",
        "        try:\n",
        "            idx = int(choice) - 1\n",
        "            if idx < 0 or idx >= len(MODEL_MENU):\n",
        "                raise ValueError()\n",
        "            entry = MODEL_MENU[idx]\n",
        "            if entry[1] == \"\":  # custom\n",
        "                MODEL_REPO = input(\"  HuggingFace repo (e.g. TheBloke/Mistral-7B-GGUF): \").strip()\n",
        "                MODEL_FILE = input(\"  Filename (e.g. mistral-7b-v0.1.Q4_K_M.gguf): \").strip()\n",
        "                N_CTX      = 2048\n",
        "            else:\n",
        "                MODEL_REPO, MODEL_FILE = entry[1], entry[2]\n",
        "                N_CTX = auto_ctx_size(entry[3])\n",
        "            print(f\"  âœ“ Selected: {MODEL_FILE}  (ctx auto-set to {N_CTX})\")\n",
        "            break\n",
        "        except ValueError:\n",
        "            print(\"  Invalid choice â€” enter a number, L+number, or press Enter.\")\n",
        "            continue\n",
        "\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MAIN\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"  MY-AI-Gizmo  v2.0  Universal Launcher\")\n",
        "print(\"  Mode   : DEBUG (verbose, full reasoning, no filtering)\")\n",
        "print(f\"  Time   : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# â”€â”€ Step 1: User picks mode (GPU/CPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "choose_mode()\n",
        "\n",
        "if USE_GPU:\n",
        "    gpu_check = sh(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\")\n",
        "    if gpu_check.returncode == 0:\n",
        "        print(f\"[âœ“] GPU: {gpu_check.stdout.strip()}\")\n",
        "    else:\n",
        "        print(\"[warn] nvidia-smi failed.\\n\"\n",
        "              \"  FIX: Runtime â†’ Change runtime type â†’ GPU in Colab menu\")\n",
        "\n",
        "# â”€â”€ Step 2: Mount Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ensure_dirs()\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        print(\"[info] Mounting Google Drive...\")\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[âœ“] Google Drive mounted\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Drive mount failed: {e}\\n\"\n",
        "              \"  FIX: Run drive.mount('/content/drive') in a separate cell first\")\n",
        "\n",
        "# â”€â”€ Step 3: Clean broken files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cleanup_broken_files()\n",
        "\n",
        "# â”€â”€ Step 4: Model manager (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "show_model_manager()\n",
        "\n",
        "# â”€â”€ Step 5: User picks model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "choose_model()\n",
        "\n",
        "# â”€â”€ Step 6: Download repo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if not download_repo_if_missing() and not WORK_DIR.exists():\n",
        "    raise SystemExit(\"Repository unavailable.\")\n",
        "\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "# â”€â”€ Step 7: Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ensure_symlinks_and_files()\n",
        "prepare_settings_file()\n",
        "write_cmd_flags()\n",
        "write_debug_character()\n",
        "write_model_loader_config()\n",
        "deploy_dual_model_extension()   # â† NEW: installs dual model tab\n",
        "\n",
        "# â”€â”€ Step 8: Download model if needed â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ“¥ Checking model...\")\n",
        "print_ram_status()\n",
        "print(\"=\" * 70)\n",
        "download_model_if_missing()\n",
        "\n",
        "# â”€â”€ Step 9: Install deps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MPL_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "start_sh      = WORK_DIR / \"start_linux.sh\"\n",
        "installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "env_marker    = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "\n",
        "install_env = os.environ.copy()\n",
        "if USE_GPU:\n",
        "    install_env.update({\n",
        "        \"MPLBACKEND\": \"Agg\", \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "        \"GPU_CHOICE\": \"A\", \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "        \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "        \"CMAKE_ARGS\": \"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\",\n",
        "        \"FORCE_CMAKE\": \"1\", \"SKIP_TORCH_TEST\": \"TRUE\", \"FORCE_CUDA\": \"TRUE\",\n",
        "    })\n",
        "    print(\"\\nðŸ“¦ Installing dependencies (GPU)...\")\n",
        "else:\n",
        "    install_env.update({\n",
        "        \"MPLBACKEND\": \"Agg\", \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "        \"GPU_CHOICE\": \"N\", \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "        \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "        \"CMAKE_ARGS\": \"-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF -DLLAMA_METAL=OFF\",\n",
        "        \"FORCE_CMAKE\": \"1\", \"CUDA_VISIBLE_DEVICES\": \"\", \"CUDACXX\": \"\",\n",
        "        \"SKIP_TORCH_TEST\": \"TRUE\", \"FORCE_CUDA\": \"FALSE\",\n",
        "    })\n",
        "    print(\"\\nðŸ“¦ Installing dependencies (CPU)...\")\n",
        "\n",
        "print(f\"Log â†’ {installer_log}\")\n",
        "\n",
        "if not start_sh.exists():\n",
        "    raise SystemExit(\"[error] start_linux.sh not found.\\n\"\n",
        "                     \"  FIX: Delete /content/text-generation-webui and re-run (repo will re-download)\")\n",
        "\n",
        "sh(\"chmod +x start_linux.sh\")\n",
        "\n",
        "if not env_marker.exists():\n",
        "    print(\"[info] First run â€” installing (5-10 min)...\")\n",
        "    code, _ = stream_with_heartbeat(\n",
        "        \"bash start_linux.sh\",\n",
        "        cwd=str(WORK_DIR), env=install_env, logfile_path=str(installer_log),\n",
        "    )\n",
        "    print(f\"[{'âœ“' if code == 0 else 'warn'}] Installer exited with code {code}\")\n",
        "    if code != 0:\n",
        "        print(\"  FIX: Delete installer_files/ folder and re-run this launcher\")\n",
        "else:\n",
        "    print(\"[info] Venv exists â€” skipping installer\")\n",
        "\n",
        "if USE_GPU:\n",
        "    install_llama_cpp_python_gpu()\n",
        "else:\n",
        "    install_llama_cpp_python_cpu()\n",
        "\n",
        "create_llama_cpp_binaries_wrapper()\n",
        "patch_gradio_launch()\n",
        "\n",
        "# â”€â”€ Step 10: Build launch wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "launch_wrapper = WORK_DIR / \"_launch_debug.py\"\n",
        "mode_label     = \"GPU\" if USE_GPU else \"CPU\"\n",
        "cuda_block     = \"\" if USE_GPU else \"\\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\"\n",
        "threads        = auto_thread_count()\n",
        "\n",
        "launch_code = f\"\"\"#!/usr/bin/env python3\n",
        "# Auto-generated DEBUG launcher â€” {mode_label}\n",
        "import sys, os\n",
        "{cuda_block}\n",
        "os.environ['MPLBACKEND']         = 'Agg'\n",
        "os.environ['MPLCONFIGDIR']       = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE']       = '1'\n",
        "\n",
        "flags = [\n",
        "    '--listen', '--share', '--verbose',\n",
        "    '--api', '--api-port', '5000',\n",
        "    '--loader', 'llama.cpp',\n",
        "    '--gpu-layers', '{GPU_LAYERS}',\n",
        "    '--ctx-size', '{N_CTX}',\n",
        "    '--batch-size', '512',\n",
        "    '--threads', '{threads}',\n",
        "    '--model', '{MODEL_FILE}',\n",
        "    '--extensions', 'dual_model',\n",
        "]\n",
        "for f in flags:\n",
        "    if f not in sys.argv:\n",
        "        sys.argv.append(f)\n",
        "\n",
        "print(\"[DEBUG LAUNCHER] {mode_label} | {MODEL_FILE} | verbose=ON | dual_model=ON\")\n",
        "print(\"[DEBUG LAUNCHER] flags:\", ' '.join(sys.argv[1:]))\n",
        "\n",
        "try:\n",
        "    import matplotlib; matplotlib.use('Agg', force=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import runpy\n",
        "runpy.run_path('server.py', run_name='__main__')\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    launch_wrapper.write_text(launch_code, encoding=\"utf-8\")\n",
        "    print(f\"[âœ“] Launch wrapper: {launch_wrapper}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] launch wrapper: {e}\")\n",
        "\n",
        "# â”€â”€ Step 11: Launch with auto-restart â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "time.sleep(2)\n",
        "\n",
        "python_exe  = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "launch_cmd  = f'{python_exe} -u \"{str(launch_wrapper)}\"'\n",
        "\n",
        "server_env  = os.environ.copy()\n",
        "server_env.update({\n",
        "    \"MPLBACKEND\": \"Agg\",\n",
        "    \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "    \"GRADIO_SERVER_NAME\": \"0.0.0.0\",\n",
        "    \"GRADIO_SHARE\": \"1\",\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"  LAUNCHING â€” DEBUG MODE â€” {mode_label}\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Model   : {MODEL_FILE}\")\n",
        "print(f\"  Char    : Debug  (verbose, no filtering)\")\n",
        "print(f\"  Threads : {threads}  (auto-detected)\")\n",
        "print(f\"  n_ctx   : {N_CTX}  (auto-sized to available RAM)\")\n",
        "print(f\"  GPU     : {'All layers on GPU (-1)' if USE_GPU else 'CPU only (0)'}\")\n",
        "print(f\"  API     : port 5000\")\n",
        "print(f\"  Ext     : dual_model (Model 2 tab)\")\n",
        "print(\"=\" * 70)\n",
        "print_ram_status()\n",
        "print(\"â³ Starting (1-2 min on first model load)...\\n\")\n",
        "\n",
        "# Auto-restart loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "captured = None\n",
        "for attempt in range(1, MAX_RESTARTS + 1):\n",
        "    server_log = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "    if attempt > 1:\n",
        "        print(f\"\\nðŸ”„ Auto-restart #{attempt - 1} of {MAX_RESTARTS - 1}...\\n\")\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Start resource monitor\n",
        "    monitor_stop = threading.Event()\n",
        "    monitor_thread = threading.Thread(\n",
        "        target=resource_monitor,\n",
        "        args=(monitor_stop, str(server_log)),\n",
        "        daemon=True\n",
        "    )\n",
        "    monitor_thread.start()\n",
        "\n",
        "    code, captured = stream_with_heartbeat(\n",
        "        launch_cmd,\n",
        "        cwd=str(WORK_DIR), env=server_env,\n",
        "        logfile_path=str(server_log),\n",
        "        capture_url_to=str(PUBLIC_URL_FILE),\n",
        "    )\n",
        "\n",
        "    monitor_stop.set()\n",
        "    monitor_thread.join(timeout=2)\n",
        "\n",
        "    # Exit conditions that should NOT restart\n",
        "    if code == 0:\n",
        "        print(\"[info] Server exited cleanly.\")\n",
        "        break\n",
        "    if code == -9:\n",
        "        print(\"[info] Server killed (manual stop or OOM).\")\n",
        "        break\n",
        "    if attempt < MAX_RESTARTS:\n",
        "        print(f\"[warn] Server crashed (code {code}) â€” will restart...\")\n",
        "    else:\n",
        "        print(f\"[warn] Server crashed {MAX_RESTARTS} times â€” giving up.\")\n",
        "\n",
        "# â”€â”€ Try ngrok if no public URL captured â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if not captured:\n",
        "    print(\"\\n[info] No Gradio public URL found â€” trying ngrok as fallback...\")\n",
        "    captured = try_setup_ngrok(port=7860)\n",
        "\n",
        "# Scan log for URL if still nothing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if not captured and server_log.exists():\n",
        "    print(\"[info] Scanning log for URL...\")\n",
        "    try:\n",
        "        log_text = server_log.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "        for pat in [\n",
        "            re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "            re.compile(r'Running on public URL:\\s*(https?://\\S+)',              re.IGNORECASE),\n",
        "            re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok[^\\s,)\\'\\\"]*)',        re.IGNORECASE),\n",
        "        ]:\n",
        "            m = pat.search(log_text)\n",
        "            if m:\n",
        "                captured = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                print(\"[âœ“] URL recovered from log\")\n",
        "                break\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if captured:\n",
        "    print(f\"  âœ… WEB UI READY!\")\n",
        "    print(f\"  PUBLIC URL : {captured}\")\n",
        "    print(\"=\" * 70)\n",
        "    try:\n",
        "        PUBLIC_URL_FILE.write_text(captured, encoding=\"utf-8\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"\\n  NEXT STEPS:\")\n",
        "    print(\"  1. Click the URL above\")\n",
        "    print(\"  2. Model tab â€” your model should be pre-selected â†’ click Load\")\n",
        "    print(\"  3. Chat tab â€” Debug character is active\")\n",
        "    print(\"  4. Dual Model tab â€” load a second model there\")\n",
        "    print(\"\\n  DEBUG TIPS:\")\n",
        "    print(\"  â€¢ --verbose = full prompts print here in the terminal\")\n",
        "    print(\"  â€¢ API: http://0.0.0.0:5000/v1\")\n",
        "    print(\"  â€¢ ngrok token â†’ save to Drive/MY-AI-Gizmo/ngrok_token.txt for auto-tunnel\")\n",
        "else:\n",
        "    print(\"  âŒ NO PUBLIC URL CAPTURED\")\n",
        "    print(\"=\" * 70)\n",
        "    if server_log.exists():\n",
        "        print(f\"\\n  Last 60 lines of {server_log.name}:\\n\")\n",
        "        try:\n",
        "            lines = server_log.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "            for line in lines[-60:]:\n",
        "                print(f\"    {line}\")\n",
        "        except Exception as e:\n",
        "            print(f\"    [could not read log: {e}]\")\n",
        "    print(\"\\n  COMMON FIXES:\")\n",
        "    print(\"  â€¢ ModuleNotFoundError    â†’ delete installer_files/ and re-run\")\n",
        "    print(\"  â€¢ Address in use         â†’ pkill -9 -f server.py\")\n",
        "    print(\"  â€¢ No Gradio URL          â†’ check Colab internet / runtime type\")\n",
        "    print(\"  â€¢ Model too big for RAM  â†’ pick a smaller model from the menu\")\n",
        "    print(\"  â€¢ OOM crash              â†’ restart runtime and pick a 7B model\")\n",
        "    if PUBLIC_URL_FILE.exists():\n",
        "        try:\n",
        "            saved = PUBLIC_URL_FILE.read_text().strip()\n",
        "            if saved:\n",
        "                print(f\"\\n  Previously saved URL: {saved}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "print(f\"\\n  Data    : {DRIVE_ROOT}\")\n",
        "print(f\"  Log     : {server_log}\")\n",
        "print_ram_status()\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… RECOMMENDED MODELS (COPY EXACTLY)\n",
        "ðŸ”¹ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "ðŸ”¹ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "ðŸ”¹ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "ðŸ”¹ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "ðŸ”¹ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "ðŸ”¹ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "âš™ï¸ WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}