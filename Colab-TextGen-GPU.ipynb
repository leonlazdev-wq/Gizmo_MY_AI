{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/Gizmo-my-ai-for-google-colab/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo â€¢ UNIVERSAL LAUNCHER  v3.4\n",
        "# ================================================================\n",
        "# v3.4 CHANGES:\n",
        "#  âœ… GitHub PAT prompt at startup â€” supports private repos\n",
        "#  âœ… New repo: Gizmo-my-ai-for-google-colab\n",
        "#  âœ… Gradio launch via runpy wrapper (like v3.1) â†’ gradio.live URL\n",
        "#     instead of direct server.py subprocess â†’ trycloudflare.com\n",
        "#  âœ… All v3.3 fixes kept:\n",
        "#      - Popen + stream output so URL prints in Colab cell\n",
        "#      - Skip llama-cpp reinstall if webui already installed it\n",
        "#      - Drive already-mounted check (no \"mountpoint\" error)\n",
        "#      - Both user_data/models AND models symlinked\n",
        "#      - No-model startup option [0]\n",
        "#      - Drive fallback to /content/MY-AI-Gizmo\n",
        "#      - model: None in settings / --model flag omitted when no model\n",
        "#  âœ… URL capture patterns: gradio.live + trycloudflare.com + ngrok\n",
        "#  âœ… Auto-restart loop (3Ã—)\n",
        "#  âœ… Dual Model | Google Workspace | Debug character | ï¼‹Toolbar\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# â”€â”€ Repo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "GITHUB_USER = \"gitleon8301\"\n",
        "GITHUB_REPO = \"Gizmo-my-ai-for-google-colab\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "REPO_FOLDER_GLOB = f\"{GITHUB_REPO}-*\"\n",
        "\n",
        "# These are set after token is collected\n",
        "REPO_ZIP = \"\"\n",
        "REPO_CLONE_URL = \"\"\n",
        "\n",
        "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "WORK_DIR           = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT         = None\n",
        "LOG_DIR            = None\n",
        "MPL_CONFIG_DIR     = None\n",
        "PUBLIC_URL_FILE    = None\n",
        "HEARTBEAT_INTERVAL = 30\n",
        "MAX_RESTARTS       = 3\n",
        "MONITOR_INTERVAL   = 60\n",
        "\n",
        "# â”€â”€ Model menu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "MODEL_MENU = [\n",
        "    (\"1  TinyLlama-1.1B  Q4_K_M  [~0.7 GB]  â† fastest\",\n",
        "     \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
        "     \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", 0.7),\n",
        "    (\"2  Phi-3-mini-4k   Q4_K_M  [~2.2 GB]  â† great quality/speed\",\n",
        "     \"bartowski/Phi-3-mini-4k-instruct-GGUF\",\n",
        "     \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\", 2.2),\n",
        "    (\"3  Mistral-7B-v0.3  Q4_K_M  [~4.4 GB]  â† best general 7B\",\n",
        "     \"bartowski/Mistral-7B-v0.3-GGUF\",\n",
        "     \"Mistral-7B-v0.3-Q4_K_M.gguf\", 4.4),\n",
        "    (\"4  Qwen2.5-Coder-7B  Q4_K_M  [~4.7 GB]  â† best coding 7B\",\n",
        "     \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-7b-instruct-q4_k_m.gguf\", 4.7),\n",
        "    (\"5  Qwen2.5-Coder-14B  Q4_K_M  [~8.9 GB]  â† needs 10+ GB RAM\",\n",
        "     \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\", 8.9),\n",
        "    (\"6  DeepSeek-Coder-33B  Q4_K_M  [~19 GB]  â† GPU only\",\n",
        "     \"TheBloke/deepseek-coder-33B-instruct-GGUF\",\n",
        "     \"deepseek-coder-33b-instruct.Q4_K_M.gguf\", 19.0),\n",
        "    (\"7  Custom â€” enter your own HF repo + filename\", \"\", \"\", 0),\n",
        "]\n",
        "\n",
        "# â”€â”€ Globals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "GITHUB_TOKEN = \"\"\n",
        "MODEL_REPO   = \"\"\n",
        "MODEL_FILE   = \"\"\n",
        "USE_MODEL    = False\n",
        "GPU_LAYERS   = -1\n",
        "N_CTX        = 4096\n",
        "USE_GPU      = True\n",
        "\n",
        "# â”€â”€ URL patterns (catches gradio.live AND trycloudflare.com) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "URL_PATTERNS = [\n",
        "    re.compile(r'Running on public URL:\\s*(https?://\\S+)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(?:public|share|tunnel|external)[^\\n]{0,40}(https?://\\S+)', re.IGNORECASE),\n",
        "]\n",
        "URL_KEYWORDS = (\"gradio.live\", \"trycloudflare.com\", \"ngrok\", \"loca.lt\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  â˜…â˜…â˜…  GITHUB TOKEN SETUP â€” runs before EVERYTHING else  â˜…â˜…â˜…\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def setup_github_token():\n",
        "    \"\"\"\n",
        "    Prompts for a GitHub Personal Access Token at startup.\n",
        "    The token is used to clone / download the private repo.\n",
        "\n",
        "    How to create a token:\n",
        "      GitHub â†’ Settings â†’ Developer Settings\n",
        "        â†’ Personal Access Tokens â†’ Tokens (classic)\n",
        "        â†’ Generate new token (classic)\n",
        "        â†’ Scope: check  [âœ“] repo\n",
        "        â†’ Copy the token that starts with  ghp_...\n",
        "    \"\"\"\n",
        "    global GITHUB_TOKEN, REPO_ZIP, REPO_CLONE_URL\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"  MY-AI-Gizmo  v3.4  â€” GitHub Authentication\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    print(\"  Your repo is PRIVATE. A Personal Access Token is required.\")\n",
        "    print()\n",
        "    print(\"  â”Œâ”€ How to get a token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "    print(\"  â”‚  1. Go to https://github.com/settings/tokens        â”‚\")\n",
        "    print(\"  â”‚  2. Personal Access Tokens â†’ Tokens (classic)                â”‚\")\n",
        "    print(\"  â”‚  3. Generate new token (classic)                              â”‚\")\n",
        "    print(\"  â”‚  4. Set scope: âœ“ repo   (full control of private repos)      â”‚\")\n",
        "    print(\"  â”‚  5. Copy the token  (starts with  ghp_...)                   â”‚\")\n",
        "    print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        token = input(\"  Paste your GitHub token here: \").strip()\n",
        "        if not token:\n",
        "            print(\"  [!] Token cannot be empty. Try again.\")\n",
        "            continue\n",
        "        if not (token.startswith(\"ghp_\") or token.startswith(\"github_pat_\") or len(token) >= 20):\n",
        "            confirm = input(\"  [?] Token looks unusual. Continue anyway? (y/n): \").strip().lower()\n",
        "            if confirm != \"y\":\n",
        "                continue\n",
        "        GITHUB_TOKEN = token\n",
        "        break\n",
        "\n",
        "    # Build authenticated URLs\n",
        "    REPO_ZIP       = (f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}\"\n",
        "                      f\"/archive/refs/heads/{GITHUB_BRANCH}.zip\")\n",
        "    REPO_CLONE_URL = (f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\")\n",
        "\n",
        "    print()\n",
        "    print(f\"  [âœ“] Token accepted â€” will authenticate as: {GITHUB_TOKEN[:4]}{'*' * (len(GITHUB_TOKEN)-8)}{GITHUB_TOKEN[-4:]}\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  DRIVE SETUP\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def mount_drive_if_needed():\n",
        "    if not IN_COLAB:\n",
        "        return False\n",
        "    if Path(\"/content/drive/MyDrive\").exists():\n",
        "        print(\"[info] Google Drive already mounted â€” skipping re-mount.\")\n",
        "        return True\n",
        "    try:\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[âœ“] Google Drive mounted\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Drive mount failed ({e}) â€” using local storage\")\n",
        "        return False\n",
        "\n",
        "def setup_drive_root(drive_ok: bool):\n",
        "    global DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, PUBLIC_URL_FILE\n",
        "    DRIVE_ROOT      = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\") if drive_ok \\\n",
        "                      else Path(\"/content/MY-AI-Gizmo\")\n",
        "    LOG_DIR         = DRIVE_ROOT / \"logs\"\n",
        "    MPL_CONFIG_DIR  = DRIVE_ROOT / \"matplotlib\"\n",
        "    PUBLIC_URL_FILE = DRIVE_ROOT / \"public_url.txt\"\n",
        "    if not drive_ok:\n",
        "        print(f\"[info] Local storage: {DRIVE_ROOT}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  UTILITIES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def sh(cmd, cwd=None, env=None):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env,\n",
        "                          capture_output=True, text=True)\n",
        "\n",
        "def get_free_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemAvailable\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception: pass\n",
        "    return 0.0\n",
        "\n",
        "def get_total_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemTotal\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception: pass\n",
        "    return 0.0\n",
        "\n",
        "def auto_thread_count():\n",
        "    try:\n",
        "        import multiprocessing\n",
        "        return max(1, min(multiprocessing.cpu_count() - 1, 4))\n",
        "    except Exception: return 2\n",
        "\n",
        "def auto_ctx_size(model_gb):\n",
        "    free = get_free_ram_gb() - model_gb - 0.5\n",
        "    if free >= 2.0: return 4096\n",
        "    if free >= 1.0: return 2048\n",
        "    if free >= 0.5: return 1024\n",
        "    return 512\n",
        "\n",
        "def print_ram_status():\n",
        "    free  = get_free_ram_gb()\n",
        "    total = get_total_ram_gb()\n",
        "    used  = total - free\n",
        "    pct   = (used / total) if total else 0\n",
        "    bar   = \"â–ˆ\" * int(pct * 20) + \"â–‘\" * (20 - int(pct * 20))\n",
        "    print(f\"  RAM [{bar}]  {used:.1f}/{total:.1f} GB  ({free:.1f} GB free)\")\n",
        "\n",
        "def list_local_models():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists(): return []\n",
        "    found = []\n",
        "    for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]:\n",
        "        found.extend(d.rglob(ext))\n",
        "    return sorted(found)\n",
        "\n",
        "def cleanup_broken_files():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists(): return\n",
        "    broken = [f for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]\n",
        "              for f in d.rglob(ext) if f.stat().st_size < 100 * 1024]\n",
        "    if broken:\n",
        "        print(f\"[info] Removing {len(broken)} broken model file(s)\")\n",
        "        for f in broken:\n",
        "            try: f.unlink()\n",
        "            except Exception: pass\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  STREAM + HEARTBEAT  (for installer only â€” server uses its own loop)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def stream_with_heartbeat(cmd, cwd=None, env=None, logfile_path=None):\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.STDOUT, cwd=cwd, env=env,\n",
        "                            text=True, bufsize=1)\n",
        "    stop    = threading.Event()\n",
        "    last_t  = time.time()\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_t >= HEARTBEAT_INTERVAL:\n",
        "                print(\"[heartbeat] still working...\")\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "    logfile = open(logfile_path, \"a\", encoding=\"utf-8\") if logfile_path else None\n",
        "\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            last_t = time.time()\n",
        "            print(line, end=\"\")\n",
        "            if logfile:\n",
        "                try: logfile.write(line)\n",
        "                except Exception: pass\n",
        "    except Exception as e:\n",
        "        print(f\"[stream error] {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop.set()\n",
        "        hb.join(timeout=1)\n",
        "        if logfile:\n",
        "            try: logfile.close()\n",
        "            except Exception: pass\n",
        "    return proc.returncode\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  SYMLINKS  (both user_data/models AND models â†’ Drive/models)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def ensure_symlinks_and_files():\n",
        "    links_map = [\n",
        "        (\"user_data/models\",        \"models\",               False),\n",
        "        (\"models\",                  \"models\",               False),\n",
        "        (\"user_data/loras\",         \"loras\",                False),\n",
        "        (\"user_data/characters\",    \"characters\",           False),\n",
        "        (\"user_data/presets\",       \"presets\",              False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\",          \"chat-history\",         False),\n",
        "        (\"outputs\",                 \"outputs\",              False),\n",
        "    ]\n",
        "    for local_rel, drive_rel, is_file in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_rel\n",
        "        local_path = WORK_DIR / local_rel\n",
        "        if is_file:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            if local_path.is_symlink() or local_path.is_file():\n",
        "                local_path.unlink()\n",
        "            elif local_path.is_dir():\n",
        "                shutil.rmtree(local_path)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] clearing {local_path}: {e}\")\n",
        "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            os.symlink(str(drive_path), str(local_path),\n",
        "                       target_is_directory=not is_file)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] symlink {local_path}: {e}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  CONFIG FILES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def write_settings_yaml():\n",
        "    threads    = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_line = f\"model: {MODEL_FILE}\" if (USE_MODEL and MODEL_FILE) else \"model: None\"\n",
        "    content = f\"\"\"# MY-AI-Gizmo Settings â€” {mode_label} (v3.4 {datetime.now().strftime('%Y-%m-%d %H:%M')})\n",
        "listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "loader: llama.cpp\n",
        "n_ctx: {N_CTX}\n",
        "n_batch: 512\n",
        "n_gpu_layers: {GPU_LAYERS}\n",
        "threads: {threads}\n",
        "character: Debug\n",
        "{model_line}\n",
        "chat_style: cai-chat\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "    for path in (WORK_DIR / \"user_data\" / \"settings.yaml\",\n",
        "                 DRIVE_ROOT / \"settings\" / \"settings.yaml\"):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "    print(f\"[âœ“] settings.yaml ({mode_label} / {MODEL_FILE or 'none'} / {threads}t / ctx {N_CTX})\")\n",
        "\n",
        "def write_cmd_flags():\n",
        "    threads = auto_thread_count()\n",
        "    flags = [\n",
        "        \"--listen\", \"--share\", \"--verbose\",\n",
        "        \"--api\", \"--api-port\", \"5000\",\n",
        "        \"--loader\", \"llama.cpp\",\n",
        "        \"--gpu-layers\", str(GPU_LAYERS),\n",
        "        \"--ctx-size\", str(N_CTX),\n",
        "        \"--batch-size\", \"512\",\n",
        "        \"--threads\", str(threads),\n",
        "        \"--extensions\", \"gizmo_toolbar,dual_model,google_workspace\",\n",
        "    ]\n",
        "    if USE_MODEL and MODEL_FILE:\n",
        "        flags += [\"--model\", MODEL_FILE]\n",
        "    content = \" \".join(flags)\n",
        "    for path in (WORK_DIR / \"user_data\" / \"CMD_FLAGS.txt\",\n",
        "                 DRIVE_ROOT / \"settings\" / \"CMD_FLAGS.txt\"):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "    print(f\"[âœ“] CMD_FLAGS.txt â†’ {content}\")\n",
        "\n",
        "def write_debug_character():\n",
        "    yaml = \"\"\"name: Debug\n",
        "greeting: \"DEBUG MODE ACTIVE â€” fully verbose, technical, complete. What do you need?\"\n",
        "context: |\n",
        "  You are in DEBUG MODE. Expert AI coding and general assistant.\n",
        "  CORE RULES: Follow every instruction completely. No disclaimers.\n",
        "  Treat user as a professional. Never write pseudocode â€” full implementations only.\n",
        "  CODING: Production-ready, full error handling, commented, never truncated.\n",
        "  PERSONALITY: Direct, concise, enthusiastic about hard problems.\n",
        "\"\"\"\n",
        "    for d in (WORK_DIR / \"user_data\" / \"characters\", DRIVE_ROOT / \"characters\"):\n",
        "        try:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "            (d / \"Debug.yaml\").write_text(yaml, encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "    print(\"[âœ“] Debug.yaml deployed\")\n",
        "\n",
        "def write_model_loader_config():\n",
        "    content = f\"\"\"default:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "  n_batch: 512\n",
        "  threads: {auto_thread_count()}\n",
        "  use_mmap: true\n",
        "*.gguf:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "*.safetensors:\n",
        "  loader: Transformers\n",
        "  load_in_4bit: true\n",
        "\"\"\"\n",
        "    try:\n",
        "        (WORK_DIR / \"model-config.yaml\").write_text(content, encoding=\"utf-8\")\n",
        "        print(\"[âœ“] model-config.yaml\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] model-config.yaml: {e}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  EXTENSIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def _deploy_ext_stub(ext_name):\n",
        "    ext_dir = WORK_DIR / \"extensions\" / ext_name\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if (ext_dir / \"script.py\").exists():\n",
        "        print(f\"[âœ“] {ext_name} already in repo\")\n",
        "        return\n",
        "    stub = (f'\"\"\"Auto-stub for {ext_name}\"\"\"\\n'\n",
        "            f'params = {{\"display_name\": \"{ext_name}\", \"is_tab\": True}}\\n'\n",
        "            f'def ui():\\n'\n",
        "            f'    import gradio as gr\\n'\n",
        "            f'    gr.Markdown(\"## {ext_name}\\\\nUpload full extension from GitHub.\")\\n')\n",
        "    (ext_dir / \"script.py\").write_text(stub, encoding=\"utf-8\")\n",
        "    print(f\"[âœ“] {ext_name} stub deployed\")\n",
        "\n",
        "def deploy_dual_model_extension():\n",
        "    ext_dir = WORK_DIR / \"extensions\" / \"dual_model\"\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if (ext_dir / \"script.py\").exists():\n",
        "        print(\"[âœ“] dual_model already exists\")\n",
        "        return\n",
        "    script = r'''\"\"\"MY-AI-Gizmo â€” Dual Model Extension\"\"\"\n",
        "import gc, threading, gradio as gr\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    LLAMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LLAMA_AVAILABLE = False\n",
        "\n",
        "params = {\"display_name\": \"Dual Model\", \"is_tab\": True}\n",
        "_lock = threading.Lock(); _model2 = None; _model2_name = \"Not loaded\"\n",
        "\n",
        "def _load(path, ctx, threads, gpu):\n",
        "    global _model2, _model2_name\n",
        "    path = path.strip()\n",
        "    if not path: return \"âŒ Enter a path.\"\n",
        "    with _lock:\n",
        "        if _model2: _model2 = None; gc.collect()\n",
        "        try:\n",
        "            _model2 = Llama(model_path=path, n_ctx=int(ctx), n_threads=int(threads),\n",
        "                            n_gpu_layers=int(gpu), verbose=False)\n",
        "            _model2_name = path.split(\"/\")[-1]\n",
        "            return f\"âœ… Loaded: {_model2_name}\"\n",
        "        except Exception as e:\n",
        "            _model2 = None; _model2_name = \"Not loaded\"; return f\"âŒ {e}\"\n",
        "\n",
        "def _unload():\n",
        "    global _model2, _model2_name\n",
        "    with _lock:\n",
        "        if not _model2: return \"â„¹ï¸ Not loaded.\"\n",
        "        _model2 = None; _model2_name = \"Not loaded\"; gc.collect()\n",
        "    return \"ğŸ—‘ï¸ Unloaded.\"\n",
        "\n",
        "def _infer(p, mt, t):\n",
        "    if not _model2: return \"âŒ Not loaded.\"\n",
        "    with _lock: r = _model2(p, max_tokens=int(mt), temperature=float(t), echo=False)\n",
        "    return r[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "def _status(): return f\"ğŸŸ¢ {_model2_name}\" if _model2 else \"ğŸ”´ Not loaded\"\n",
        "\n",
        "def _api(prompt, mt, t):\n",
        "    try:\n",
        "        import urllib.request, json\n",
        "        payload = json.dumps({\"model\":\"gpt-3.5-turbo\",\"messages\":[{\"role\":\"user\",\"content\":prompt}],\n",
        "                              \"max_tokens\":int(mt),\"temperature\":float(t)}).encode()\n",
        "        req = urllib.request.Request(\"http://127.0.0.1:5000/v1/chat/completions\",\n",
        "              data=payload, headers={\"Content-Type\":\"application/json\"}, method=\"POST\")\n",
        "        with urllib.request.urlopen(req, timeout=120) as r:\n",
        "            return json.loads(r.read())[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "    except Exception: return None\n",
        "\n",
        "def _m2(msg, hist, mt, t): return hist+[[msg, _infer(msg,mt,t)]], \"\"\n",
        "def _pipe(msg, hist, mt, t, inst, _s):\n",
        "    m1 = _api(msg,mt,t) or \"[M1 unavailable]\"\n",
        "    m2 = _infer(f\"{inst}\\n\\nQ: {msg}\\n\\nDraft:\\n{m1}\\n\\nImproved:\", mt, t)\n",
        "    return hist+[[msg, f\"**[M1]**\\n{m1}\\n\\n---\\n**[M2 â€” {_model2_name}]**\\n{m2}\"]], \"\"\n",
        "def _debate(msg, hist, mt, t):\n",
        "    m1 = _api(msg,mt,t) or \"[M1 unavailable]\"\n",
        "    m2 = _infer(msg,mt,t)\n",
        "    return hist+[[msg, f\"**[M1]**\\n{m1}\\n\\n---\\n**[M2]**\\n{m2}\"]], \"\"\n",
        "\n",
        "def ui():\n",
        "    if not LLAMA_AVAILABLE:\n",
        "        gr.Markdown(\"âš ï¸ llama-cpp-python not installed.\"); return\n",
        "    gr.Markdown(\"## ğŸ¤– Dual Model\")\n",
        "    sb = gr.Textbox(value=_status(), label=\"Status\", interactive=False)\n",
        "    gr.Button(\"ğŸ”„ Refresh\", size=\"sm\").click(fn=_status, outputs=sb)\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=3): mp = gr.Textbox(label=\"Model path (.gguf)\")\n",
        "        with gr.Column(scale=1):\n",
        "            cs = gr.Slider(256,8192,2048,256,label=\"Context\")\n",
        "            ts = gr.Slider(1,8,2,1,label=\"Threads\")\n",
        "            gs = gr.Slider(0,100,0,1,label=\"GPU layers\")\n",
        "    rb = gr.Textbox(label=\"\", interactive=False)\n",
        "    with gr.Row():\n",
        "        gr.Button(\"â¬†ï¸ Load\", variant=\"primary\").click(fn=_load, inputs=[mp,cs,ts,gs], outputs=rb).then(fn=_status, outputs=sb)\n",
        "        gr.Button(\"ğŸ—‘ï¸ Unload\", variant=\"stop\").click(fn=_unload, outputs=rb).then(fn=_status, outputs=sb)\n",
        "    with gr.Row(): mt=gr.Slider(64,2048,512,64,label=\"Max tokens\"); t=gr.Slider(0,1.5,0.7,0.05,label=\"Temp\")\n",
        "    with gr.Tab(\"ğŸ’¬ Solo\"):\n",
        "        cb=gr.Chatbot(height=400); i=gr.Textbox()\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Send â¤\",variant=\"primary\").click(fn=_m2,inputs=[i,cb,mt,t],outputs=[cb,i])\n",
        "            gr.Button(\"ğŸ—‘ Clear\",size=\"sm\").click(fn=lambda:([],\"\"),outputs=[cb,i])\n",
        "        i.submit(fn=_m2,inputs=[i,cb,mt,t],outputs=[cb,i])\n",
        "    with gr.Tab(\"ğŸ”— Pipeline\"):\n",
        "        inst=gr.Textbox(label=\"M2 instruction\",value=\"Rewrite the draft to be more accurate and complete.\",lines=2)\n",
        "        cbp=gr.Chatbot(height=400); ip=gr.Textbox(); st=gr.State({})\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Run â¤\",variant=\"primary\").click(fn=_pipe,inputs=[ip,cbp,mt,t,inst,st],outputs=[cbp,ip])\n",
        "            gr.Button(\"ğŸ—‘ Clear\",size=\"sm\").click(fn=lambda:([],\"\"),outputs=[cbp,ip])\n",
        "        ip.submit(fn=_pipe,inputs=[ip,cbp,mt,t,inst,st],outputs=[cbp,ip])\n",
        "    with gr.Tab(\"âš”ï¸ Debate\"):\n",
        "        cbd=gr.Chatbot(height=400); id_=gr.Textbox()\n",
        "        with gr.Row():\n",
        "            gr.Button(\"Ask Both â¤\",variant=\"primary\").click(fn=_debate,inputs=[id_,cbd,mt,t],outputs=[cbd,id_])\n",
        "            gr.Button(\"ğŸ—‘ Clear\",size=\"sm\").click(fn=lambda:([],\"\"),outputs=[cbd,id_])\n",
        "        id_.submit(fn=_debate,inputs=[id_,cbd,mt,t],outputs=[cbd,id_])\n",
        "'''\n",
        "    (ext_dir / \"script.py\").write_text(script, encoding=\"utf-8\")\n",
        "    print(\"[âœ“] dual_model deployed\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  LLAMA-CPP\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def install_llama_cpp_gpu(python_exe):\n",
        "    print(\"\\nğŸ”§ Checking llama-cpp GPU...\")\n",
        "    cv = sh(\"nvcc --version\")\n",
        "    cuda_major, cuda_minor = \"12\", \"1\"\n",
        "    if cv.returncode == 0:\n",
        "        m = re.search(r'release (\\d+)\\.(\\d+)', cv.stdout)\n",
        "        if m: cuda_major, cuda_minor = m.group(1), m.group(2)\n",
        "    cuda_tag = f\"cu{cuda_major}{cuda_minor}\"\n",
        "    r = sh(f'\"{python_exe}\" -m pip install llama-cpp-binaries '\n",
        "           f'--extra-index-url https://abetlen.github.io/llama-cpp-python/whl/{cuda_tag} --no-cache-dir')\n",
        "    if r.returncode == 0:\n",
        "        print(\"[âœ“] llama-cpp-binaries (CUDA) installed\"); return\n",
        "    gpu_env = os.environ.copy()\n",
        "    gpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\", \"FORCE_CMAKE\": \"1\"})\n",
        "    r = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=gpu_env)\n",
        "    print(\"[âœ“] Compiled from source\" if r.returncode == 0 else \"[warn] GPU llama-cpp failed\")\n",
        "\n",
        "def install_llama_cpp_cpu(python_exe):\n",
        "    print(\"\\nğŸ”§ Installing llama-cpp (CPU)...\")\n",
        "    sh(f'\"{python_exe}\" -m pip uninstall -y llama-cpp-python llama-cpp-python-cuda')\n",
        "    cpu_env = os.environ.copy()\n",
        "    cpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUDA=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\",\n",
        "                    \"FORCE_CMAKE\": \"1\", \"CUDACXX\": \"\"})\n",
        "    r = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=cpu_env)\n",
        "    print(\"[âœ“] CPU llama-cpp done\" if r.returncode == 0 else f\"[warn] code {r.returncode}\")\n",
        "\n",
        "def create_llama_cpp_wrapper(python_exe):\n",
        "    wrapper = '''\"\"\"Compatibility wrapper for llama_cpp_binaries.\"\"\"\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "def get_binary_path():\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        p = Path(llama_cpp.__file__).parent / \"bin\" / \"llama-server\"\n",
        "        if p.exists(): return str(p)\n",
        "    except ImportError: pass\n",
        "    b = shutil.which(\"llama-server\")\n",
        "    return b or \"PYTHON_SERVER\"\n",
        "def ensure_binary():\n",
        "    try: return get_binary_path() is not None\n",
        "    except Exception: return False\n",
        "'''\n",
        "    try:\n",
        "        modules = WORK_DIR / \"modules\"\n",
        "        modules.mkdir(parents=True, exist_ok=True)\n",
        "        (modules / \"llama_cpp_binaries.py\").write_text(wrapper, encoding=\"utf-8\")\n",
        "        print(\"[âœ“] llama_cpp_binaries.py created\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] wrapper: {e}\")\n",
        "\n",
        "def install_google_workspace_deps(python_exe):\n",
        "    pkgs = \"google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\"\n",
        "    print(\"\\nğŸ”§ Installing Google Workspace libs...\")\n",
        "    r = sh(f'\"{python_exe}\" -m pip install {pkgs} -q')\n",
        "    print(\"[âœ“] Google libs installed\" if r.returncode == 0 else f\"[warn] code {r.returncode}\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MODEL DOWNLOAD\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def download_model_if_missing():\n",
        "    if not USE_MODEL:\n",
        "        print(\"[info] No model selected â€” skipping download\")\n",
        "        return True\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = models_dir / MODEL_FILE\n",
        "    if model_path.exists() and model_path.stat().st_size > 100 * 1024 * 1024:\n",
        "        print(f\"[âœ“] Model exists ({model_path.stat().st_size/(1024**3):.1f} GB)\")\n",
        "        return True\n",
        "    if not MODEL_REPO:\n",
        "        return model_path.exists()\n",
        "    print(f\"\\nğŸ“¥ DOWNLOADING: {MODEL_FILE}\")\n",
        "    hf_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILE}?download=true\"\n",
        "    for cmd in (f'wget -q --show-progress -O \"{model_path}\" \"{hf_url}\"',\n",
        "                f'curl -L --progress-bar -o \"{model_path}\" \"{hf_url}\"'):\n",
        "        r = subprocess.run(cmd, shell=True)\n",
        "        if r.returncode == 0 and model_path.exists() and model_path.stat().st_size > 100*1024*1024:\n",
        "            print(f\"[âœ“] Download complete â€” {model_path.stat().st_size/(1024**3):.2f} GB\")\n",
        "            return True\n",
        "        try: model_path.unlink()\n",
        "        except Exception: pass\n",
        "    print(\"[error] Download failed.\"); return False\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  REPO DOWNLOAD  (uses authenticated git clone â†’ private repo support)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def download_repo_if_missing():\n",
        "    if WORK_DIR.exists():\n",
        "        print(f\"[info] WORK_DIR exists: {WORK_DIR}\")\n",
        "        return True\n",
        "\n",
        "    print(\"[info] Cloning repository (authenticated)...\")\n",
        "\n",
        "    # â”€â”€ Try git clone first (cleanest for private repos) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    r = sh(f\"git clone --depth=1 {REPO_CLONE_URL} {WORK_DIR}\")\n",
        "    if r.returncode == 0 and WORK_DIR.exists():\n",
        "        print(f\"[âœ“] Repo cloned to {WORK_DIR}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"[warn] git clone failed (code {r.returncode}): {r.stderr.strip()}\")\n",
        "    print(\"[info] Trying zip download fallback...\")\n",
        "\n",
        "    # â”€â”€ Fallback: authenticated zip download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try: tmp_zip.unlink()\n",
        "    except Exception: pass\n",
        "\n",
        "    for cmd in (f\"wget -q -O {tmp_zip} '{REPO_ZIP}'\",\n",
        "                f\"curl -s -L -o {tmp_zip} '{REPO_ZIP}'\"):\n",
        "        r = sh(cmd)\n",
        "        if r.returncode == 0 and tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            break\n",
        "    else:\n",
        "        print(\"[error] Zip download also failed.\")\n",
        "        print(\"        â€¢ Double-check your token has  repo  scope\")\n",
        "        print(\"        â€¢ Make sure the repo name is correct\")\n",
        "        return False\n",
        "\n",
        "    sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "    found = next(Path(\"/content\").glob(REPO_FOLDER_GLOB), None)\n",
        "    if not found:\n",
        "        print(\"[error] Extracted folder not found.\"); return False\n",
        "    found.rename(WORK_DIR)\n",
        "    print(f\"[info] Repo extracted to {WORK_DIR}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  NGROK FALLBACK\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def try_setup_ngrok(port=7860):\n",
        "    try:\n",
        "        sh(\"pip install pyngrok -q\")\n",
        "        from pyngrok import ngrok, conf\n",
        "        token_file = DRIVE_ROOT / \"ngrok_token.txt\"\n",
        "        if token_file.exists():\n",
        "            token = token_file.read_text().strip()\n",
        "            if token: conf.get_default().auth_token = token\n",
        "        url = ngrok.connect(port, \"http\").public_url\n",
        "        print(f\"\\n{'='*70}\\nğŸŒ NGROK URL: {url}\\n{'='*70}\\n\")\n",
        "        try: PUBLIC_URL_FILE.write_text(url)\n",
        "        except Exception: pass\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  INTERACTIVE MENUS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def choose_mode():\n",
        "    global USE_GPU, GPU_LAYERS, N_CTX\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  MY-AI-Gizmo  v3.4 â€” Choose Mode\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"  RAM: {get_free_ram_gb():.1f} GB free / {get_total_ram_gb():.1f} GB total\")\n",
        "    print(\"  [1]  GPU  â€” CUDA required (Colab T4/A100)\")\n",
        "    print(\"  [2]  CPU  â€” Works everywhere, slower\")\n",
        "    print(\"=\"*70)\n",
        "    while True:\n",
        "        c = input(\"  1=GPU or 2=CPU: \").strip()\n",
        "        if c == \"1\":  USE_GPU = True;  GPU_LAYERS = -1; N_CTX = 4096; break\n",
        "        elif c == \"2\": USE_GPU = False; GPU_LAYERS = 0;  break\n",
        "        else: print(\"  Enter 1 or 2.\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "def show_model_manager():\n",
        "    models = list_local_models()\n",
        "    if not models: return\n",
        "    print(\"\\n\" + \"â”€\"*70)\n",
        "    print(\"  MODEL MANAGER â€” files in your storage\")\n",
        "    print(\"â”€\"*70)\n",
        "    for i, m in enumerate(models, 1):\n",
        "        try:   size = f\"{m.stat().st_size/(1024**3):.2f} GB\"\n",
        "        except Exception: size = \"?\"\n",
        "        print(f\"  [D{i}]  {m.name:<55} {size}\")\n",
        "    print(\"â”€\"*70)\n",
        "    print(\"  Type D1, D2... to delete a model, or Enter to continue\")\n",
        "    while True:\n",
        "        c = input(\"\\n  Choice: \").strip()\n",
        "        if not c: break\n",
        "        if c.upper().startswith(\"D\") and len(c) > 1:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                if 0 <= idx < len(models):\n",
        "                    confirm = input(f\"  Delete {models[idx].name}? (y/n): \").strip().lower()\n",
        "                    if confirm == \"y\":\n",
        "                        models[idx].unlink()\n",
        "                        print(\"  [âœ“] Deleted\")\n",
        "                        models = list_local_models()\n",
        "                else: print(\"  Invalid number.\")\n",
        "            except Exception as e: print(f\"  Error: {e}\")\n",
        "        else: print(\"  Use D1, D2... to delete, or Enter to continue.\")\n",
        "\n",
        "def choose_model():\n",
        "    global MODEL_REPO, MODEL_FILE, N_CTX, USE_MODEL\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  MODEL SELECTOR\")\n",
        "    print(\"=\"*70)\n",
        "    local = list_local_models()\n",
        "    if local:\n",
        "        print(\"  â”€â”€ On your storage â”€â”€\")\n",
        "        for i, m in enumerate(local, 1):\n",
        "            try:   size = f\"{m.stat().st_size/(1024**3):.1f} GB\"\n",
        "            except Exception: size = \"?\"\n",
        "            print(f\"  [L{i}]  {m.name}  ({size})\")\n",
        "        print()\n",
        "    print(\"  â”€â”€ Download new â”€â”€\")\n",
        "    for entry in MODEL_MENU:\n",
        "        print(f\"  {entry[0]}\")\n",
        "    print(f\"\\n  Free RAM: {get_free_ram_gb():.1f} GB\")\n",
        "    print(\"  [0]  START WITHOUT ANY MODEL  (load from UI later)\")\n",
        "    print(\"  Enter = use first local model (or download Qwen2.5-Coder-14B)\")\n",
        "    print(\"=\"*70)\n",
        "    while True:\n",
        "        c = input(\"  Choice: \").strip()\n",
        "        if c == \"0\":\n",
        "            USE_MODEL = False; MODEL_FILE = \"\"; MODEL_REPO = \"\"; N_CTX = 4096\n",
        "            print(\"  âœ“ Starting without a model\"); break\n",
        "        if c.upper().startswith(\"L\") and local:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                if 0 <= idx < len(local):\n",
        "                    sel = local[idx]; USE_MODEL = True\n",
        "                    MODEL_FILE = sel.name; MODEL_REPO = \"\"\n",
        "                    N_CTX = auto_ctx_size(sel.stat().st_size/(1024**3))\n",
        "                    print(f\"  âœ“ {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "                else: print(\"  Invalid number.\")\n",
        "            except Exception as e: print(f\"  Error: {e}\")\n",
        "            continue\n",
        "        if not c:\n",
        "            if local:\n",
        "                sel = local[0]; USE_MODEL = True; MODEL_FILE = sel.name; MODEL_REPO = \"\"\n",
        "                N_CTX = auto_ctx_size(sel.stat().st_size/(1024**3))\n",
        "                print(f\"  âœ“ {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "            else:\n",
        "                USE_MODEL = True; MODEL_REPO = MODEL_MENU[4][1]; MODEL_FILE = MODEL_MENU[4][2]\n",
        "                N_CTX = auto_ctx_size(MODEL_MENU[4][3])\n",
        "                print(f\"  âœ“ {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "        try:\n",
        "            idx = int(c) - 1\n",
        "            if idx < 0 or idx >= len(MODEL_MENU): raise ValueError()\n",
        "            entry = MODEL_MENU[idx]\n",
        "            if not entry[1]:\n",
        "                MODEL_REPO = input(\"  HF repo: \").strip()\n",
        "                MODEL_FILE = input(\"  Filename: \").strip()\n",
        "                N_CTX = 2048\n",
        "            else:\n",
        "                MODEL_REPO, MODEL_FILE = entry[1], entry[2]\n",
        "                N_CTX = auto_ctx_size(entry[3])\n",
        "            USE_MODEL = True\n",
        "            print(f\"  âœ“ {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "        except ValueError:\n",
        "            print(\"  Invalid. Enter 0, L1/L2..., 1-7, or press Enter.\")\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  BUILD GRADIO LAUNCH WRAPPER\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def build_launch_wrapper(python_exe):\n",
        "    threads    = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"NO MODEL\"\n",
        "    cuda_block = \"\" if USE_GPU else \"\\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\"\n",
        "    model_flag = f\"'--model', '{MODEL_FILE}',\" if (USE_MODEL and MODEL_FILE) else \"# no model\"\n",
        "\n",
        "    code = f\"\"\"#!/usr/bin/env python3\n",
        "# MY-AI-Gizmo launch wrapper v3.4 â€” Gradio share=True\n",
        "import sys, os\n",
        "{cuda_block}\n",
        "os.environ['MPLBACKEND']         = 'Agg'\n",
        "os.environ['MPLCONFIGDIR']       = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE']       = '1'\n",
        "\n",
        "flags = [\n",
        "    '--listen', '--share', '--verbose',\n",
        "    '--api', '--api-port', '5000',\n",
        "    '--loader', 'llama.cpp',\n",
        "    '--gpu-layers', '{GPU_LAYERS}',\n",
        "    '--ctx-size', '{N_CTX}',\n",
        "    '--batch-size', '512',\n",
        "    '--threads', '{threads}',\n",
        "    {model_flag}\n",
        "    '--extensions', 'gizmo_toolbar,dual_model,google_workspace',\n",
        "]\n",
        "for f in flags:\n",
        "    if f not in sys.argv:\n",
        "        sys.argv.append(f)\n",
        "\n",
        "print(\"[WRAPPER v3.4] {mode_label} | {model_desc} | ï¼‹button | Google Workspace | Dual Model\")\n",
        "try:\n",
        "    import matplotlib; matplotlib.use('Agg', force=True)\n",
        "except Exception: pass\n",
        "\n",
        "import runpy\n",
        "runpy.run_path('server.py', run_name='__main__')\n",
        "\"\"\"\n",
        "    wrapper_path = WORK_DIR / \"_gizmo_launch.py\"\n",
        "    wrapper_path.write_text(code, encoding=\"utf-8\")\n",
        "    print(\"[âœ“] Launch wrapper: _gizmo_launch.py\")\n",
        "    return str(wrapper_path)\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  SERVER LAUNCH WITH STREAMING + URL CAPTURE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def launch(python_exe, wrapper_path):\n",
        "    cmd = [python_exe, \"-u\", wrapper_path]\n",
        "    env = os.environ.copy()\n",
        "    env.update({\n",
        "        \"MPLBACKEND\":         \"Agg\",\n",
        "        \"MPLCONFIGDIR\":       str(MPL_CONFIG_DIR),\n",
        "        \"GRADIO_SERVER_NAME\": \"0.0.0.0\",\n",
        "        \"GRADIO_SHARE\":       \"1\",\n",
        "    })\n",
        "    if not USE_GPU:\n",
        "        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "    captured = None\n",
        "\n",
        "    for attempt in range(1, MAX_RESTARTS + 1):\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"  ğŸš€ Starting server (attempt {attempt}/{MAX_RESTARTS})\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        if attempt > 1:\n",
        "            time.sleep(5)\n",
        "\n",
        "        log_path = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "        logfile  = None\n",
        "        try: logfile = open(log_path, \"a\", encoding=\"utf-8\")\n",
        "        except Exception: pass\n",
        "\n",
        "        os.chdir(WORK_DIR)\n",
        "        proc = subprocess.Popen(\n",
        "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "            env=env, text=True, bufsize=1,\n",
        "        )\n",
        "\n",
        "        last_output = time.time()\n",
        "        stop_hb     = threading.Event()\n",
        "\n",
        "        def heartbeat():\n",
        "            while not stop_hb.wait(HEARTBEAT_INTERVAL):\n",
        "                if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                    print(\"[heartbeat] server still running...\")\n",
        "\n",
        "        hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "        hb.start()\n",
        "\n",
        "        try:\n",
        "            for line in proc.stdout:\n",
        "                last_output = time.time()\n",
        "                print(line, end=\"\", flush=True)\n",
        "                if logfile:\n",
        "                    try: logfile.write(line)\n",
        "                    except Exception: pass\n",
        "\n",
        "                if not captured:\n",
        "                    for pat in URL_PATTERNS:\n",
        "                        m = pat.search(line)\n",
        "                        if m:\n",
        "                            url = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                            if any(k in url.lower() for k in URL_KEYWORDS):\n",
        "                                captured = url\n",
        "                                print(f\"\\n{'='*70}\")\n",
        "                                print(f\"  ğŸŒ PUBLIC URL: {captured}\")\n",
        "                                print(f\"{'='*70}\\n\", flush=True)\n",
        "                                try: PUBLIC_URL_FILE.write_text(captured)\n",
        "                                except Exception: pass\n",
        "                                break\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n[info] Interrupted by user\")\n",
        "            proc.terminate()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[error] Stream error: {e}\")\n",
        "        finally:\n",
        "            stop_hb.set()\n",
        "            hb.join(timeout=1)\n",
        "            if logfile:\n",
        "                try: logfile.close()\n",
        "                except Exception: pass\n",
        "\n",
        "        rc = proc.wait()\n",
        "        print(f\"\\n[info] Server exited with code {rc}\")\n",
        "\n",
        "        if rc in (0, -9): break\n",
        "        if attempt < MAX_RESTARTS:\n",
        "            print(f\"[warn] Crashed (code {rc}) â€” restarting...\")\n",
        "        else:\n",
        "            print(\"[info] Max restarts reached.\")\n",
        "\n",
        "    return captured\n",
        "\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "#  MAIN\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "    # â•‘   STEP 1 â€” GitHub Token  (MUST happen before everything else)      â•‘\n",
        "    # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    setup_github_token()\n",
        "\n",
        "    # â”€â”€ Banner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    print(\"=\"*70)\n",
        "    print(\"  MY-AI-Gizmo  v3.4  Universal Launcher\")\n",
        "    print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"  Repo: Gizmo-my-ai-for-google-colab  (private â€” token auth)\")\n",
        "    print(\"  ï¼‹button | Styles | Google Docs | Slides | Dual Model\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # â”€â”€ GPU / CPU choice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    choose_mode()\n",
        "\n",
        "    if USE_GPU:\n",
        "        r = sh(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\")\n",
        "        print(f\"[{'âœ“' if r.returncode==0 else 'warn'}] GPU: \"\n",
        "              f\"{r.stdout.strip() if r.returncode==0 else 'not found'}\")\n",
        "\n",
        "    # â”€â”€ Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    drive_ok = mount_drive_if_needed()\n",
        "    setup_drive_root(drive_ok)\n",
        "\n",
        "    for d in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR,\n",
        "              DRIVE_ROOT / \"models\", DRIVE_ROOT / \"settings\", DRIVE_ROOT / \"characters\"):\n",
        "        try: d.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception: pass\n",
        "\n",
        "    cleanup_broken_files()\n",
        "    show_model_manager()\n",
        "    choose_model()\n",
        "\n",
        "    # â”€â”€ Repo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not download_repo_if_missing() and not WORK_DIR.exists():\n",
        "        raise SystemExit(\"Repository unavailable â€” check your token and repo name.\")\n",
        "\n",
        "    # â”€â”€ Symlinks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    ensure_symlinks_and_files()\n",
        "\n",
        "    # â”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    print(\"\\nğŸ“¥ Checking model...\")\n",
        "    print_ram_status()\n",
        "    if not download_model_if_missing():\n",
        "        raise SystemExit(1)\n",
        "\n",
        "    # â”€â”€ Config files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    write_settings_yaml()\n",
        "    write_cmd_flags()\n",
        "    write_debug_character()\n",
        "    write_model_loader_config()\n",
        "\n",
        "    # â”€â”€ Extensions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    print(\"\\nğŸ“¦ Deploying extensions...\")\n",
        "    _deploy_ext_stub(\"gizmo_toolbar\")\n",
        "    _deploy_ext_stub(\"google_workspace\")\n",
        "    deploy_dual_model_extension()\n",
        "\n",
        "    # â”€â”€ Install env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    start_sh   = WORK_DIR / \"start_linux.sh\"\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    if not start_sh.exists():\n",
        "        raise SystemExit(\"[error] start_linux.sh not found â€” check your repo.\")\n",
        "    sh(\"chmod +x start_linux.sh\")\n",
        "\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] First run â€” installing env (5-10 min)...\")\n",
        "        MPL_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        install_env = os.environ.copy()\n",
        "        if USE_GPU:\n",
        "            install_env.update({\n",
        "                \"MPLBACKEND\":\"Agg\",\"MPLCONFIGDIR\":str(MPL_CONFIG_DIR),\n",
        "                \"GPU_CHOICE\":\"A\",\"LAUNCH_AFTER_INSTALL\":\"FALSE\",\n",
        "                \"INSTALL_EXTENSIONS\":\"FALSE\",\n",
        "                \"CMAKE_ARGS\":\"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\",\n",
        "                \"FORCE_CMAKE\":\"1\",\"SKIP_TORCH_TEST\":\"TRUE\",\"FORCE_CUDA\":\"TRUE\",\n",
        "            })\n",
        "        else:\n",
        "            install_env.update({\n",
        "                \"MPLBACKEND\":\"Agg\",\"MPLCONFIGDIR\":str(MPL_CONFIG_DIR),\n",
        "                \"GPU_CHOICE\":\"N\",\"LAUNCH_AFTER_INSTALL\":\"FALSE\",\n",
        "                \"INSTALL_EXTENSIONS\":\"FALSE\",\n",
        "                \"CMAKE_ARGS\":\"-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF\",\n",
        "                \"FORCE_CMAKE\":\"1\",\"CUDA_VISIBLE_DEVICES\":\"\",\"CUDACXX\":\"\",\n",
        "                \"SKIP_TORCH_TEST\":\"TRUE\",\"FORCE_CUDA\":\"FALSE\",\n",
        "            })\n",
        "        installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "        code = stream_with_heartbeat(\n",
        "            \"bash start_linux.sh\", cwd=str(WORK_DIR),\n",
        "            env=install_env, logfile_path=str(installer_log))\n",
        "        print(f\"[{'âœ“' if code==0 else 'warn'}] Installer code {code}\")\n",
        "        python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    # â”€â”€ Post-install: skip llama-cpp if webui already installed it â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    pip_exe = str(Path(python_exe).parent / \"pip\") if Path(python_exe).exists() else \"pip\"\n",
        "    llama_check = sh(f'\"{pip_exe}\" show llama-cpp-binaries 2>/dev/null')\n",
        "    if llama_check.returncode == 0 and \"llama-cpp-binaries\" in llama_check.stdout:\n",
        "        ver = next((l for l in llama_check.stdout.splitlines() if l.startswith(\"Version:\")), \"\")\n",
        "        print(f\"[info] llama-cpp-binaries already installed ({ver.strip()}) â€” skipping reinstall\")\n",
        "    else:\n",
        "        if USE_GPU: install_llama_cpp_gpu(python_exe)\n",
        "        else:       install_llama_cpp_cpu(python_exe)\n",
        "\n",
        "    create_llama_cpp_wrapper(python_exe)\n",
        "    install_google_workspace_deps(python_exe)\n",
        "\n",
        "    # â”€â”€ Kill old servers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "    sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "    sh(\"pkill -9 -f '_gizmo_launch'\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    # â”€â”€ Build Gradio launch wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    wrapper_path = build_launch_wrapper(python_exe)\n",
        "\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"(none â€” load from UI)\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"  LAUNCHING v3.4 â€” {mode_label}\")\n",
        "    print(f\"  Model   : {model_desc}\")\n",
        "    print(f\"  Threads : {auto_thread_count()}  |  ctx: {N_CTX}\")\n",
        "    print(f\"  Extensions: ï¼‹Toolbar | Dual Model | Google Workspace\")\n",
        "    print(f\"  URL will appear below â€” wait ~30 s after model loads\")\n",
        "    print(\"=\"*70)\n",
        "    print_ram_status()\n",
        "    print(\"â³ Starting server...\\n\")\n",
        "\n",
        "    # â”€â”€ LAUNCH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    captured = launch(python_exe, wrapper_path)\n",
        "\n",
        "    # â”€â”€ Fallback: ngrok â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    if not captured:\n",
        "        print(\"\\n[info] No URL captured â€” trying ngrok fallback...\")\n",
        "        captured = try_setup_ngrok(7860)\n",
        "\n",
        "    # â”€â”€ Final summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    if captured:\n",
        "        print(f\"  âœ… READY!  â†’  {captured}\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"  â€¢ ï¼‹ button (bottom-left)  â†’ styles, connectors, tools\")\n",
        "        print(\"  â€¢ ğŸ”— Google Workspace tab  â†’ connect Docs & Slides\")\n",
        "        print(\"  â€¢ ğŸ¤– Dual Model tab        â†’ load a second model\")\n",
        "        print(\"  â€¢ API: http://0.0.0.0:5000/v1\")\n",
        "        if not USE_MODEL:\n",
        "            print(\"  â€¢ âš ï¸  No model loaded â€” go to Model tab in UI to load one\")\n",
        "    else:\n",
        "        print(\"  âŒ NO PUBLIC URL CAPTURED\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"  Fixes: pkill -9 -f server.py | delete installer_files/ | check Colab internet\")\n",
        "    print_ram_status()\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, glob\n",
        "\n",
        "base = \"/content/text-generation-webui\"\n",
        "model_name = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "dest_dir = f\"{base}/user_data/models\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "# Find where the model actually landed\n",
        "found = glob.glob(f\"{base}/**/{model_name}\", recursive=True) + \\\n",
        "        glob.glob(f\"/content/**/{model_name}\", recursive=True)\n",
        "\n",
        "if found:\n",
        "    src = found[0]\n",
        "    dst = f\"{dest_dir}/{model_name}\"\n",
        "    if src != dst:\n",
        "        print(f\"Moving {src} â†’ {dst}\")\n",
        "        shutil.move(src, dst)\n",
        "    print(\"âœ“ Model is in the right place\")\n",
        "else:\n",
        "    print(\"âœ— Model not found â€” re-download needed\")\n",
        "    # Re-download directly to the right place\n",
        "    os.system(f\"wget -q --show-progress -P {dest_dir} https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/{model_name}\")\n",
        "    print(\"âœ“ Download complete\")"
      ],
      "metadata": {
        "id": "xz1cXI-NPY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… RECOMMENDED MODELS (COPY EXACTLY)\n",
        "ğŸ”¹ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "ğŸ”¹ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "âš™ï¸ WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}