{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonlazdev-wq/Gizmo-my-ai-for-google-colab/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo ‚Ä¢ UNIVERSAL LAUNCHER v3.6.0 (Colab + Win11 localhost)\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import subprocess\n",
        "import threading\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# -------- optional colab import --------\n",
        "try:\n",
        "    from google.colab import drive as colab_drive  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# -------- repo config --------\n",
        "GITHUB_USER   = \"leonlazdev-wq\"\n",
        "GITHUB_REPO   = \"Gizmo-my-ai-for-google-colab\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "\n",
        "GITHUB_TOKEN = \"\"\n",
        "REPO_ZIP = \"\"\n",
        "REPO_CLONE_URL = \"\"\n",
        "\n",
        "# -------- runtime globals --------\n",
        "USE_WINDOWS_LOCALHOST = False\n",
        "WORK_DIR = None\n",
        "DRIVE_ROOT = None\n",
        "LOG_DIR = None\n",
        "MPL_CONFIG_DIR = None\n",
        "PUBLIC_URL_FILE = None\n",
        "\n",
        "HEARTBEAT_INTERVAL = 30\n",
        "MAX_RESTARTS = 3\n",
        "\n",
        "USE_GPU = True\n",
        "GPU_LAYERS = -1\n",
        "N_CTX = 4096\n",
        "USE_MODEL = False\n",
        "MODEL_REPO = \"\"\n",
        "MODEL_FILE = \"\"\n",
        "\n",
        "EXTENSIONS = \"gizmo_toolbar,dual_model,google_workspace,learning_center,student_utils,model_hub\"\n",
        "\n",
        "MODEL_MENU = [\n",
        "    (\"1 TinyLlama-1.1B\", \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\", \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", 0.7),\n",
        "    (\"2 Phi-3-mini-4k\", \"bartowski/Phi-3-mini-4k-instruct-GGUF\", \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\", 2.2),\n",
        "    (\"3 Mistral-7B-v0.3\", \"bartowski/Mistral-7B-v0.3-GGUF\", \"Mistral-7B-v0.3-Q4_K_M.gguf\", 4.4),\n",
        "    (\"4 Qwen2.5-Coder-7B\", \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\", \"qwen2.5-coder-7b-instruct-q4_k_m.gguf\", 4.7),\n",
        "    (\"5 Qwen2.5-Coder-14B\", \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\", \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\", 8.9),\n",
        "    (\"6 Custom\", \"\", \"\", 0),\n",
        "]\n",
        "\n",
        "URL_PATTERNS = [\n",
        "    re.compile(r\"Running on public URL:\\s*(https?://\\S+)\", re.IGNORECASE),\n",
        "    re.compile(r\"(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live\\S*)\", re.IGNORECASE),\n",
        "    re.compile(r\"(https?://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com\\S*)\", re.IGNORECASE),\n",
        "    re.compile(r\"(https?://[a-zA-Z0-9\\-]+\\.ngrok\\S*)\", re.IGNORECASE),\n",
        "]\n",
        "URL_KEYWORDS = (\"gradio.live\", \"trycloudflare.com\", \"ngrok\", \"loca.lt\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# helpers\n",
        "# ------------------------------------------------------------\n",
        "def sh(cmd, cwd=None, env=None):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env, capture_output=True, text=True)\n",
        "\n",
        "def run(cmd, cwd=None, env=None):\n",
        "    return subprocess.run(cmd, cwd=cwd, env=env, text=True, capture_output=True)\n",
        "\n",
        "def get_free_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\", \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemAvailable\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def get_total_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\", \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemTotal\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def auto_thread_count():\n",
        "    try:\n",
        "        import multiprocessing\n",
        "        return max(1, min(multiprocessing.cpu_count() - 1, 4))\n",
        "    except Exception:\n",
        "        return 2\n",
        "\n",
        "def auto_ctx_size(model_gb):\n",
        "    free = get_free_ram_gb() - model_gb - 0.5\n",
        "    if free >= 2.0:\n",
        "        return 4096\n",
        "    if free >= 1.0:\n",
        "        return 2048\n",
        "    if free >= 0.5:\n",
        "        return 1024\n",
        "    return 512\n",
        "\n",
        "def print_ram_status():\n",
        "    free = get_free_ram_gb()\n",
        "    total = get_total_ram_gb()\n",
        "    used = total - free\n",
        "    pct = (used / total) if total else 0\n",
        "    bar = \"‚ñà\" * int(pct * 20) + \"‚ñë\" * (20 - int(pct * 20))\n",
        "    print(f\"RAM [{bar}] {used:.1f}/{total:.1f} GB ({free:.1f} GB free)\")\n",
        "\n",
        "def token_file_path():\n",
        "    if Path(\"/content/drive/MyDrive\").exists():\n",
        "        return Path(\"/content/drive/MyDrive/MY-AI-Gizmo/github_token.txt\")\n",
        "    return Path(\"/content/MY-AI-Gizmo/github_token.txt\")\n",
        "\n",
        "def load_saved_token():\n",
        "    for p in (\n",
        "        Path(\"/content/drive/MyDrive/MY-AI-Gizmo/github_token.txt\"),\n",
        "        Path(\"/content/MY-AI-Gizmo/github_token.txt\"),\n",
        "    ):\n",
        "        if p.exists():\n",
        "            try:\n",
        "                t = p.read_text(encoding=\"utf-8\").strip()\n",
        "                if len(t) >= 10:\n",
        "                    return t\n",
        "            except Exception:\n",
        "                pass\n",
        "    return \"\"\n",
        "\n",
        "def save_token(token):\n",
        "    p = token_file_path()\n",
        "    try:\n",
        "        p.parent.mkdir(parents=True, exist_ok=True)\n",
        "        p.write_text(token, encoding=\"utf-8\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] could not save token: {e}\")\n",
        "\n",
        "def build_urls():\n",
        "    global REPO_ZIP, REPO_CLONE_URL\n",
        "    REPO_ZIP = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}/archive/refs/heads/{GITHUB_BRANCH}.zip\"\n",
        "    REPO_CLONE_URL = f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\"\n",
        "\n",
        "def kill_old_servers():\n",
        "    sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "    sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "    sh(\"pkill -9 -f '_gizmo_launch'\")\n",
        "    time.sleep(2)\n",
        "\n",
        "def prompt_yes_no(text, default=\"y\"):\n",
        "    raw = input(f\"{text} ({'Y/n' if default=='y' else 'y/N'}): \").strip().lower()\n",
        "    if not raw:\n",
        "        return default == \"y\"\n",
        "    return raw in (\"y\", \"yes\")\n",
        "\n",
        "def choose_runtime_mode():\n",
        "    global USE_WINDOWS_LOCALHOST\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Runtime mode\")\n",
        "    print(\"1) Normal cloud Colab (/content paths)\")\n",
        "    print(\"2) Windows 11 localhost runtime (Colab Local Runtime)\")\n",
        "    print(\"=\" * 70)\n",
        "    c = input(\"Choose 1 or 2: \").strip()\n",
        "    USE_WINDOWS_LOCALHOST = (c == \"2\")\n",
        "    print(f\"[info] Windows localhost mode: {USE_WINDOWS_LOCALHOST}\")\n",
        "\n",
        "def setup_paths():\n",
        "    global WORK_DIR, DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, PUBLIC_URL_FILE\n",
        "    if USE_WINDOWS_LOCALHOST:\n",
        "        default_work = Path.cwd() / \"text-generation-webui\"\n",
        "        inp = input(f\"Repo path [{default_work}]: \").strip()\n",
        "        WORK_DIR = Path(inp) if inp else default_work\n",
        "\n",
        "        default_root = Path.cwd() / \"MY-AI-Gizmo-data\"\n",
        "        inp2 = input(f\"Data path [{default_root}]: \").strip()\n",
        "        DRIVE_ROOT = Path(inp2) if inp2 else default_root\n",
        "    else:\n",
        "        WORK_DIR = Path(\"/content/text-generation-webui\")\n",
        "        drive_ok = False\n",
        "        if IN_COLAB:\n",
        "            try:\n",
        "                if not Path(\"/content/drive/MyDrive\").exists():\n",
        "                    colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "                drive_ok = Path(\"/content/drive/MyDrive\").exists()\n",
        "            except Exception as e:\n",
        "                print(f\"[warn] drive mount failed: {e}\")\n",
        "        DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\") if drive_ok else Path(\"/content/MY-AI-Gizmo\")\n",
        "\n",
        "    LOG_DIR = DRIVE_ROOT / \"logs\"\n",
        "    MPL_CONFIG_DIR = DRIVE_ROOT / \"matplotlib\"\n",
        "    PUBLIC_URL_FILE = DRIVE_ROOT / \"public_url.txt\"\n",
        "    for p in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, DRIVE_ROOT / \"models\", DRIVE_ROOT / \"settings\", DRIVE_ROOT / \"characters\"):\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def setup_token():\n",
        "    global GITHUB_TOKEN\n",
        "    saved = load_saved_token()\n",
        "    if saved and prompt_yes_no(f\"Use saved token (...{saved[-3:]})?\", \"y\"):\n",
        "        GITHUB_TOKEN = saved\n",
        "        build_urls()\n",
        "        return\n",
        "    while True:\n",
        "        t = input(\"Paste GitHub token: \").strip()\n",
        "        if t:\n",
        "            GITHUB_TOKEN = t\n",
        "            break\n",
        "        print(\"Token required.\")\n",
        "    save_token(GITHUB_TOKEN)\n",
        "    build_urls()\n",
        "\n",
        "def check_repo_update():\n",
        "    if not WORK_DIR.exists():\n",
        "        return \"new\"\n",
        "    return \"fresh\" if prompt_yes_no(\"Did you update the repo on GitHub and want fresh re-clone?\", \"n\") else \"keep\"\n",
        "\n",
        "def apply_repo_update(mode):\n",
        "    if mode == \"fresh\" and WORK_DIR.exists():\n",
        "        kill_old_servers()\n",
        "        shutil.rmtree(WORK_DIR, ignore_errors=True)\n",
        "\n",
        "def clone_repo():\n",
        "    print(\"[info] cloning repo...\")\n",
        "    r = sh(f\"git clone --depth=1 {REPO_CLONE_URL} {WORK_DIR}\")\n",
        "    if r.returncode == 0 and WORK_DIR.exists():\n",
        "        return True\n",
        "    print(\"[warn] clone failed, trying zip fallback...\")\n",
        "    tmp = Path(\"/tmp/repo.zip\") if os.name != \"nt\" else Path(\"repo.zip\")\n",
        "    try:\n",
        "        if tmp.exists():\n",
        "            tmp.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    for cmd in (f\"wget -q -O {tmp} '{REPO_ZIP}'\", f\"curl -s -L -o {tmp} '{REPO_ZIP}'\"):\n",
        "        rr = sh(cmd)\n",
        "        if rr.returncode == 0 and tmp.exists() and tmp.stat().st_size > 1000:\n",
        "            break\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    if os.name == \"nt\":\n",
        "        sh(f'powershell -NoProfile -Command \"Expand-Archive -Path \\\\\"{tmp}\\\\\" -DestinationPath \\\\\".\\\\\" -Force\"')\n",
        "        found = next(Path(\".\").glob(f\"{GITHUB_REPO}-*\"), None)\n",
        "    else:\n",
        "        sh(f\"unzip -q {tmp} -d /content\")\n",
        "        found = next(Path(\"/content\").glob(f\"{GITHUB_REPO}-*\"), None)\n",
        "\n",
        "    if not found:\n",
        "        return False\n",
        "\n",
        "    if WORK_DIR.exists():\n",
        "        shutil.rmtree(WORK_DIR, ignore_errors=True)\n",
        "    found.rename(WORK_DIR)\n",
        "    return True\n",
        "\n",
        "def choose_mode():\n",
        "    global USE_GPU, GPU_LAYERS, N_CTX\n",
        "    print(\"Mode: [1] GPU [2] CPU\")\n",
        "    c = input(\"Choose 1/2: \").strip()\n",
        "    if c == \"2\":\n",
        "        USE_GPU = False\n",
        "        GPU_LAYERS = 0\n",
        "        N_CTX = 4096\n",
        "    else:\n",
        "        USE_GPU = True\n",
        "        GPU_LAYERS = -1\n",
        "        N_CTX = 4096\n",
        "\n",
        "def list_local_models():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    found = []\n",
        "    for ext in (\"*.gguf\", \"*.safetensors\", \"*.bin\"):\n",
        "        found.extend(d.rglob(ext))\n",
        "    return sorted(found)\n",
        "\n",
        "def choose_model():\n",
        "    global USE_MODEL, MODEL_REPO, MODEL_FILE, N_CTX\n",
        "    local = list_local_models()\n",
        "    print(\"Model selector:\")\n",
        "    for i, m in enumerate(local, 1):\n",
        "        print(f\"  [L{i}] {m.name}\")\n",
        "    for m in MODEL_MENU:\n",
        "        print(\" \", m[0])\n",
        "    print(\"  [0] start without model\")\n",
        "\n",
        "    while True:\n",
        "        c = input(\"Choice: \").strip()\n",
        "        if c == \"0\":\n",
        "            USE_MODEL = False\n",
        "            MODEL_REPO = \"\"\n",
        "            MODEL_FILE = \"\"\n",
        "            return\n",
        "        if c.upper().startswith(\"L\") and local:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                sel = local[idx]\n",
        "                USE_MODEL = True\n",
        "                MODEL_REPO = \"\"\n",
        "                MODEL_FILE = sel.name\n",
        "                N_CTX = auto_ctx_size(sel.stat().st_size / (1024**3))\n",
        "                return\n",
        "            except Exception:\n",
        "                print(\"invalid local model\")\n",
        "                continue\n",
        "        try:\n",
        "            idx = int(c) - 1\n",
        "            item = MODEL_MENU[idx]\n",
        "            if item[1]:\n",
        "                USE_MODEL = True\n",
        "                MODEL_REPO, MODEL_FILE = item[1], item[2]\n",
        "                N_CTX = auto_ctx_size(item[3])\n",
        "                return\n",
        "            MODEL_REPO = input(\"HF repo: \").strip()\n",
        "            MODEL_FILE = input(\"Filename: \").strip()\n",
        "            USE_MODEL = True\n",
        "            N_CTX = 2048\n",
        "            return\n",
        "        except Exception:\n",
        "            print(\"invalid choice\")\n",
        "\n",
        "def download_model_if_missing():\n",
        "    if not USE_MODEL:\n",
        "        print(\"[info] no model selected\")\n",
        "        return True\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    p = models_dir / MODEL_FILE\n",
        "    if p.exists() and p.stat().st_size > 100 * 1024 * 1024:\n",
        "        return True\n",
        "    if not MODEL_REPO:\n",
        "        return False\n",
        "    url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILE}?download=true\"\n",
        "    for cmd in (f'wget -q --show-progress -O \"{p}\" \"{url}\"', f'curl -L --progress-bar -o \"{p}\" \"{url}\"'):\n",
        "        r = subprocess.run(cmd, shell=True)\n",
        "        if r.returncode == 0 and p.exists() and p.stat().st_size > 100 * 1024 * 1024:\n",
        "            return True\n",
        "        try:\n",
        "            p.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "    return False\n",
        "\n",
        "# -------- robust link/copy mapping (fix WinError 1314) --------\n",
        "def _safe_remove(path: Path):\n",
        "    try:\n",
        "        if path.is_symlink() or path.is_file():\n",
        "            path.unlink()\n",
        "        elif path.is_dir():\n",
        "            shutil.rmtree(path, ignore_errors=True)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _copy_fallback(src: Path, dst: Path, is_file: bool):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if is_file:\n",
        "        if src.exists():\n",
        "            shutil.copy2(src, dst)\n",
        "        else:\n",
        "            dst.write_text(\"\", encoding=\"utf-8\")\n",
        "    else:\n",
        "        if dst.exists():\n",
        "            shutil.rmtree(dst, ignore_errors=True)\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "\n",
        "def _link_or_copy(src: Path, dst: Path, is_file: bool):\n",
        "    _safe_remove(dst)\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # try symlink first\n",
        "    try:\n",
        "        os.symlink(str(src), str(dst), target_is_directory=not is_file)\n",
        "        return \"symlink\"\n",
        "    except Exception as e:\n",
        "        err = str(e)\n",
        "\n",
        "    # windows junction fallback for dirs\n",
        "    if os.name == \"nt\" and not is_file:\n",
        "        try:\n",
        "            subprocess.run([\"cmd\", \"/c\", \"mklink\", \"/J\", str(dst), str(src)], check=True, capture_output=True)\n",
        "            return \"junction\"\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # fallback copy\n",
        "    _copy_fallback(src, dst, is_file)\n",
        "    return \"copy\"\n",
        "\n",
        "def ensure_mappings():\n",
        "    links_map = [\n",
        "        (\"user_data/models\", \"models\", False),\n",
        "        (\"models\", \"models\", False),\n",
        "        (\"user_data/loras\", \"loras\", False),\n",
        "        (\"user_data/characters\", \"characters\", False),\n",
        "        (\"user_data/presets\", \"presets\", False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\", \"chat-history\", False),\n",
        "        (\"outputs\", \"outputs\", False),\n",
        "    ]\n",
        "\n",
        "    for local_rel, data_rel, is_file in links_map:\n",
        "        src = DRIVE_ROOT / data_rel\n",
        "        dst = WORK_DIR / local_rel\n",
        "        if is_file:\n",
        "            src.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not src.exists():\n",
        "                src.write_text(\"\", encoding=\"utf-8\")\n",
        "        else:\n",
        "            src.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        mode = _link_or_copy(src, dst, is_file)\n",
        "        print(f\"[map] {local_rel} <= {data_rel} ({mode})\")\n",
        "\n",
        "def write_settings():\n",
        "    threads = auto_thread_count()\n",
        "    model_line = f\"model: {MODEL_FILE}\" if (USE_MODEL and MODEL_FILE) else \"model: None\"\n",
        "    content = f\"\"\"listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "loader: llama.cpp\n",
        "n_ctx: {N_CTX}\n",
        "n_batch: 512\n",
        "n_gpu_layers: {GPU_LAYERS}\n",
        "threads: {threads}\n",
        "character: Debug\n",
        "{model_line}\n",
        "chat_style: cai-chat\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "    for p in (WORK_DIR / \"user_data\" / \"settings.yaml\", DRIVE_ROOT / \"settings\" / \"settings.yaml\"):\n",
        "        p.parent.mkdir(parents=True, exist_ok=True)\n",
        "        p.write_text(content, encoding=\"utf-8\")\n",
        "\n",
        "def write_cmd_flags():\n",
        "    flags = [\n",
        "        \"--listen\", \"--share\", \"--verbose\",\n",
        "        \"--api\", \"--api-port\", \"5000\",\n",
        "        \"--loader\", \"llama.cpp\",\n",
        "        \"--gpu-layers\", str(GPU_LAYERS),\n",
        "        \"--ctx-size\", str(N_CTX),\n",
        "        \"--batch-size\", \"512\",\n",
        "        \"--threads\", str(auto_thread_count()),\n",
        "        \"--extensions\", EXTENSIONS,\n",
        "    ]\n",
        "    if USE_MODEL and MODEL_FILE:\n",
        "        flags += [\"--model\", MODEL_FILE]\n",
        "    content = \" \".join(flags)\n",
        "    for p in (WORK_DIR / \"user_data\" / \"CMD_FLAGS.txt\", DRIVE_ROOT / \"settings\" / \"CMD_FLAGS.txt\"):\n",
        "        p.parent.mkdir(parents=True, exist_ok=True)\n",
        "        p.write_text(content, encoding=\"utf-8\")\n",
        "\n",
        "def ensure_extension_dirs():\n",
        "    for ext in (\"gizmo_toolbar\", \"dual_model\", \"google_workspace\", \"learning_center\", \"student_utils\", \"model_hub\"):\n",
        "        d = WORK_DIR / \"extensions\" / ext\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def build_launch_wrapper(python_exe):\n",
        "    threads = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"NO MODEL\"\n",
        "    cuda_block = \"os.environ['CUDA_VISIBLE_DEVICES'] = ''\" if not USE_GPU else \"\"\n",
        "    model_flag = f\"'--model', '{MODEL_FILE}',\" if (USE_MODEL and MODEL_FILE) else \"\"\n",
        "\n",
        "    code = f\"\"\"#!/usr/bin/env python3\n",
        "import sys, os\n",
        "{cuda_block}\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['MPLCONFIGDIR'] = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE'] = '1'\n",
        "\n",
        "flags = [\n",
        "    '--listen', '--share', '--verbose',\n",
        "    '--api', '--api-port', '5000',\n",
        "    '--loader', 'llama.cpp',\n",
        "    '--gpu-layers', '{GPU_LAYERS}',\n",
        "    '--ctx-size', '{N_CTX}',\n",
        "    '--batch-size', '512',\n",
        "    '--threads', '{threads}',\n",
        "    {model_flag}\n",
        "    '--extensions', '{EXTENSIONS}',\n",
        "]\n",
        "flags = [f for f in flags if f]\n",
        "for f in flags:\n",
        "    if f not in sys.argv:\n",
        "        sys.argv.append(f)\n",
        "\n",
        "print('[WRAPPER v3.6.0] Mode: {mode_label} | Model: {model_desc}')\n",
        "print('[WRAPPER] Extensions: {EXTENSIONS}')\n",
        "\n",
        "# Gradio compatibility shim (for 4.37.x)\n",
        "try:\n",
        "    import gradio as gr\n",
        "    if not hasattr(gr, 'Timer'):\n",
        "        class _GizmoTimerShim:\n",
        "            def __init__(self, *args, **kwargs): pass\n",
        "            def tick(self, *args, **kwargs): return None\n",
        "        gr.Timer = _GizmoTimerShim\n",
        "        print('[WRAPPER] Applied gr.Timer shim')\n",
        "except Exception as e:\n",
        "    print(f'[WRAPPER] Timer shim warning: {{e}}')\n",
        "\n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg', force=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import traceback, runpy\n",
        "try:\n",
        "    runpy.run_path('server.py', run_name='__main__')\n",
        "except SystemExit:\n",
        "    pass\n",
        "except Exception:\n",
        "    print('\\\\n[ERROR] server.py raised an exception:')\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\"\"\"\n",
        "    wrapper = WORK_DIR / \"_gizmo_launch.py\"\n",
        "    wrapper.write_text(code, encoding=\"utf-8\")\n",
        "    return str(wrapper)\n",
        "\n",
        "def launch(python_exe, wrapper_path):\n",
        "    cmd = [python_exe, \"-u\", wrapper_path]\n",
        "    env = os.environ.copy()\n",
        "    env.update({\n",
        "        \"MPLBACKEND\": \"Agg\",\n",
        "        \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "        \"GRADIO_SERVER_NAME\": \"0.0.0.0\",\n",
        "        \"GRADIO_SHARE\": \"1\",\n",
        "    })\n",
        "    if not USE_GPU:\n",
        "        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "    captured = None\n",
        "\n",
        "    for attempt in range(1, MAX_RESTARTS + 1):\n",
        "        print(f\"\\n{'='*70}\\nüöÄ Starting server (attempt {attempt}/{MAX_RESTARTS})\\n{'='*70}\\n\")\n",
        "        if attempt > 1:\n",
        "            time.sleep(5)\n",
        "\n",
        "        log_path = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "        logfile = open(log_path, \"a\", encoding=\"utf-8\")\n",
        "        os.chdir(WORK_DIR)\n",
        "\n",
        "        proc = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            env=env,\n",
        "            text=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "\n",
        "        stop_hb = threading.Event()\n",
        "        last_out = [time.time()]\n",
        "\n",
        "        def hb():\n",
        "            while not stop_hb.wait(HEARTBEAT_INTERVAL):\n",
        "                if time.time() - last_out[0] >= HEARTBEAT_INTERVAL:\n",
        "                    print(\"[heartbeat] still running...\")\n",
        "\n",
        "        t = threading.Thread(target=hb, daemon=True)\n",
        "        t.start()\n",
        "\n",
        "        try:\n",
        "            for line in proc.stdout:\n",
        "                last_out[0] = time.time()\n",
        "                print(line, end=\"\", flush=True)\n",
        "                logfile.write(line)\n",
        "\n",
        "                if not captured:\n",
        "                    for pat in URL_PATTERNS:\n",
        "                        m = pat.search(line)\n",
        "                        if m:\n",
        "                            url = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                            if any(k in url.lower() for k in URL_KEYWORDS):\n",
        "                                captured = url\n",
        "                                print(f\"\\nüåê PUBLIC URL: {captured}\\n\")\n",
        "                                try:\n",
        "                                    PUBLIC_URL_FILE.write_text(captured)\n",
        "                                except Exception:\n",
        "                                    pass\n",
        "                                break\n",
        "        finally:\n",
        "            stop_hb.set()\n",
        "            t.join(timeout=1)\n",
        "            logfile.close()\n",
        "\n",
        "        rc = proc.wait()\n",
        "        print(f\"[info] server exit code: {rc}\")\n",
        "        if rc in (0, -9):\n",
        "            break\n",
        "        if attempt < MAX_RESTARTS:\n",
        "            print(\"[warn] restarting...\")\n",
        "        else:\n",
        "            print(\"[warn] max restarts reached\")\n",
        "\n",
        "    return captured\n",
        "\n",
        "def install_env_if_needed():\n",
        "    start_sh = WORK_DIR / \"start_linux.sh\"\n",
        "    if not start_sh.exists():\n",
        "        raise SystemExit(\"‚ùå start_linux.sh not found\")\n",
        "\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    py = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    sh(\"chmod +x start_linux.sh\", cwd=str(WORK_DIR))\n",
        "\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] first run: installing env...\")\n",
        "        install_env = os.environ.copy()\n",
        "        install_env.update({\n",
        "            \"MPLBACKEND\": \"Agg\",\n",
        "            \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "            \"GPU_CHOICE\": \"A\" if USE_GPU else \"N\",\n",
        "            \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "            \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "            \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "        })\n",
        "        if not USE_GPU:\n",
        "            install_env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "        subprocess.run(\"bash start_linux.sh\", shell=True, cwd=str(WORK_DIR), env=install_env)\n",
        "        py = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    return py\n",
        "\n",
        "def print_windows_localhost_instructions():\n",
        "    if not USE_WINDOWS_LOCALHOST:\n",
        "        return\n",
        "    print(\"\\n[Windows localhost mode detected]\")\n",
        "    print(\"Make sure on Windows host you already ran:\")\n",
        "    print(\"  py -3 -m jupyter serverextension enable --py jupyter_http_over_ws\")\n",
        "    print('  py -3 -m notebook --NotebookApp.allow_origin=\"https://colab.research.google.com\" --port=8888 --NotebookApp.port_retries=0 --no-browser')\n",
        "    print(\"Then connect Colab to that local runtime before running launcher.\\n\")\n",
        "\n",
        "# ---------------- main ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"MY-AI-Gizmo Launcher v3.6.0\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    choose_runtime_mode()\n",
        "    print_windows_localhost_instructions()\n",
        "\n",
        "    setup_paths()\n",
        "    setup_token()\n",
        "\n",
        "    mode = check_repo_update()\n",
        "    apply_repo_update(mode)\n",
        "\n",
        "    if not WORK_DIR.exists():\n",
        "        if not clone_repo():\n",
        "            raise SystemExit(\"‚ùå clone failed\")\n",
        "\n",
        "    choose_mode()\n",
        "    choose_model()\n",
        "    print_ram_status()\n",
        "\n",
        "    if not download_model_if_missing():\n",
        "        raise SystemExit(\"‚ùå model download failed\")\n",
        "\n",
        "    ensure_mappings()\n",
        "    write_settings()\n",
        "    write_cmd_flags()\n",
        "    ensure_extension_dirs()\n",
        "\n",
        "    pyexe = install_env_if_needed()\n",
        "    kill_old_servers()\n",
        "\n",
        "    wrapper = build_launch_wrapper(pyexe)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"LAUNCHING ({'GPU' if USE_GPU else 'CPU'})\")\n",
        "    print(f\"Model: {MODEL_FILE if USE_MODEL else '(none)'}\")\n",
        "    print(f\"Extensions: {EXTENSIONS}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    url = launch(pyexe, wrapper)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    if url:\n",
        "        print(f\"‚úÖ READY: {url}\")\n",
        "        print(\"Expected tabs: Learning Center, Student Utils, Model Hub, Google Workspace, Dual Model\")\n",
        "    else:\n",
        "        print(\"‚ùå No public URL captured. Check logs in:\", LOG_DIR)\n",
        "    print(\"=\"*70)\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "Qi3tWZ53XtPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# GIZMO AUTO MERGE SCRIPT (COLAB)\n",
        "# Accept ALL incoming changes\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"\\n=== Gizmo Auto Merge Tool ===\\n\")\n",
        "\n",
        "# -------- USER INPUT --------\n",
        "\n",
        "repo_url = input(\"Enter GitHub repo URL (example: https://github.com/USER/REPO.git): \")\n",
        "\n",
        "branch = input(\"Target branch (usually main): \")\n",
        "if branch.strip() == \"\":\n",
        "    branch = \"main\"\n",
        "\n",
        "incoming_branch = input(\"Incoming branch to merge FROM: \")\n",
        "\n",
        "token = getpass.getpass(\"Paste GitHub Token (hidden): \")\n",
        "\n",
        "# -------- SETUP --------\n",
        "\n",
        "repo_name = repo_url.split(\"/\")[-1].replace(\".git\",\"\")\n",
        "\n",
        "auth_repo = repo_url.replace(\n",
        "    \"https://\",\n",
        "    f\"https://{token}@\"\n",
        ")\n",
        "\n",
        "print(\"\\nCloning or updating repo...\\n\")\n",
        "\n",
        "if os.path.exists(repo_name):\n",
        "    os.system(f\"rm -rf {repo_name}\")\n",
        "\n",
        "os.system(f\"git clone {auth_repo}\")\n",
        "\n",
        "os.chdir(repo_name)\n",
        "\n",
        "# -------- CONFIG --------\n",
        "\n",
        "os.system(\"git config user.email 'colab@gizmo.ai'\")\n",
        "os.system(\"git config user.name 'Colab Gizmo Bot'\")\n",
        "\n",
        "# -------- BACKUP --------\n",
        "\n",
        "print(\"\\nCreating backup branch...\\n\")\n",
        "\n",
        "os.system(f\"git checkout {branch}\")\n",
        "os.system(\"git checkout -b backup-before-merge\")\n",
        "\n",
        "# -------- MERGE --------\n",
        "\n",
        "print(\"\\nMerging incoming changes...\\n\")\n",
        "\n",
        "os.system(f\"git checkout {branch}\")\n",
        "os.system(\"git fetch origin\")\n",
        "\n",
        "merge_code = os.system(\n",
        "    f\"git merge -X theirs origin/{incoming_branch}\"\n",
        ")\n",
        "\n",
        "# -------- AUTO RESOLVE --------\n",
        "\n",
        "print(\"\\nResolving conflicts automatically...\\n\")\n",
        "\n",
        "os.system(\n",
        "    \"git diff --name-only --diff-filter=U | xargs -r git checkout --theirs --\"\n",
        ")\n",
        "\n",
        "os.system(\"git add -A\")\n",
        "\n",
        "os.system(\n",
        "    \"git commit -m 'Auto-resolve conflicts: accepted incoming changes'\"\n",
        ")\n",
        "\n",
        "# -------- PUSH --------\n",
        "\n",
        "print(\"\\nPushing to GitHub...\\n\")\n",
        "\n",
        "os.system(f\"git push origin {branch}\")\n",
        "\n",
        "# -------- DONE --------\n",
        "\n",
        "print(\"\\nSUCCESS!\")\n",
        "print(\"All incoming changes merged.\")\n",
        "print(\"Conflicts resolved automatically.\")\n",
        "print(\"Backup branch created: backup-before-merge\")"
      ],
      "metadata": {
        "id": "xz1cXI-NPY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}