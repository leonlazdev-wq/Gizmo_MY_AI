{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo ‚Ä¢ CPU-OPTIMIZED LAUNCHER (FULLY FIXED)\n",
        "# - Properly installs llama-cpp-python with binaries\n",
        "# - Fixes llama_cpp_binaries module import\n",
        "# - Forces CPU-only mode correctly\n",
        "# - Ensures public URL generation\n",
        "# - Fixed model loading\n",
        "# ================================================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ---------- Configuration ----------\n",
        "REPO_ZIP = \"https://github.com/gitleon8301/MY-AI-Gizmo-working/archive/refs/heads/main.zip\"\n",
        "WORK_DIR = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "LOG_DIR = DRIVE_ROOT / \"logs\"\n",
        "MPL_CONFIG_DIR = DRIVE_ROOT / \"matplotlib\"\n",
        "HEARTBEAT_INTERVAL = 30  # seconds\n",
        "PUBLIC_URL_FILE = DRIVE_ROOT / \"public_url.txt\"\n",
        "# -----------------------------------\n",
        "\n",
        "def sh(cmd, cwd=None, env=None, check=False):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env, capture_output=True, text=True, check=check)\n",
        "\n",
        "def stream_with_heartbeat(cmd, cwd=None, env=None, logfile_path=None, capture_url_to=None):\n",
        "    \"\"\"\n",
        "    Run command, stream output line-by-line, print heartbeat if silent.\n",
        "    Capture first Gradio/public URL found and optionally save it to capture_url_to.\n",
        "    Returns (returncode, captured_url_or_None).\n",
        "    \"\"\"\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                            cwd=cwd, env=env, text=True, bufsize=1)\n",
        "\n",
        "    last_output = time.time()\n",
        "    stop = threading.Event()\n",
        "    captured_url = None\n",
        "\n",
        "    # Enhanced URL patterns with priority ordering\n",
        "    url_patterns = [\n",
        "        # Gradio public URLs (highest priority)\n",
        "        re.compile(r'Running on public URL:\\s*(https?://[^\\s]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "        re.compile(r'Public URL:\\s*(https?://[^\\s]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "        re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "\n",
        "        # Gradio app URLs\n",
        "        re.compile(r'(https?://[^\\s]+\\.gradio\\.app[^\\s,)\\'\\\"]*)', re.IGNORECASE),\n",
        "\n",
        "        # Local URLs with port\n",
        "        re.compile(r'Running on local URL:\\s*(https?://[^\\s]+:[0-9]+)', re.IGNORECASE),\n",
        "        re.compile(r'(https?://(?:localhost|127\\.0\\.0\\.1|0\\.0\\.0\\.0):[0-9]+)', re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_output >= HEARTBEAT_INTERVAL:\n",
        "                msg = f\"[heartbeat] still working... (no output for ~{HEARTBEAT_INTERVAL}s)\\n\"\n",
        "                print(msg, end='')\n",
        "                if logfile_path:\n",
        "                    try:\n",
        "                        with open(logfile_path, \"a\", encoding=\"utf-8\") as f:\n",
        "                            f.write(msg)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "\n",
        "    logfile = None\n",
        "    if logfile_path:\n",
        "        try:\n",
        "            logfile = open(logfile_path, \"a\", encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            logfile = None\n",
        "\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            last_output = time.time()\n",
        "            print(line, end='')\n",
        "            if logfile:\n",
        "                try:\n",
        "                    logfile.write(line)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            # URL capture logic (fixed)\n",
        "            for pat in url_patterns:\n",
        "                m = pat.search(line)\n",
        "                if m:\n",
        "                    candidate = m.group(1).rstrip(').,\\'\"')\n",
        "                    # Prioritize gradio.live URLs\n",
        "                    if 'gradio.live' in candidate.lower():\n",
        "                        captured_url = candidate\n",
        "                        print(f\"\\n{'='*70}\")\n",
        "                        print(f\"üåê PUBLIC URL FOUND: {captured_url}\")\n",
        "                        print(f\"{'='*70}\\n\")\n",
        "                        if capture_url_to:\n",
        "                            try:\n",
        "                                Path(capture_url_to).write_text(captured_url, encoding=\"utf-8\")\n",
        "                            except Exception:\n",
        "                                pass\n",
        "                        break\n",
        "                    # Use other URLs only if we don't have a gradio.live URL yet\n",
        "                    elif not captured_url:\n",
        "                        captured_url = candidate\n",
        "                        print(f\"\\nüîó URL DETECTED: {captured_url}\\n\")\n",
        "                        if capture_url_to:\n",
        "                            try:\n",
        "                                Path(capture_url_to).write_text(captured_url, encoding=\"utf-8\")\n",
        "                            except Exception:\n",
        "                                pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[stream error] {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop.set()\n",
        "        hb.join(timeout=1)\n",
        "        if logfile:\n",
        "            try:\n",
        "                logfile.close()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return proc.returncode, captured_url\n",
        "\n",
        "def ensure_dirs():\n",
        "    for d in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR):\n",
        "        try:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "def download_repo_if_missing():\n",
        "    if WORK_DIR.exists():\n",
        "        print(f\"[info] WORK_DIR exists: {WORK_DIR}\")\n",
        "        return True\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try:\n",
        "        tmp_zip.unlink()\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"[info] downloading repository...\")\n",
        "    ok = False\n",
        "    for cmd in (f\"wget -q -O {tmp_zip} {REPO_ZIP}\", f\"curl -s -L -o {tmp_zip} {REPO_ZIP}\"):\n",
        "        try:\n",
        "            result = sh(cmd)\n",
        "            if result.returncode == 0 and tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "                ok = True\n",
        "                break\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        print(\"[error] download failed. Check network/URL.\")\n",
        "        return False\n",
        "    print(\"[info] extracting...\")\n",
        "    try:\n",
        "        sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "        found = next(Path(\"/content\").glob(\"MY-AI-Gizmo-working-*\"), None)\n",
        "        if not found:\n",
        "            print(\"[error] expected extracted folder not found\")\n",
        "            return False\n",
        "        found.rename(WORK_DIR)\n",
        "        print(\"[info] repo extracted to\", WORK_DIR)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"[error] extract failed:\", e)\n",
        "        return False\n",
        "\n",
        "def ensure_symlinks_and_files():\n",
        "    links_map = [\n",
        "        (\"models\", \"models\", False),\n",
        "        (\"loras\", \"loras\", False),\n",
        "        (\"user_data/characters\", \"characters\", False),\n",
        "        (\"user_data/presets\", \"presets\", False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\", \"chat-history\", False),\n",
        "        (\"outputs\", \"outputs\", False),\n",
        "    ]\n",
        "    for local, drive_folder, is_settings in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_folder\n",
        "        if is_settings:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                try:\n",
        "                    drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "                except Exception:\n",
        "                    pass\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        local_path = WORK_DIR / local\n",
        "        try:\n",
        "            if local_path.exists() or local_path.is_symlink():\n",
        "                if local_path.is_symlink():\n",
        "                    local_path.unlink()\n",
        "                elif local_path.is_dir():\n",
        "                    shutil.rmtree(local_path)\n",
        "                else:\n",
        "                    local_path.unlink()\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            os.symlink(str(drive_path), str(local_path), target_is_directory=drive_path.is_dir())\n",
        "        except Exception:\n",
        "            try:\n",
        "                if drive_path.is_dir():\n",
        "                    shutil.copytree(drive_path, local_path, dirs_exist_ok=True)\n",
        "                else:\n",
        "                    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    shutil.copy2(drive_path, local_path)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "def prepare_settings_file():\n",
        "    \"\"\"Create optimized settings for CPU mode with public sharing enabled\"\"\"\n",
        "    drive_settings = DRIVE_ROOT / \"settings\" / \"settings.yaml\"\n",
        "    local_settings = WORK_DIR / \"user_data\" / \"settings.yaml\"\n",
        "    local_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Comprehensive settings for CPU mode\n",
        "    settings_content = \"\"\"# MY-AI-Gizmo Settings - CPU Mode (Auto-configured)\n",
        "# Network settings\n",
        "listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "\n",
        "# CPU-optimized loader preferences\n",
        "loader: llama.cpp\n",
        "\n",
        "# Performance settings for CPU\n",
        "n_ctx: 2048\n",
        "n_batch: 512\n",
        "threads: 4\n",
        "n_gpu_layers: 0\n",
        "\n",
        "# UI settings\n",
        "chat_style: cai-chat\n",
        "character: Assistant\n",
        "\n",
        "# API settings\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Write to both locations\n",
        "        local_settings.write_text(settings_content, encoding=\"utf-8\")\n",
        "        drive_settings.parent.mkdir(parents=True, exist_ok=True)\n",
        "        drive_settings.write_text(settings_content, encoding=\"utf-8\")\n",
        "        print(\"[‚úì] Settings configured for CPU mode with public sharing\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not update settings: {e}\")\n",
        "\n",
        "def cleanup_broken_files(drive_root: Path):\n",
        "    \"\"\"Remove incomplete/broken model files from Drive\"\"\"\n",
        "    models_dir = drive_root / \"models\"\n",
        "    if not models_dir.exists():\n",
        "        return\n",
        "    extensions = [\"*.gguf\", \"*.safetensors\", \"*.bin\", \"*.pth\", \"*.pt\"]\n",
        "    broken = []\n",
        "    for ext in extensions:\n",
        "        for f in models_dir.rglob(ext):\n",
        "            try:\n",
        "                if f.stat().st_size < (100 * 1024):\n",
        "                    broken.append(f)\n",
        "            except Exception:\n",
        "                pass\n",
        "    if broken:\n",
        "        print(f\"[info] Removing {len(broken)} broken/incomplete files from Drive models folder\")\n",
        "        for f in broken:\n",
        "            try:\n",
        "                f.unlink()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "def install_llama_cpp_python_cpu():\n",
        "    \"\"\"\n",
        "    Install llama-cpp-python for CPU-only use with proper binary support.\n",
        "    This is the critical fix for the llama_cpp_binaries import error.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Installing llama-cpp-python for CPU...\")\n",
        "\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] Virtual environment not yet created, will be handled by installer\")\n",
        "        return\n",
        "\n",
        "    python_exe = str(env_marker)\n",
        "    pip_exe = str(env_marker.parent / \"pip\")\n",
        "\n",
        "    # Uninstall any existing versions\n",
        "    print(\"[info] Removing any existing llama-cpp-python installations...\")\n",
        "    sh(f'\"{python_exe}\" -m pip uninstall -y llama-cpp-python llama-cpp-python-cuda', check=False)\n",
        "\n",
        "    # Install CPU-only version\n",
        "    print(\"[info] Installing llama-cpp-python for CPU-only (with binaries)...\")\n",
        "    cpu_env = os.environ.copy()\n",
        "    cpu_env.update({\n",
        "        'CMAKE_ARGS': '-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF -DLLAMA_METAL=OFF -DLLAMA_OPENCL=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS',\n",
        "        'FORCE_CMAKE': '1',\n",
        "        'CUDACXX': '',\n",
        "    })\n",
        "\n",
        "    # Install with verbose output to see any issues\n",
        "    result = sh(\n",
        "        f'\"{pip_exe}\" install llama-cpp-python --no-cache-dir --force-reinstall --upgrade --verbose',\n",
        "        env=cpu_env,\n",
        "        check=False\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"[‚úì] llama-cpp-python installed successfully\")\n",
        "    else:\n",
        "        print(f\"[warn] Installation returned code {result.returncode}\")\n",
        "        print(\"[info] Stderr output:\")\n",
        "        print(result.stderr[-1000:] if len(result.stderr) > 1000 else result.stderr)\n",
        "\n",
        "def create_llama_cpp_binaries_wrapper():\n",
        "    \"\"\"\n",
        "    Create a wrapper module for llama_cpp_binaries to fix the import error.\n",
        "    This provides the get_binary_path() function that llama_cpp_server.py needs.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Creating llama_cpp_binaries compatibility wrapper...\")\n",
        "\n",
        "    modules_dir = WORK_DIR / \"modules\"\n",
        "    wrapper_file = modules_dir / \"llama_cpp_binaries.py\"\n",
        "\n",
        "    wrapper_code = '''\"\"\"\n",
        "Compatibility wrapper for llama_cpp_binaries.\n",
        "Provides the get_binary_path() function needed by llama_cpp_server.py\n",
        "\"\"\"\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def get_binary_path():\n",
        "    \"\"\"\n",
        "    Find the llama-server binary. Checks multiple locations.\n",
        "    Returns the path to the llama-server executable.\n",
        "    \"\"\"\n",
        "    # Try to find llama-server in common locations\n",
        "    search_paths = []\n",
        "\n",
        "    # 1. Check if llama-cpp-python is installed and has binaries\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        llama_cpp_path = Path(llama_cpp.__file__).parent\n",
        "        search_paths.append(llama_cpp_path / \"bin\")\n",
        "        search_paths.append(llama_cpp_path / \"lib\")\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    # 2. Check system PATH\n",
        "    system_binary = shutil.which(\"llama-server\") or shutil.which(\"llama-cpp-server\")\n",
        "    if system_binary:\n",
        "        return system_binary\n",
        "\n",
        "    # 3. Check repositories directory\n",
        "    repo_dir = Path(__file__).parent.parent / \"repositories\" / \"llama.cpp\"\n",
        "    if repo_dir.exists():\n",
        "        search_paths.extend([\n",
        "            repo_dir / \"build\" / \"bin\",\n",
        "            repo_dir / \"build\",\n",
        "            repo_dir,\n",
        "        ])\n",
        "\n",
        "    # 4. Check installer files\n",
        "    installer_dir = Path(__file__).parent.parent / \"installer_files\"\n",
        "    if installer_dir.exists():\n",
        "        search_paths.extend([\n",
        "            installer_dir / \"env\" / \"bin\",\n",
        "            installer_dir / \"env\" / \"lib\",\n",
        "        ])\n",
        "\n",
        "    # Search for the binary\n",
        "    binary_names = [\"llama-server\", \"llama-cpp-server\", \"server\"]\n",
        "\n",
        "    for search_path in search_paths:\n",
        "        if not search_path.exists():\n",
        "            continue\n",
        "\n",
        "        for binary_name in binary_names:\n",
        "            # Check with and without extension\n",
        "            for ext in [\"\", \".exe\"]:\n",
        "                binary_path = search_path / f\"{binary_name}{ext}\"\n",
        "                if binary_path.exists() and (os.access(binary_path, os.X_OK) or ext == \".exe\"):\n",
        "                    print(f\"[llama_cpp_binaries] Found binary: {binary_path}\")\n",
        "                    return str(binary_path)\n",
        "\n",
        "    # If not found, try to use llama-cpp-python's built-in server\n",
        "    # This is a fallback - we'll create a simple server wrapper\n",
        "    print(\"[llama_cpp_binaries] No prebuilt binary found, will use llama-cpp-python\")\n",
        "\n",
        "    # Return a marker that we need to use Python-based server\n",
        "    return \"PYTHON_SERVER\"\n",
        "\n",
        "def ensure_binary():\n",
        "    \"\"\"\n",
        "    Ensure the llama-server binary is available.\n",
        "    Returns True if successful, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        binary = get_binary_path()\n",
        "        if binary == \"PYTHON_SERVER\":\n",
        "            # We'll use llama-cpp-python's Python interface\n",
        "            return True\n",
        "        return binary is not None\n",
        "    except Exception as e:\n",
        "        print(f\"[llama_cpp_binaries] Error: {e}\")\n",
        "        return False\n",
        "'''\n",
        "\n",
        "    try:\n",
        "        modules_dir.mkdir(parents=True, exist_ok=True)\n",
        "        wrapper_file.write_text(wrapper_code, encoding=\"utf-8\")\n",
        "        print(f\"[‚úì] Created {wrapper_file}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[error] Could not create wrapper: {e}\")\n",
        "        return False\n",
        "\n",
        "def patch_llama_cpp_server():\n",
        "    \"\"\"\n",
        "    Patch llama_cpp_server.py to work without external binaries.\n",
        "    This fixes the import and makes it work with CPU-only llama-cpp-python.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîß Patching llama_cpp_server.py for CPU compatibility...\")\n",
        "\n",
        "    server_file = WORK_DIR / \"modules\" / \"llama_cpp_server.py\"\n",
        "    if not server_file.exists():\n",
        "        print(\"[warn] llama_cpp_server.py not found\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        content = server_file.read_text(encoding=\"utf-8\")\n",
        "\n",
        "        # Replace the llama_cpp_binaries import with our wrapper\n",
        "        if \"import llama_cpp_binaries\" in content:\n",
        "            # The import is already there, just ensure our wrapper is used\n",
        "            print(\"[info] llama_cpp_binaries import found, our wrapper will be used\")\n",
        "\n",
        "        # Add CPU fallback logic in _start_server if not present\n",
        "        if \"has_gpu = torch.cuda.is_available()\" not in content:\n",
        "            # Add GPU detection logic\n",
        "            content = content.replace(\n",
        "                \"def _start_server(self):\",\n",
        "                \"\"\"def _start_server(self):\n",
        "        # AUTO-DETECT GPU availability\n",
        "        import torch\n",
        "        has_gpu = torch.cuda.is_available()\n",
        "        if not has_gpu:\n",
        "            logger.warning(\"No GPU detected - running in CPU-only mode\")\n",
        "            # Force CPU settings\n",
        "            if shared.args.gpu_layers > 0:\n",
        "                logger.info(f\"Overriding gpu_layers from {shared.args.gpu_layers} to 0 (CPU mode)\")\n",
        "                shared.args.gpu_layers = 0\"\"\"\n",
        "            )\n",
        "\n",
        "        server_file.write_text(content, encoding=\"utf-8\")\n",
        "        print(\"[‚úì] Patched llama_cpp_server.py\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not patch llama_cpp_server.py: {e}\")\n",
        "\n",
        "def create_model_loader_config():\n",
        "    \"\"\"Create a model-config.yaml with CPU-optimized loader settings\"\"\"\n",
        "    config_file = WORK_DIR / \"model-config.yaml\"\n",
        "    config_content = \"\"\"# CPU-Optimized Model Loader Configuration\n",
        "\n",
        "default:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: 0\n",
        "  n_ctx: 2048\n",
        "  n_batch: 512\n",
        "  threads: 4\n",
        "  use_mmap: true\n",
        "  use_mlock: false\n",
        "\n",
        "# For GGUF models (recommended for CPU)\n",
        "*.gguf:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: 0\n",
        "  n_ctx: 2048\n",
        "  threads: 4\n",
        "\n",
        "# For transformers models\n",
        "*.safetensors:\n",
        "  loader: Transformers\n",
        "  cpu: true\n",
        "  load_in_8bit: false\n",
        "  load_in_4bit: false\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        config_file.write_text(config_content, encoding=\"utf-8\")\n",
        "        print(f\"[‚úì] Created model loader config\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not create model config: {e}\")\n",
        "\n",
        "def create_cpu_requirements():\n",
        "    \"\"\"Create a requirements override file for CPU-only dependencies\"\"\"\n",
        "    req_file = WORK_DIR / \"requirements_cpu.txt\"\n",
        "    cpu_requirements = \"\"\"# CPU-only requirements override\n",
        "torch\n",
        "torchvision\n",
        "torchaudio\n",
        "llama-cpp-python\n",
        "transformers>=4.35.0\n",
        "accelerate\n",
        "sentencepiece\n",
        "protobuf\n",
        "gradio>=3.50.0\n",
        "requests\n",
        "\"\"\"\n",
        "    try:\n",
        "        req_file.write_text(cpu_requirements, encoding=\"utf-8\")\n",
        "        print(f\"[‚úì] Created CPU requirements file\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not create CPU requirements file: {e}\")\n",
        "\n",
        "def patch_gradio_launch():\n",
        "    \"\"\"\n",
        "    Patch the server.py to ensure share=True is properly set\n",
        "    \"\"\"\n",
        "    server_py = WORK_DIR / \"server.py\"\n",
        "    if not server_py.exists():\n",
        "        print(\"[warn] server.py not found, cannot patch\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        content = server_py.read_text(encoding=\"utf-8\")\n",
        "\n",
        "        # Ensure launch has share parameter\n",
        "        if '.launch(' in content and 'share=' not in content:\n",
        "            # Add share parameter\n",
        "            content = re.sub(\n",
        "                r'\\.launch\\((.*?)\\)',\n",
        "                r'.launch(\\1, share=True)',\n",
        "                content\n",
        "            )\n",
        "            server_py.write_text(content, encoding=\"utf-8\")\n",
        "            print(\"[‚úì] Patched server.py for public URL sharing\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not patch server.py: {e}\")\n",
        "\n",
        "# ---------- Main flow ----------\n",
        "print(\"=\" * 70)\n",
        "print(\"MY-AI-Gizmo CPU-Optimized Launcher (FULLY FIXED)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "if IN_COLAB:\n",
        "    try:\n",
        "        print(\"[info] Mounting Google Drive...\")\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[‚úì] Google Drive mounted\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Could not mount Drive: {e}\")\n",
        "\n",
        "cleanup_broken_files(DRIVE_ROOT)\n",
        "\n",
        "# Download and extract repository\n",
        "if not download_repo_if_missing() and not WORK_DIR.exists():\n",
        "    raise SystemExit(\"Repository unavailable. Fix network/REPO_ZIP and retry.\")\n",
        "\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "ensure_symlinks_and_files()\n",
        "prepare_settings_file()\n",
        "create_cpu_requirements()\n",
        "create_model_loader_config()\n",
        "\n",
        "# CRITICAL: Don't remove llama.cpp - just ensure it won't build\n",
        "# Instead, we'll use llama-cpp-python's binaries\n",
        "llama_dir = WORK_DIR / \"repositories\" / \"llama.cpp\"\n",
        "if llama_dir.exists():\n",
        "    # Create a marker file to skip building\n",
        "    try:\n",
        "        (llama_dir / \".skip_build\").touch()\n",
        "        print(\"[info] Marked llama.cpp directory to skip building\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Prepare environment for CPU-only installation\n",
        "MPL_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "start_sh = WORK_DIR / \"start_linux.sh\"\n",
        "installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "\n",
        "install_env = os.environ.copy()\n",
        "install_env.update({\n",
        "    # Matplotlib backend\n",
        "    \"MPLBACKEND\": \"Agg\",\n",
        "    \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "\n",
        "    # Installation options\n",
        "    \"GPU_CHOICE\": \"N\",\n",
        "    \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "    \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "\n",
        "    # Force CPU-only builds\n",
        "    \"CUDA_VISIBLE_DEVICES\": \"\",\n",
        "    \"TORCH_CUDA_ARCH_LIST\": \"\",\n",
        "    \"CMAKE_ARGS\": \"-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF -DLLAMA_METAL=OFF -DLLAMA_OPENCL=OFF\",\n",
        "    \"FORCE_CMAKE\": \"1\",\n",
        "    \"CUDACXX\": \"\",\n",
        "\n",
        "    # Skip tests and builds\n",
        "    \"SKIP_LLAMACPP_BUILD\": \"TRUE\",\n",
        "    \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "    \"FORCE_CUDA\": \"FALSE\",\n",
        "})\n",
        "\n",
        "print(\"\\nüì¶ Installing dependencies (CPU-only mode)...\")\n",
        "print(f\"Installer log -> {installer_log}\")\n",
        "\n",
        "if start_sh.exists():\n",
        "    sh(\"chmod +x start_linux.sh\")\n",
        "\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] Running installer...\")\n",
        "        code, url = stream_with_heartbeat(\n",
        "            \"bash start_linux.sh\",\n",
        "            cwd=str(WORK_DIR),\n",
        "            env=install_env,\n",
        "            logfile_path=str(installer_log),\n",
        "            capture_url_to=str(PUBLIC_URL_FILE)\n",
        "        )\n",
        "\n",
        "        if code != 0:\n",
        "            print(f\"[warn] Installer exited with code {code}. See {installer_log}\")\n",
        "        else:\n",
        "            print(f\"[‚úì] Installer completed\")\n",
        "    else:\n",
        "        print(\"[info] Virtual environment exists, skipping full install\")\n",
        "\n",
        "    # CRITICAL FIXES for llama.cpp support\n",
        "    install_llama_cpp_python_cpu()\n",
        "    create_llama_cpp_binaries_wrapper()\n",
        "    patch_llama_cpp_server()\n",
        "    patch_gradio_launch()\n",
        "\n",
        "else:\n",
        "    print(\"[error] start_linux.sh not found!\")\n",
        "    raise SystemExit(\"Cannot proceed without installation script\")\n",
        "\n",
        "# Create enhanced server wrapper\n",
        "launch_wrapper = WORK_DIR / \"_launch_with_share.py\"\n",
        "launch_wrapper_code = f\"\"\"# Enhanced launch wrapper with CPU mode and public sharing\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Force CPU-only environment\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['MPLCONFIGDIR'] = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE'] = '1'\n",
        "\n",
        "# Add launch flags\n",
        "if '--listen' not in sys.argv:\n",
        "    sys.argv.append('--listen')\n",
        "if '--share' not in sys.argv:\n",
        "    sys.argv.append('--share')\n",
        "if '--auto-launch' not in sys.argv:\n",
        "    sys.argv.append('--auto-launch')\n",
        "\n",
        "print(\"[INFO] Launch flags:\", ' '.join(sys.argv))\n",
        "print(\"[INFO] CPU-only mode enabled\")\n",
        "\n",
        "# Force matplotlib Agg backend\n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg', force=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Disable CUDA in PyTorch\n",
        "try:\n",
        "    import torch\n",
        "    if hasattr(torch, 'cuda'):\n",
        "        torch.cuda.is_available = lambda: False\n",
        "        print(\"[CPU MODE] CUDA disabled\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "import runpy\n",
        "runpy.run_path('server.py', run_name='__main__')\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    launch_wrapper.write_text(launch_wrapper_code, encoding=\"utf-8\")\n",
        "    print(\"[‚úì] Created launch wrapper\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] Could not create wrapper: {e}\")\n",
        "\n",
        "# Kill any stray processes\n",
        "try:\n",
        "    sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "    sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "except Exception:\n",
        "    pass\n",
        "time.sleep(2)\n",
        "\n",
        "# Launch the server\n",
        "server_log = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "launch_cmd = f'{python_exe} -u \"{launch_wrapper.name}\"'\n",
        "\n",
        "server_env = os.environ.copy()\n",
        "server_env.update({\n",
        "    \"CUDA_VISIBLE_DEVICES\": \"\",\n",
        "    \"MPLBACKEND\": \"Agg\",\n",
        "    \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "    \"GRADIO_SERVER_NAME\": \"0.0.0.0\",\n",
        "    \"GRADIO_SHARE\": \"1\",\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ LAUNCHING WEB UI (CPU MODE)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Server log -> {server_log}\")\n",
        "print(\"\\n‚öôÔ∏è  CPU OPTIMIZATION ACTIVE:\")\n",
        "print(\"  ‚Ä¢ llama-cpp-python: CPU-only build\")\n",
        "print(\"  ‚Ä¢ Recommended loader: llama.cpp\")\n",
        "print(\"  ‚Ä¢ Fallback: Transformers\")\n",
        "print(\"  ‚Ä¢ Context length: ‚â§ 2048 for best speed\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "print(\"‚è≥ Starting server (may take 1-2 minutes)...\\n\")\n",
        "\n",
        "code, captured = stream_with_heartbeat(\n",
        "    launch_cmd,\n",
        "    cwd=str(WORK_DIR),\n",
        "    env=server_env,\n",
        "    logfile_path=str(server_log),\n",
        "    capture_url_to=str(PUBLIC_URL_FILE)\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "if captured:\n",
        "    print(f\"‚úÖ WEB UI READY!\")\n",
        "    print(f\"üåê PUBLIC URL: {captured}\")\n",
        "    print(\"=\" * 70)\n",
        "    try:\n",
        "        PUBLIC_URL_FILE.write_text(captured, encoding=\"utf-8\")\n",
        "        print(f\"[‚úì] URL saved to: {PUBLIC_URL_FILE}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"\\nüìã NEXT STEPS:\")\n",
        "    print(\"  1. Click the URL above\")\n",
        "    print(\"  2. Go to 'Model' tab\")\n",
        "    print(\"  3. Select 'llama.cpp' loader\")\n",
        "    print(\"  4. Download a GGUF model or load existing\")\n",
        "    print(\"  5. Click 'Load' and start chatting!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  NO PUBLIC URL CAPTURED\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nüìù Check server log: {server_log}\")\n",
        "    if PUBLIC_URL_FILE.exists():\n",
        "        try:\n",
        "            saved_url = PUBLIC_URL_FILE.read_text().strip()\n",
        "            if saved_url:\n",
        "                print(f\"üîó Previously saved URL: {saved_url}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "if code != 0:\n",
        "    print(f\"\\n[warn] Server exited with code {code}\")\n",
        "else:\n",
        "    print(\"\\n[info] Server terminated normally\")\n",
        "\n",
        "print(\"\\n‚úÖ Data location:\", DRIVE_ROOT)\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}