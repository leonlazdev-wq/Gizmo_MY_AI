{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonlazdev-wq/Gizmo-my-ai-for-google-colab/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ================================================================\n",
        "# MY-AI-Gizmo ‚Ä¢ UNIVERSAL LAUNCHER  v3.5.2 - COLAB READY\n",
        "# ================================================================\n",
        "# v3.5.2 CHANGES:\n",
        "#  üîß FIX: String escaping in launch wrapper (was causing SyntaxError)\n",
        "#  üîß FIX: Simplified print statements to avoid f-string nesting issues\n",
        "#  ‚úÖ All v3.5.1 features kept (repo update menu, token saving, etc.)\n",
        "#  ‚úÖ TESTED: Works in Google Colab CPU mode\n",
        "# ================================================================\n",
        "\n",
        "import os, sys, subprocess, shutil, re, time, threading\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from google.colab import drive as colab_drive\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    colab_drive = None\n",
        "    IN_COLAB = False\n",
        "\n",
        "# ‚îÄ‚îÄ Repo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "GITHUB_USER   = \"leonlazdev-wq\"\n",
        "GITHUB_REPO   = \"Gizmo-my-ai-for-google-colab\"\n",
        "GITHUB_BRANCH = \"main\"\n",
        "REPO_ZIP       = \"\"\n",
        "REPO_CLONE_URL = \"\"\n",
        "\n",
        "# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "WORK_DIR           = Path(\"/content/text-generation-webui\")\n",
        "DRIVE_ROOT         = None\n",
        "LOG_DIR            = None\n",
        "MPL_CONFIG_DIR     = None\n",
        "PUBLIC_URL_FILE    = None\n",
        "HEARTBEAT_INTERVAL = 30\n",
        "MAX_RESTARTS       = 3\n",
        "\n",
        "# ‚îÄ‚îÄ Model menu ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "MODEL_MENU = [\n",
        "    (\"1  TinyLlama-1.1B  Q4_K_M  [~0.7 GB]  <- fastest\",\n",
        "     \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
        "     \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", 0.7),\n",
        "    (\"2  Phi-3-mini-4k   Q4_K_M  [~2.2 GB]  <- great quality/speed\",\n",
        "     \"bartowski/Phi-3-mini-4k-instruct-GGUF\",\n",
        "     \"Phi-3-mini-4k-instruct-Q4_K_M.gguf\", 2.2),\n",
        "    (\"3  Mistral-7B-v0.3  Q4_K_M  [~4.4 GB]  <- best general 7B\",\n",
        "     \"bartowski/Mistral-7B-v0.3-GGUF\",\n",
        "     \"Mistral-7B-v0.3-Q4_K_M.gguf\", 4.4),\n",
        "    (\"4  Qwen2.5-Coder-7B  Q4_K_M  [~4.7 GB]  <- best coding 7B\",\n",
        "     \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-7b-instruct-q4_k_m.gguf\", 4.7),\n",
        "    (\"5  Qwen2.5-Coder-14B  Q4_K_M  [~8.9 GB]  <- needs 10+ GB RAM\",\n",
        "     \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\",\n",
        "     \"qwen2.5-coder-14b-instruct-q4_k_m.gguf\", 8.9),\n",
        "    (\"6  DeepSeek-Coder-33B  Q4_K_M  [~19 GB]  <- GPU only\",\n",
        "     \"TheBloke/deepseek-coder-33B-instruct-GGUF\",\n",
        "     \"deepseek-coder-33b-instruct.Q4_K_M.gguf\", 19.0),\n",
        "    (\"7  Custom ‚Äî enter your own HF repo + filename\", \"\", \"\", 0),\n",
        "]\n",
        "\n",
        "# ‚îÄ‚îÄ Globals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "GITHUB_TOKEN = \"\"\n",
        "MODEL_REPO   = \"\"\n",
        "MODEL_FILE   = \"\"\n",
        "USE_MODEL    = False\n",
        "GPU_LAYERS   = -1\n",
        "N_CTX        = 4096\n",
        "USE_GPU      = True\n",
        "\n",
        "URL_PATTERNS = [\n",
        "    re.compile(r'Running on public URL:\\s*(https?://\\S+)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.gradio\\.live\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.trycloudflare\\.com\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(https?://[a-zA-Z0-9\\-]+\\.ngrok\\S*)', re.IGNORECASE),\n",
        "    re.compile(r'(?:public|share|tunnel|external)[^\\n]{0,40}(https?://\\S+)', re.IGNORECASE),\n",
        "]\n",
        "URL_KEYWORDS = (\"gradio.live\", \"trycloudflare.com\", \"ngrok\", \"loca.lt\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  GITHUB TOKEN\n",
        "# =============================================================================\n",
        "\n",
        "def _token_file_path():\n",
        "    if Path(\"/content/drive/MyDrive\").exists():\n",
        "        return Path(\"/content/drive/MyDrive/MY-AI-Gizmo/github_token.txt\")\n",
        "    return Path(\"/content/MY-AI-Gizmo/github_token.txt\")\n",
        "\n",
        "def _load_saved_token():\n",
        "    for candidate in (\n",
        "        Path(\"/content/drive/MyDrive/MY-AI-Gizmo/github_token.txt\"),\n",
        "        Path(\"/content/MY-AI-Gizmo/github_token.txt\"),\n",
        "    ):\n",
        "        if candidate.exists():\n",
        "            try:\n",
        "                tok = candidate.read_text(encoding=\"utf-8\").strip()\n",
        "                if len(tok) >= 10:\n",
        "                    return tok\n",
        "            except Exception:\n",
        "                pass\n",
        "    return \"\"\n",
        "\n",
        "def _save_token(token):\n",
        "    path = _token_file_path()\n",
        "    try:\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        path.write_text(token, encoding=\"utf-8\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [warn] Could not save token: {e}\")\n",
        "\n",
        "def _build_urls():\n",
        "    global REPO_ZIP, REPO_CLONE_URL\n",
        "    REPO_ZIP       = (f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}\"\n",
        "                      f\"/archive/refs/heads/{GITHUB_BRANCH}.zip\")\n",
        "    REPO_CLONE_URL = (f\"https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\")\n",
        "\n",
        "def setup_github_token():\n",
        "    global GITHUB_TOKEN\n",
        "    print(\"=\" * 70)\n",
        "    print(\"  MY-AI-Gizmo  v3.5.2  ‚Äî GitHub Authentication\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "    saved = _load_saved_token()\n",
        "    if saved:\n",
        "        last3 = saved[-3:]\n",
        "        print(f\"  [üíæ] Token found  ( ends in: ...{last3} )\")\n",
        "        ans = input(\"  Use this saved token? (y = yes / n = enter a new one): \").strip().lower()\n",
        "        if ans != \"n\":\n",
        "            GITHUB_TOKEN = saved\n",
        "            print(f\"  [‚úì] Using saved token  ...{last3}\")\n",
        "            print(\"=\" * 70)\n",
        "            _build_urls()\n",
        "            return\n",
        "    print(\"  Your repo is PRIVATE. A Personal Access Token is required.\")\n",
        "    print()\n",
        "    print(\"  How to get a token:\")\n",
        "    print(\"    1. Go to https://github.com/settings/tokens\")\n",
        "    print(\"    2. Personal Access Tokens ‚Üí Tokens (classic)\")\n",
        "    print(\"    3. Generate new token (classic)\")\n",
        "    print(\"    4. Set scope: ‚úì repo   (full control of private repos)\")\n",
        "    print(\"    5. Copy the token  (starts with  ghp_...)\")\n",
        "    print()\n",
        "    while True:\n",
        "        token = input(\"  Paste your GitHub token here: \").strip()\n",
        "        if not token:\n",
        "            print(\"  [!] Token cannot be empty. Try again.\")\n",
        "            continue\n",
        "        if not (token.startswith(\"ghp_\") or token.startswith(\"github_pat_\") or len(token) >= 20):\n",
        "            confirm = input(\"  [?] Token looks unusual. Continue anyway? (y/n): \").strip().lower()\n",
        "            if confirm != \"y\":\n",
        "                continue\n",
        "        GITHUB_TOKEN = token\n",
        "        break\n",
        "    _build_urls()\n",
        "    _save_token(GITHUB_TOKEN)\n",
        "    last3 = GITHUB_TOKEN[-3:]\n",
        "    print()\n",
        "    print(f\"  [‚úì] Token accepted & saved  ( ends in: ...{last3} )\")\n",
        "    print(\"  üíæ Token saved ‚Äî next launch will ask if you want to reuse it.\")\n",
        "    print(\"=\" * 70)\n",
        "    print()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  REPO UPDATE CHECK\n",
        "# =============================================================================\n",
        "\n",
        "def _kill_old_servers():\n",
        "    print(\"  [üõë] Killing old servers...\")\n",
        "    sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "    sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "    sh(\"pkill -9 -f '_gizmo_launch'\")\n",
        "    time.sleep(2)\n",
        "    print(\"  [‚úì] Servers stopped\")\n",
        "\n",
        "\n",
        "def check_repo_update():\n",
        "    \"\"\"\n",
        "    Ask if user updated the repo on GitHub.\n",
        "    y ‚Üí wipe local copy and re-clone fresh (gets new tabs, new code)\n",
        "    n ‚Üí use existing files (fastest, ~2 min launch)\n",
        "    Returns: 'fresh' | 'keep' | 'new'\n",
        "    \"\"\"\n",
        "    if not WORK_DIR.exists():\n",
        "        return 'new'  # Nothing local yet\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"  REPO UPDATE CHECK\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"  Did you update / push changes to your GitHub repo?\")\n",
        "    print()\n",
        "    print(\"  y  ‚Äî YES, re-clone fresh  (new tabs/features will appear)\")\n",
        "    print(\"        Note: re-installs Python env on first time, ~10 min\")\n",
        "    print(\"  n  ‚Äî NO, keep existing files  (2 min launch, no reinstall)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    while True:\n",
        "        ans = input(\"  Updated repo? (y/n): \").strip().lower()\n",
        "        if ans in (\"y\", \"yes\"):\n",
        "            return 'fresh'\n",
        "        elif ans in (\"n\", \"no\"):\n",
        "            return 'keep'\n",
        "        print(\"  Please type  y  or  n\")\n",
        "\n",
        "\n",
        "def apply_repo_update(mode):\n",
        "    \"\"\"\n",
        "    'fresh' ‚Üí kill servers + wipe WORK_DIR so clone_repo() runs fresh.\n",
        "    'keep'/'new' ‚Üí nothing to do here.\n",
        "    \"\"\"\n",
        "    if mode == 'fresh':\n",
        "        print(\"\\n  [üîÑ] Wiping old repo so your updates load correctly...\")\n",
        "        _kill_old_servers()\n",
        "        if WORK_DIR.exists():\n",
        "            print(f\"  [üóëÔ∏è] Removing {WORK_DIR} ...\")\n",
        "            try:\n",
        "                shutil.rmtree(WORK_DIR)\n",
        "                print(\"  [‚úì] Old repo removed\")\n",
        "            except Exception as e:\n",
        "                print(f\"  [warn] shutil failed ({e}) ‚Äî trying shell rm...\")\n",
        "                sh(f\"rm -rf '{WORK_DIR}'\")\n",
        "                if not WORK_DIR.exists():\n",
        "                    print(\"  [‚úì] Old repo removed\")\n",
        "                else:\n",
        "                    print(\"  [warn] Could not fully remove ‚Äî will overwrite\")\n",
        "        print(\"  [‚úì] Ready for fresh clone\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  DRIVE SETUP\n",
        "# =============================================================================\n",
        "\n",
        "def mount_drive_if_needed():\n",
        "    if not IN_COLAB:\n",
        "        return False\n",
        "    if Path(\"/content/drive/MyDrive\").exists():\n",
        "        print(\"[info] Google Drive already mounted\")\n",
        "        return True\n",
        "    try:\n",
        "        colab_drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[‚úì] Google Drive mounted\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] Drive mount failed ({e}) ‚Äî using local storage\")\n",
        "        return False\n",
        "\n",
        "def setup_drive_root(drive_ok):\n",
        "    global DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR, PUBLIC_URL_FILE\n",
        "    DRIVE_ROOT      = (Path(\"/content/drive/MyDrive/MY-AI-Gizmo\") if drive_ok\n",
        "                       else Path(\"/content/MY-AI-Gizmo\"))\n",
        "    LOG_DIR         = DRIVE_ROOT / \"logs\"\n",
        "    MPL_CONFIG_DIR  = DRIVE_ROOT / \"matplotlib\"\n",
        "    PUBLIC_URL_FILE = DRIVE_ROOT / \"public_url.txt\"\n",
        "    if not drive_ok:\n",
        "        print(f\"[info] Local storage: {DRIVE_ROOT}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "def sh(cmd, cwd=None, env=None):\n",
        "    return subprocess.run(cmd, shell=True, cwd=cwd, env=env,\n",
        "                          capture_output=True, text=True)\n",
        "\n",
        "def get_free_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemAvailable\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def get_total_ram_gb():\n",
        "    try:\n",
        "        with open(\"/proc/meminfo\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"MemTotal\"):\n",
        "                    return int(line.split()[1]) / 1024 / 1024\n",
        "    except Exception:\n",
        "        pass\n",
        "    return 0.0\n",
        "\n",
        "def auto_thread_count():\n",
        "    try:\n",
        "        import multiprocessing\n",
        "        return max(1, min(multiprocessing.cpu_count() - 1, 4))\n",
        "    except Exception:\n",
        "        return 2\n",
        "\n",
        "def auto_ctx_size(model_gb):\n",
        "    free = get_free_ram_gb() - model_gb - 0.5\n",
        "    if free >= 2.0: return 4096\n",
        "    if free >= 1.0: return 2048\n",
        "    if free >= 0.5: return 1024\n",
        "    return 512\n",
        "\n",
        "def print_ram_status():\n",
        "    free  = get_free_ram_gb()\n",
        "    total = get_total_ram_gb()\n",
        "    used  = total - free\n",
        "    pct   = (used / total) if total else 0\n",
        "    bar   = \"‚ñà\" * int(pct * 20) + \"‚ñë\" * (20 - int(pct * 20))\n",
        "    print(f\"  RAM [{bar}]  {used:.1f}/{total:.1f} GB  ({free:.1f} GB free)\")\n",
        "\n",
        "def list_local_models():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists():\n",
        "        return []\n",
        "    found = []\n",
        "    for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]:\n",
        "        found.extend(d.rglob(ext))\n",
        "    return sorted(found)\n",
        "\n",
        "def cleanup_broken_files():\n",
        "    d = DRIVE_ROOT / \"models\"\n",
        "    if not d.exists():\n",
        "        return\n",
        "    broken = [f for ext in [\"*.gguf\", \"*.safetensors\", \"*.bin\"]\n",
        "              for f in d.rglob(ext) if f.stat().st_size < 100 * 1024]\n",
        "    if broken:\n",
        "        print(f\"[info] Removing {len(broken)} broken model file(s)\")\n",
        "        for f in broken:\n",
        "            try: f.unlink()\n",
        "            except Exception: pass\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  STREAM + HEARTBEAT\n",
        "# =============================================================================\n",
        "\n",
        "def stream_with_heartbeat(cmd, cwd=None, env=None, logfile_path=None):\n",
        "    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n",
        "                            stderr=subprocess.STDOUT, cwd=cwd, env=env,\n",
        "                            text=True, bufsize=1)\n",
        "    stop   = threading.Event()\n",
        "    last_t = [time.time()]\n",
        "\n",
        "    def heartbeat():\n",
        "        while not stop.wait(HEARTBEAT_INTERVAL):\n",
        "            if time.time() - last_t[0] >= HEARTBEAT_INTERVAL:\n",
        "                print(\"[heartbeat] still working...\")\n",
        "\n",
        "    hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "    hb.start()\n",
        "    logfile = open(logfile_path, \"a\", encoding=\"utf-8\") if logfile_path else None\n",
        "    try:\n",
        "        for line in proc.stdout:\n",
        "            last_t[0] = time.time()\n",
        "            print(line, end=\"\")\n",
        "            if logfile:\n",
        "                try: logfile.write(line)\n",
        "                except Exception: pass\n",
        "    except Exception as e:\n",
        "        print(f\"[stream error] {e}\")\n",
        "    finally:\n",
        "        proc.wait()\n",
        "        stop.set()\n",
        "        hb.join(timeout=1)\n",
        "        if logfile:\n",
        "            try: logfile.close()\n",
        "            except Exception: pass\n",
        "    return proc.returncode\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  SYMLINKS\n",
        "# =============================================================================\n",
        "\n",
        "def ensure_symlinks_and_files():\n",
        "    links_map = [\n",
        "        (\"user_data/models\",        \"models\",                 False),\n",
        "        (\"models\",                  \"models\",                 False),\n",
        "        (\"user_data/loras\",         \"loras\",                  False),\n",
        "        (\"user_data/characters\",    \"characters\",             False),\n",
        "        (\"user_data/presets\",       \"presets\",                False),\n",
        "        (\"user_data/settings.yaml\", \"settings/settings.yaml\", True),\n",
        "        (\"user_data/settings.json\", \"settings/settings.json\", True),\n",
        "        (\"user_data/chat\",          \"chat-history\",           False),\n",
        "        (\"outputs\",                 \"outputs\",                False),\n",
        "    ]\n",
        "    for local_rel, drive_rel, is_file in links_map:\n",
        "        drive_path = DRIVE_ROOT / drive_rel\n",
        "        local_path = WORK_DIR / local_rel\n",
        "        if is_file:\n",
        "            drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            if not drive_path.exists():\n",
        "                drive_path.write_text(\"\", encoding=\"utf-8\")\n",
        "        else:\n",
        "            drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            if local_path.is_symlink() or local_path.is_file():\n",
        "                local_path.unlink()\n",
        "            elif local_path.is_dir():\n",
        "                shutil.rmtree(local_path)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] clearing {local_path}: {e}\")\n",
        "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            os.symlink(str(drive_path), str(local_path),\n",
        "                       target_is_directory=not is_file)\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] symlink {local_path}: {e}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  CONFIG FILES\n",
        "# =============================================================================\n",
        "\n",
        "def write_settings_yaml():\n",
        "    threads    = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_line = f\"model: {MODEL_FILE}\" if (USE_MODEL and MODEL_FILE) else \"model: None\"\n",
        "    content = f\"\"\"# MY-AI-Gizmo Settings ‚Äî {mode_label} (v3.5.2 {datetime.now().strftime('%Y-%m-%d %H:%M')})\n",
        "listen: true\n",
        "share: true\n",
        "auto_launch: false\n",
        "loader: llama.cpp\n",
        "n_ctx: {N_CTX}\n",
        "n_batch: 512\n",
        "n_gpu_layers: {GPU_LAYERS}\n",
        "threads: {threads}\n",
        "character: Debug\n",
        "{model_line}\n",
        "chat_style: cai-chat\n",
        "api: true\n",
        "api_port: 5000\n",
        "\"\"\"\n",
        "    for path in (WORK_DIR / \"user_data\" / \"settings.yaml\",\n",
        "                 DRIVE_ROOT / \"settings\" / \"settings.yaml\"):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(f\"[‚úì] settings.yaml ({mode_label} / {MODEL_FILE or 'none'} / {threads}t / ctx {N_CTX})\")\n",
        "\n",
        "def write_cmd_flags():\n",
        "    threads = auto_thread_count()\n",
        "    flags = [\n",
        "        \"--listen\", \"--share\", \"--verbose\",\n",
        "        \"--api\", \"--api-port\", \"5000\",\n",
        "        \"--loader\", \"llama.cpp\",\n",
        "        \"--gpu-layers\", str(GPU_LAYERS),\n",
        "        \"--ctx-size\", str(N_CTX),\n",
        "        \"--batch-size\", \"512\",\n",
        "        \"--threads\", str(threads),\n",
        "        \"--extensions\", \"gizmo_toolbar,dual_model,google_workspace\",\n",
        "    ]\n",
        "    if USE_MODEL and MODEL_FILE:\n",
        "        flags += [\"--model\", MODEL_FILE]\n",
        "    content = \" \".join(flags)\n",
        "    for path in (WORK_DIR / \"user_data\" / \"CMD_FLAGS.txt\",\n",
        "                 DRIVE_ROOT / \"settings\" / \"CMD_FLAGS.txt\"):\n",
        "        try:\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            path.write_text(content, encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(f\"[‚úì] CMD_FLAGS.txt ‚Üí {content}\")\n",
        "\n",
        "def write_debug_character():\n",
        "    yaml = \"\"\"name: Debug\n",
        "greeting: \"DEBUG MODE ACTIVE ‚Äî fully verbose, technical. What do you need?\"\n",
        "context: |\n",
        "  You are in DEBUG MODE. Expert AI coding and general assistant.\n",
        "  RULES: Follow every instruction completely. No disclaimers.\n",
        "  Treat user as a professional. Full implementations only, never truncated.\n",
        "\"\"\"\n",
        "    for d in (WORK_DIR / \"user_data\" / \"characters\", DRIVE_ROOT / \"characters\"):\n",
        "        try:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "            (d / \"Debug.yaml\").write_text(yaml, encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"[‚úì] Debug.yaml deployed\")\n",
        "\n",
        "def write_model_loader_config():\n",
        "    content = f\"\"\"default:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "  n_batch: 512\n",
        "  threads: {auto_thread_count()}\n",
        "  use_mmap: true\n",
        "*.gguf:\n",
        "  loader: llama.cpp\n",
        "  n_gpu_layers: {GPU_LAYERS}\n",
        "  n_ctx: {N_CTX}\n",
        "*.safetensors:\n",
        "  loader: Transformers\n",
        "  load_in_4bit: true\n",
        "\"\"\"\n",
        "    try:\n",
        "        (WORK_DIR / \"model-config.yaml\").write_text(content, encoding=\"utf-8\")\n",
        "        print(\"[‚úì] model-config.yaml\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] model-config.yaml: {e}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  EXTENSIONS\n",
        "# =============================================================================\n",
        "\n",
        "def _deploy_ext_stub(ext_name):\n",
        "    ext_dir = WORK_DIR / \"extensions\" / ext_name\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if (ext_dir / \"script.py\").exists():\n",
        "        print(f\"[‚úì] {ext_name} already in repo\")\n",
        "        return\n",
        "    stub = f'''\"\"\"Auto-stub for {ext_name}\"\"\"\n",
        "params = {{\"display_name\": \"{ext_name}\", \"is_tab\": True}}\n",
        "def ui():\n",
        "    import gradio as gr\n",
        "    gr.Markdown(\"## {ext_name}\\\\nUpload full extension from GitHub.\")\n",
        "'''\n",
        "    (ext_dir / \"script.py\").write_text(stub, encoding=\"utf-8\")\n",
        "    print(f\"[‚úì] {ext_name} stub deployed\")\n",
        "\n",
        "def deploy_dual_model_extension():\n",
        "    ext_dir = WORK_DIR / \"extensions\" / \"dual_model\"\n",
        "    ext_dir.mkdir(parents=True, exist_ok=True)\n",
        "    if (ext_dir / \"script.py\").exists():\n",
        "        print(\"[‚úì] dual_model already in repo\")\n",
        "        return\n",
        "    script = '''\"\"\"MY-AI-Gizmo ‚Äî Dual Model Extension\"\"\"\n",
        "import gc, threading, gradio as gr\n",
        "try:\n",
        "    from llama_cpp import Llama\n",
        "    LLAMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LLAMA_AVAILABLE = False\n",
        "\n",
        "params = {\"display_name\": \"Dual Model\", \"is_tab\": True}\n",
        "_lock = threading.Lock()\n",
        "_model2 = None\n",
        "_model2_name = \"Not loaded\"\n",
        "\n",
        "def _status():\n",
        "    return f\"üü¢ {_model2_name}\" if _model2 else \"üî¥ Not loaded\"\n",
        "\n",
        "def ui():\n",
        "    if not LLAMA_AVAILABLE:\n",
        "        gr.Markdown(\"‚ö†Ô∏è llama-cpp-python not installed\")\n",
        "        return\n",
        "    gr.Markdown(\"## ü§ñ Dual Model\\\\nLoad a second model for comparison or pipeline use.\")\n",
        "'''\n",
        "    (ext_dir / \"script.py\").write_text(script, encoding=\"utf-8\")\n",
        "    print(\"[‚úì] dual_model deployed\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  LLAMA-CPP\n",
        "# =============================================================================\n",
        "\n",
        "def install_llama_cpp_gpu(python_exe):\n",
        "    print(\"\\nüîß Checking llama-cpp GPU...\")\n",
        "    cv = sh(\"nvcc --version\")\n",
        "    cuda_major, cuda_minor = \"12\", \"1\"\n",
        "    if cv.returncode == 0:\n",
        "        m = re.search(r'release (\\d+)\\.(\\d+)', cv.stdout)\n",
        "        if m: cuda_major, cuda_minor = m.group(1), m.group(2)\n",
        "    cuda_tag = f\"cu{cuda_major}{cuda_minor}\"\n",
        "    r = sh(f'\"{python_exe}\" -m pip install llama-cpp-binaries '\n",
        "           f'--extra-index-url https://abetlen.github.io/llama-cpp-python/whl/{cuda_tag} --no-cache-dir')\n",
        "    if r.returncode == 0:\n",
        "        print(\"[‚úì] llama-cpp-binaries (CUDA) installed\")\n",
        "        return\n",
        "    gpu_env = os.environ.copy()\n",
        "    gpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\", \"FORCE_CMAKE\": \"1\"})\n",
        "    r = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=gpu_env)\n",
        "    print(\"[‚úì] Compiled from source\" if r.returncode == 0 else \"[warn] GPU llama-cpp failed\")\n",
        "\n",
        "def install_llama_cpp_cpu(python_exe):\n",
        "    print(\"\\nüîß Installing llama-cpp (CPU)...\")\n",
        "    sh(f'\"{python_exe}\" -m pip uninstall -y llama-cpp-python llama-cpp-python-cuda')\n",
        "    cpu_env = os.environ.copy()\n",
        "    cpu_env.update({\"CMAKE_ARGS\": \"-DLLAMA_CUDA=OFF -DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\",\n",
        "                    \"FORCE_CMAKE\": \"1\", \"CUDACXX\": \"\"})\n",
        "    r = sh(f'\"{python_exe}\" -m pip install llama-cpp-python --no-cache-dir --force-reinstall', env=cpu_env)\n",
        "    print(\"[‚úì] CPU llama-cpp done\" if r.returncode == 0 else f\"[warn] code {r.returncode}\")\n",
        "\n",
        "def create_llama_cpp_wrapper(python_exe):\n",
        "    wrapper = '''\"\"\"Compatibility wrapper for llama_cpp_binaries.\"\"\"\n",
        "import os, shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def get_binary_path():\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        p = Path(llama_cpp.__file__).parent / \"bin\" / \"llama-server\"\n",
        "        if p.exists():\n",
        "            return str(p)\n",
        "    except ImportError:\n",
        "        pass\n",
        "    b = shutil.which(\"llama-server\")\n",
        "    return b or \"PYTHON_SERVER\"\n",
        "\n",
        "def ensure_binary():\n",
        "    try:\n",
        "        return get_binary_path() is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "'''\n",
        "    try:\n",
        "        modules = WORK_DIR / \"modules\"\n",
        "        modules.mkdir(parents=True, exist_ok=True)\n",
        "        (modules / \"llama_cpp_binaries.py\").write_text(wrapper, encoding=\"utf-8\")\n",
        "        print(\"[‚úì] llama_cpp_binaries.py created\")\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] wrapper: {e}\")\n",
        "\n",
        "def install_google_workspace_deps(python_exe):\n",
        "    pkgs = \"google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\"\n",
        "    print(\"\\nüîß Installing Google Workspace libs...\")\n",
        "    r = sh(f'\"{python_exe}\" -m pip install {pkgs} -q')\n",
        "    print(\"[‚úì] Google libs installed\" if r.returncode == 0 else f\"[warn] code {r.returncode}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  REPO BUG PATCHER\n",
        "# =============================================================================\n",
        "\n",
        "def patch_repo_bugs():\n",
        "    fixes = []\n",
        "    gw = WORK_DIR / \"modules\" / \"google_workspace_tools.py\"\n",
        "    if gw.exists():\n",
        "        c = gw.read_text(encoding=\"utf-8\")\n",
        "        if \"apply_slide_designer_prompt\" not in c:\n",
        "            stub = \"\"\"\n",
        "\n",
        "# AUTO-PATCHED by launcher v3.5.2\n",
        "def apply_slide_designer_prompt(prompt='', slide_index=0):\n",
        "    return f'[stub] {prompt}'\n",
        "\n",
        "def add_image_to_slide(image_path='', slide_index=0, **kw):\n",
        "    return None\n",
        "\"\"\"\n",
        "            gw.write_text(c + stub, encoding=\"utf-8\")\n",
        "            fixes.append(\"google_workspace_tools: added missing stubs\")\n",
        "\n",
        "    ui_s = WORK_DIR / \"modules\" / \"ui_session.py\"\n",
        "    if ui_s.exists():\n",
        "        c = ui_s.read_text(encoding=\"utf-8\")\n",
        "        if \"from modules.google_workspace_tools import\" in c and \"try:\" not in c[:max(0, c.find(\"google_workspace_tools\")-50)]:\n",
        "            safe = \"\"\"try:\n",
        "    from modules.google_workspace_tools import (\n",
        "        add_image_to_slide, apply_slide_designer_prompt)\n",
        "except ImportError:\n",
        "    def apply_slide_designer_prompt(*a, **kw): return ''\n",
        "    def add_image_to_slide(*a, **kw): return None\n",
        "\"\"\"\n",
        "            c = re.sub(r'from modules\\.google_workspace_tools import[^\\n]+\\n', safe, c, count=1)\n",
        "            ui_s.write_text(c, encoding=\"utf-8\")\n",
        "            fixes.append(\"ui_session.py: wrapped bad import\")\n",
        "\n",
        "    if fixes:\n",
        "        print(\"[‚úì] Patches applied:\", \", \".join(fixes))\n",
        "    else:\n",
        "        print(\"[‚úì] Repo patch check ‚Äî nothing to fix\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  MODEL DOWNLOAD\n",
        "# =============================================================================\n",
        "\n",
        "def download_model_if_missing():\n",
        "    if not USE_MODEL:\n",
        "        print(\"[info] No model selected ‚Äî skipping download\")\n",
        "        return True\n",
        "    models_dir = DRIVE_ROOT / \"models\"\n",
        "    models_dir.mkdir(parents=True, exist_ok=True)\n",
        "    model_path = models_dir / MODEL_FILE\n",
        "    if model_path.exists() and model_path.stat().st_size > 100 * 1024 * 1024:\n",
        "        print(f\"[‚úì] Model exists ({model_path.stat().st_size/(1024**3):.1f} GB)\")\n",
        "        return True\n",
        "    if not MODEL_REPO:\n",
        "        return model_path.exists()\n",
        "    print(f\"\\nüì• DOWNLOADING: {MODEL_FILE}\")\n",
        "    hf_url = f\"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILE}?download=true\"\n",
        "    for cmd in (f'wget -q --show-progress -O \"{model_path}\" \"{hf_url}\"',\n",
        "                f'curl -L --progress-bar -o \"{model_path}\" \"{hf_url}\"'):\n",
        "        r = subprocess.run(cmd, shell=True)\n",
        "        if r.returncode == 0 and model_path.exists() and model_path.stat().st_size > 100*1024*1024:\n",
        "            print(f\"[‚úì] Download complete ‚Äî {model_path.stat().st_size/(1024**3):.2f} GB\")\n",
        "            return True\n",
        "        try: model_path.unlink()\n",
        "        except Exception: pass\n",
        "    print(\"[error] Download failed.\")\n",
        "    return False\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  REPO CLONE\n",
        "# =============================================================================\n",
        "\n",
        "def clone_repo():\n",
        "    print(\"[info] Cloning repository (authenticated)...\")\n",
        "    r = sh(f\"git clone --depth=1 {REPO_CLONE_URL} {WORK_DIR}\")\n",
        "    if r.returncode == 0 and WORK_DIR.exists():\n",
        "        print(f\"[‚úì] Repo cloned to {WORK_DIR}\")\n",
        "        return True\n",
        "    print(f\"[warn] git clone failed ({r.returncode}): {r.stderr.strip()[:120]}\")\n",
        "    print(\"[info] Trying zip download fallback...\")\n",
        "    tmp_zip = Path(\"/content/repo.zip\")\n",
        "    try: tmp_zip.unlink()\n",
        "    except Exception: pass\n",
        "    for cmd in (f\"wget -q -O {tmp_zip} '{REPO_ZIP}'\",\n",
        "                f\"curl -s -L -o {tmp_zip} '{REPO_ZIP}'\"):\n",
        "        r = sh(cmd)\n",
        "        if r.returncode == 0 and tmp_zip.exists() and tmp_zip.stat().st_size > 1000:\n",
        "            break\n",
        "    else:\n",
        "        print(\"[error] Zip download also failed.\")\n",
        "        return False\n",
        "    sh(f\"unzip -q {tmp_zip} -d /content\")\n",
        "    found = next(Path(\"/content\").glob(f\"{GITHUB_REPO}-*\"), None)\n",
        "    if not found:\n",
        "        print(\"[error] Extracted folder not found.\")\n",
        "        return False\n",
        "    found.rename(WORK_DIR)\n",
        "    print(f\"[info] Repo extracted to {WORK_DIR}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  NGROK FALLBACK\n",
        "# =============================================================================\n",
        "\n",
        "def try_setup_ngrok(port=7860):\n",
        "    try:\n",
        "        sh(\"pip install pyngrok -q\")\n",
        "        from pyngrok import ngrok, conf\n",
        "        token_file = DRIVE_ROOT / \"ngrok_token.txt\"\n",
        "        if token_file.exists():\n",
        "            token = token_file.read_text().strip()\n",
        "            if token: conf.get_default().auth_token = token\n",
        "        url = ngrok.connect(port, \"http\").public_url\n",
        "        print(f\"\\n{'='*70}\\nüåê NGROK URL: {url}\\n{'='*70}\\n\")\n",
        "        try: PUBLIC_URL_FILE.write_text(url)\n",
        "        except Exception: pass\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        print(f\"[warn] ngrok: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  INTERACTIVE MENUS\n",
        "# =============================================================================\n",
        "\n",
        "def choose_mode():\n",
        "    global USE_GPU, GPU_LAYERS, N_CTX\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"  MY-AI-Gizmo  v3.5.1 ‚Äî Choose Mode\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"  RAM: {get_free_ram_gb():.1f} GB free / {get_total_ram_gb():.1f} GB total\")\n",
        "    print(\"  [1]  GPU  ‚Äî CUDA required (Colab T4/A100)\")\n",
        "    print(\"  [2]  CPU  ‚Äî Works everywhere, slower\")\n",
        "    print(\"=\" * 70)\n",
        "    while True:\n",
        "        c = input(\"  1=GPU or 2=CPU: \").strip()\n",
        "        if c == \"1\":   USE_GPU = True;  GPU_LAYERS = -1; N_CTX = 4096; break\n",
        "        elif c == \"2\": USE_GPU = False; GPU_LAYERS = 0;  break\n",
        "        else: print(\"  Enter 1 or 2.\")\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "def show_model_manager():\n",
        "    models = list_local_models()\n",
        "    if not models: return\n",
        "    print(\"\\n\" + \"‚îÄ\" * 70)\n",
        "    print(\"  MODEL MANAGER ‚Äî files in your storage\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "    for i, m in enumerate(models, 1):\n",
        "        try:   size = f\"{m.stat().st_size/(1024**3):.2f} GB\"\n",
        "        except Exception: size = \"?\"\n",
        "        print(f\"  [D{i}]  {m.name:<55} {size}\")\n",
        "    print(\"‚îÄ\" * 70)\n",
        "    print(\"  Type D1, D2... to delete a model, or Enter to continue\")\n",
        "    while True:\n",
        "        c = input(\"\\n  Choice: \").strip()\n",
        "        if not c: break\n",
        "        if c.upper().startswith(\"D\") and len(c) > 1:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                if 0 <= idx < len(models):\n",
        "                    confirm = input(f\"  Delete {models[idx].name}? (y/n): \").strip().lower()\n",
        "                    if confirm == \"y\":\n",
        "                        models[idx].unlink()\n",
        "                        print(\"  [‚úì] Deleted\")\n",
        "                        models = list_local_models()\n",
        "                else: print(\"  Invalid number.\")\n",
        "            except Exception as e: print(f\"  Error: {e}\")\n",
        "        else: print(\"  Use D1, D2... or Enter to continue.\")\n",
        "\n",
        "def choose_model():\n",
        "    global MODEL_REPO, MODEL_FILE, N_CTX, USE_MODEL\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"  MODEL SELECTOR\")\n",
        "    print(\"=\" * 70)\n",
        "    local = list_local_models()\n",
        "    if local:\n",
        "        print(\"  ‚îÄ‚îÄ On your storage ‚îÄ‚îÄ\")\n",
        "        for i, m in enumerate(local, 1):\n",
        "            try:   size = f\"{m.stat().st_size/(1024**3):.1f} GB\"\n",
        "            except Exception: size = \"?\"\n",
        "            print(f\"  [L{i}]  {m.name}  ({size})\")\n",
        "        print()\n",
        "    print(\"  ‚îÄ‚îÄ Download new ‚îÄ‚îÄ\")\n",
        "    for entry in MODEL_MENU:\n",
        "        print(f\"  {entry[0]}\")\n",
        "    print(f\"\\n  Free RAM: {get_free_ram_gb():.1f} GB\")\n",
        "    print(\"  [0]  START WITHOUT ANY MODEL  (load from UI later)\")\n",
        "    print(\"  Enter = use first local model (or download Qwen2.5-Coder-14B)\")\n",
        "    print(\"=\" * 70)\n",
        "    while True:\n",
        "        c = input(\"  Choice: \").strip()\n",
        "        if c == \"0\":\n",
        "            USE_MODEL = False; MODEL_FILE = \"\"; MODEL_REPO = \"\"; N_CTX = 4096\n",
        "            print(\"  ‚úì Starting without a model\"); break\n",
        "        if c.upper().startswith(\"L\") and local:\n",
        "            try:\n",
        "                idx = int(c[1:]) - 1\n",
        "                if 0 <= idx < len(local):\n",
        "                    sel = local[idx]; USE_MODEL = True\n",
        "                    MODEL_FILE = sel.name; MODEL_REPO = \"\"\n",
        "                    N_CTX = auto_ctx_size(sel.stat().st_size/(1024**3))\n",
        "                    print(f\"  ‚úì {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "                else: print(\"  Invalid number.\")\n",
        "            except Exception as e: print(f\"  Error: {e}\")\n",
        "            continue\n",
        "        if not c:\n",
        "            if local:\n",
        "                sel = local[0]; USE_MODEL = True; MODEL_FILE = sel.name; MODEL_REPO = \"\"\n",
        "                N_CTX = auto_ctx_size(sel.stat().st_size/(1024**3))\n",
        "                print(f\"  ‚úì {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "            else:\n",
        "                USE_MODEL = True; MODEL_REPO = MODEL_MENU[4][1]; MODEL_FILE = MODEL_MENU[4][2]\n",
        "                N_CTX = auto_ctx_size(MODEL_MENU[4][3])\n",
        "                print(f\"  ‚úì {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "        try:\n",
        "            idx = int(c) - 1\n",
        "            if idx < 0 or idx >= len(MODEL_MENU): raise ValueError()\n",
        "            entry = MODEL_MENU[idx]\n",
        "            if not entry[1]:\n",
        "                MODEL_REPO = input(\"  HF repo: \").strip()\n",
        "                MODEL_FILE = input(\"  Filename: \").strip()\n",
        "                N_CTX = 2048\n",
        "            else:\n",
        "                MODEL_REPO, MODEL_FILE = entry[1], entry[2]\n",
        "                N_CTX = auto_ctx_size(entry[3])\n",
        "            USE_MODEL = True\n",
        "            print(f\"  ‚úì {MODEL_FILE}  (ctx={N_CTX})\"); break\n",
        "        except ValueError:\n",
        "            print(\"  Invalid. Enter 0, L1/L2..., 1-7, or press Enter.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  GRADIO LAUNCH WRAPPER  (FIXED STRING ESCAPING)\n",
        "# =============================================================================\n",
        "\n",
        "def build_launch_wrapper(python_exe):\n",
        "    threads    = auto_thread_count()\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"NO MODEL\"\n",
        "\n",
        "    # Build flag components\n",
        "    cpu_flag   = \"'--cpu',\" if not USE_GPU else \"\"\n",
        "    cuda_block = \"os.environ['CUDA_VISIBLE_DEVICES'] = ''\" if not USE_GPU else \"\"\n",
        "    model_flag = f\"'--model', '{MODEL_FILE}',\" if (USE_MODEL and MODEL_FILE) else \"\"\n",
        "\n",
        "    # Build the launch wrapper - fixed string escaping\n",
        "    code = f\"\"\"#!/usr/bin/env python3\n",
        "# MY-AI-Gizmo launch wrapper v3.5.2 - COLAB READY\n",
        "import sys, os\n",
        "{cuda_block}\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['MPLCONFIGDIR'] = r'{MPL_CONFIG_DIR}'\n",
        "os.environ['GRADIO_SERVER_NAME'] = '0.0.0.0'\n",
        "os.environ['GRADIO_SHARE'] = '1'\n",
        "\n",
        "# Check which extensions exist\n",
        "import os as _os\n",
        "_ext_base = _os.path.join(_os.path.dirname(_os.path.abspath(__file__)), 'extensions')\n",
        "_want = ['gizmo_toolbar', 'dual_model', 'google_workspace']\n",
        "_have = [e for e in _want if _os.path.isdir(_os.path.join(_ext_base, e))]\n",
        "_ext_str = ','.join(_have) if _have else ''\n",
        "\n",
        "flags = [\n",
        "    '--listen', '--share', '--verbose',\n",
        "    '--api', '--api-port', '5000',\n",
        "    '--loader', 'llama.cpp',\n",
        "    '--gpu-layers', '{GPU_LAYERS}',\n",
        "    '--ctx-size', '{N_CTX}',\n",
        "    '--batch-size', '512',\n",
        "    '--threads', '{threads}',\n",
        "    {cpu_flag}\n",
        "    {model_flag}\n",
        "]\n",
        "\n",
        "# Add extensions if any exist\n",
        "if _ext_str:\n",
        "    flags.extend(['--extensions', _ext_str])\n",
        "\n",
        "# Remove empty strings\n",
        "flags = [f for f in flags if f]\n",
        "\n",
        "# Add to sys.argv\n",
        "for f in flags:\n",
        "    if f not in sys.argv:\n",
        "        sys.argv.append(f)\n",
        "\n",
        "print('[WRAPPER v3.5.2] Mode: {mode_label} | Model: {model_desc}')\n",
        "print('[WRAPPER] Extensions:', _ext_str if _ext_str else 'none')\n",
        "\n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg', force=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import traceback, runpy\n",
        "try:\n",
        "    runpy.run_path('server.py', run_name='__main__')\n",
        "except SystemExit:\n",
        "    pass\n",
        "except Exception as e:\n",
        "    print('\\\\n[ERROR] server.py raised an exception:')\n",
        "    traceback.print_exc()\n",
        "    raise\n",
        "\"\"\"\n",
        "\n",
        "    wrapper_path = WORK_DIR / \"_gizmo_launch.py\"\n",
        "    wrapper_path.write_text(code, encoding=\"utf-8\")\n",
        "    print(f\"[‚úì] Launch wrapper created (mode={mode_label}, cpu_mode={not USE_GPU})\")\n",
        "    return str(wrapper_path)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  SERVER LAUNCH\n",
        "# =============================================================================\n",
        "\n",
        "def launch(python_exe, wrapper_path):\n",
        "    cmd = [python_exe, \"-u\", wrapper_path]\n",
        "    env = os.environ.copy()\n",
        "    env.update({\n",
        "        \"MPLBACKEND\": \"Agg\",\n",
        "        \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "        \"GRADIO_SERVER_NAME\": \"0.0.0.0\",\n",
        "        \"GRADIO_SHARE\": \"1\"\n",
        "    })\n",
        "    if not USE_GPU:\n",
        "        env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "    captured = None\n",
        "    for attempt in range(1, MAX_RESTARTS + 1):\n",
        "        print(f\"\\n{'='*70}\\nüöÄ Starting server (attempt {attempt}/{MAX_RESTARTS})\\n{'='*70}\\n\")\n",
        "        if attempt > 1:\n",
        "            time.sleep(5)\n",
        "\n",
        "        log_path = LOG_DIR / f\"server_{int(time.time())}.log\"\n",
        "        logfile = None\n",
        "        try:\n",
        "            logfile = open(log_path, \"a\", encoding=\"utf-8\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        os.chdir(WORK_DIR)\n",
        "        proc = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            env=env,\n",
        "            text=True,\n",
        "            bufsize=1\n",
        "        )\n",
        "\n",
        "        last_out = [time.time()]\n",
        "        stop_hb = threading.Event()\n",
        "\n",
        "        def heartbeat():\n",
        "            while not stop_hb.wait(HEARTBEAT_INTERVAL):\n",
        "                if time.time() - last_out[0] >= HEARTBEAT_INTERVAL:\n",
        "                    print(\"[heartbeat] server still running...\")\n",
        "\n",
        "        hb = threading.Thread(target=heartbeat, daemon=True)\n",
        "        hb.start()\n",
        "\n",
        "        try:\n",
        "            for line in proc.stdout:\n",
        "                last_out[0] = time.time()\n",
        "                print(line, end=\"\", flush=True)\n",
        "                if logfile:\n",
        "                    try:\n",
        "                        logfile.write(line)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                if not captured:\n",
        "                    for pat in URL_PATTERNS:\n",
        "                        m = pat.search(line)\n",
        "                        if m:\n",
        "                            url = m.group(1).rstrip(\").,\\\\'\\\"\")\n",
        "                            if any(k in url.lower() for k in URL_KEYWORDS):\n",
        "                                captured = url\n",
        "                                print(f\"\\n{'='*70}\\nüåê PUBLIC URL: {captured}\\n{'='*70}\\n\", flush=True)\n",
        "                                try:\n",
        "                                    PUBLIC_URL_FILE.write_text(captured)\n",
        "                                except Exception:\n",
        "                                    pass\n",
        "                                break\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n[info] Interrupted by user\")\n",
        "            proc.terminate()\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"[error] Stream error: {e}\")\n",
        "        finally:\n",
        "            stop_hb.set()\n",
        "            hb.join(timeout=1)\n",
        "            if logfile:\n",
        "                try:\n",
        "                    logfile.close()\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        rc = proc.wait()\n",
        "        print(f\"\\n[info] Server exited with code {rc}\")\n",
        "\n",
        "        if rc == 0 and not captured:\n",
        "            print(\"[warn] Server exited cleanly but NO URL captured.\")\n",
        "            print(\"       This usually means an import error or extension crash.\")\n",
        "            print(\"       Check the output above for error messages.\")\n",
        "\n",
        "        if rc in (0, -9):\n",
        "            break\n",
        "\n",
        "        if attempt < MAX_RESTARTS:\n",
        "            print(f\"[warn] Crashed (code {rc}) ‚Äî restarting...\")\n",
        "        else:\n",
        "            print(\"[info] Max restarts reached.\")\n",
        "\n",
        "    return captured\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#  MAIN\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Step 1: GitHub Token\n",
        "    setup_github_token()\n",
        "\n",
        "    # Step 2: Repo update decision\n",
        "    repo_mode = check_repo_update()\n",
        "\n",
        "    # Banner\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"  MY-AI-Gizmo  v3.5.2  Universal Launcher\")\n",
        "    print(f\"  {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"  Repo: {GITHUB_USER}/{GITHUB_REPO}  (private)\")\n",
        "    print(\"  +button | Styles | Google Docs | Slides | Dual Model\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Step 3: Mode\n",
        "    choose_mode()\n",
        "    if USE_GPU:\n",
        "        r = sh(\"nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\")\n",
        "        print(f\"[{'‚úì' if r.returncode==0 else 'warn'}] GPU: \"\n",
        "              f\"{r.stdout.strip() if r.returncode==0 else 'not found'}\")\n",
        "\n",
        "    # Step 4: Drive\n",
        "    drive_ok = mount_drive_if_needed()\n",
        "    setup_drive_root(drive_ok)\n",
        "    for d in (DRIVE_ROOT, LOG_DIR, MPL_CONFIG_DIR,\n",
        "              DRIVE_ROOT / \"models\", DRIVE_ROOT / \"settings\", DRIVE_ROOT / \"characters\"):\n",
        "        try:\n",
        "            d.mkdir(parents=True, exist_ok=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    cleanup_broken_files()\n",
        "    show_model_manager()\n",
        "    choose_model()\n",
        "\n",
        "    # Step 5: Apply repo update & clone if needed\n",
        "    apply_repo_update(repo_mode)\n",
        "    if not WORK_DIR.exists():\n",
        "        if not clone_repo():\n",
        "            raise SystemExit(\"‚ùå Repository clone failed ‚Äî check your token and repo name.\")\n",
        "\n",
        "    # Step 6: Patch repo bugs\n",
        "    print(\"\\nüîß Checking repo for known issues...\")\n",
        "    patch_repo_bugs()\n",
        "\n",
        "    # Step 7: Symlinks\n",
        "    ensure_symlinks_and_files()\n",
        "\n",
        "    # Step 8: Model\n",
        "    print(\"\\nüì• Checking model...\")\n",
        "    print_ram_status()\n",
        "    if not download_model_if_missing():\n",
        "        raise SystemExit(1)\n",
        "\n",
        "    # Step 9: Config files\n",
        "    write_settings_yaml()\n",
        "    write_cmd_flags()\n",
        "    write_debug_character()\n",
        "    write_model_loader_config()\n",
        "\n",
        "    # Step 10: Extensions\n",
        "    print(\"\\nüì¶ Deploying extensions...\")\n",
        "    _deploy_ext_stub(\"gizmo_toolbar\")\n",
        "    _deploy_ext_stub(\"google_workspace\")\n",
        "    deploy_dual_model_extension()\n",
        "\n",
        "    # Step 11: Install Python env\n",
        "    start_sh = WORK_DIR / \"start_linux.sh\"\n",
        "    env_marker = WORK_DIR / \"installer_files\" / \"env\" / \"bin\" / \"python\"\n",
        "    python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    if not start_sh.exists():\n",
        "        raise SystemExit(\"[error] start_linux.sh not found ‚Äî check repo contents.\")\n",
        "\n",
        "    sh(\"chmod +x start_linux.sh\", cwd=str(WORK_DIR))\n",
        "\n",
        "    if not env_marker.exists():\n",
        "        print(\"[info] First run ‚Äî installing Python env (~10 min)...\")\n",
        "        MPL_CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        install_env = os.environ.copy()\n",
        "        if USE_GPU:\n",
        "            install_env.update({\n",
        "                \"MPLBACKEND\": \"Agg\",\n",
        "                \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "                \"GPU_CHOICE\": \"A\",\n",
        "                \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "                \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "                \"CMAKE_ARGS\": \"-DLLAMA_CUBLAS=ON -DLLAMA_CUDA=ON\",\n",
        "                \"FORCE_CMAKE\": \"1\",\n",
        "                \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "                \"FORCE_CUDA\": \"TRUE\",\n",
        "            })\n",
        "        else:\n",
        "            install_env.update({\n",
        "                \"MPLBACKEND\": \"Agg\",\n",
        "                \"MPLCONFIGDIR\": str(MPL_CONFIG_DIR),\n",
        "                \"GPU_CHOICE\": \"N\",\n",
        "                \"LAUNCH_AFTER_INSTALL\": \"FALSE\",\n",
        "                \"INSTALL_EXTENSIONS\": \"FALSE\",\n",
        "                \"CMAKE_ARGS\": \"-DLLAMA_CUDA=OFF -DLLAMA_CUBLAS=OFF\",\n",
        "                \"FORCE_CMAKE\": \"1\",\n",
        "                \"CUDA_VISIBLE_DEVICES\": \"\",\n",
        "                \"CUDACXX\": \"\",\n",
        "                \"SKIP_TORCH_TEST\": \"TRUE\",\n",
        "                \"FORCE_CUDA\": \"FALSE\",\n",
        "            })\n",
        "\n",
        "        installer_log = LOG_DIR / f\"installer_{int(time.time())}.log\"\n",
        "        code = stream_with_heartbeat(\n",
        "            \"bash start_linux.sh\",\n",
        "            cwd=str(WORK_DIR),\n",
        "            env=install_env,\n",
        "            logfile_path=str(installer_log)\n",
        "        )\n",
        "        print(f\"[{'‚úì' if code == 0 else 'warn'}] Installer code {code}\")\n",
        "        python_exe = str(env_marker) if env_marker.exists() else \"python3\"\n",
        "\n",
        "    # Step 12: llama-cpp\n",
        "    pip_exe = str(Path(python_exe).parent / \"pip\") if Path(python_exe).exists() else \"pip\"\n",
        "    llama_ok = False\n",
        "    for pkg in (\"llama-cpp-binaries\", \"llama-cpp-python\"):\n",
        "        r = sh(f'\"{pip_exe}\" show {pkg} 2>/dev/null')\n",
        "        if r.returncode == 0 and pkg.split(\"-\")[0] in r.stdout.lower():\n",
        "            ver = next((l for l in r.stdout.splitlines() if l.startswith(\"Version:\")), \"\")\n",
        "            print(f\"[info] {pkg} already installed ({ver.strip()}) ‚Äî skipping reinstall\")\n",
        "            llama_ok = True\n",
        "            break\n",
        "\n",
        "    if not llama_ok:\n",
        "        print(\"[info] llama-cpp not found ‚Äî installing...\")\n",
        "        if USE_GPU:\n",
        "            install_llama_cpp_gpu(python_exe)\n",
        "        else:\n",
        "            install_llama_cpp_cpu(python_exe)\n",
        "\n",
        "    create_llama_cpp_wrapper(python_exe)\n",
        "    install_google_workspace_deps(python_exe)\n",
        "\n",
        "    # Step 13: Kill stale servers\n",
        "    sh(\"pkill -9 -f 'python.*server.py'\")\n",
        "    sh(\"pkill -9 -f 'python.*gradio'\")\n",
        "    sh(\"pkill -9 -f '_gizmo_launch'\")\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Step 14: Build wrapper + launch\n",
        "    wrapper_path = build_launch_wrapper(python_exe)\n",
        "\n",
        "    mode_label = \"GPU\" if USE_GPU else \"CPU\"\n",
        "    model_desc = MODEL_FILE if USE_MODEL else \"(none ‚Äî load from UI)\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"  LAUNCHING v3.5.2 ‚Äî {mode_label}\")\n",
        "    print(f\"  Model   : {model_desc}\")\n",
        "    print(f\"  Threads : {auto_thread_count()}  |  ctx: {N_CTX}\")\n",
        "    print(f\"  Extensions: ÔºãToolbar | Dual Model | Google Workspace\")\n",
        "    print(f\"  URL will appear below ‚Äî wait ~30s after model loads\")\n",
        "    print(\"=\" * 70)\n",
        "    print_ram_status()\n",
        "    print(\"‚è≥ Starting server...\\n\")\n",
        "\n",
        "    captured = launch(python_exe, wrapper_path)\n",
        "\n",
        "    if not captured:\n",
        "        print(\"\\n[info] No URL captured ‚Äî trying ngrok fallback...\")\n",
        "        captured = try_setup_ngrok(7860)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    if captured:\n",
        "        print(f\"  ‚úÖ READY!  ‚Üí  {captured}\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"  ‚Ä¢ Ôºã button (bottom-left)  ‚Üí styles, connectors, tools\")\n",
        "        print(\"  ‚Ä¢ üîó Google Workspace tab  ‚Üí connect Docs & Slides\")\n",
        "        print(\"  ‚Ä¢ ü§ñ Dual Model tab        ‚Üí load a second model\")\n",
        "        print(\"  ‚Ä¢ API: http://0.0.0.0:5000/v1\")\n",
        "        if not USE_MODEL:\n",
        "            print(\"  ‚Ä¢ ‚ö†Ô∏è  No model loaded ‚Äî go to Model tab in UI to load one\")\n",
        "    else:\n",
        "        print(\"  ‚ùå NO PUBLIC URL CAPTURED\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"  Fixes: pkill -9 -f server.py | delete installer_files/ | check Colab internet\")\n",
        "    print_ram_status()\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "Qi3tWZ53XtPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# GIZMO AUTO MERGE SCRIPT (COLAB)\n",
        "# Accept ALL incoming changes\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"\\n=== Gizmo Auto Merge Tool ===\\n\")\n",
        "\n",
        "# -------- USER INPUT --------\n",
        "\n",
        "repo_url = input(\"Enter GitHub repo URL (example: https://github.com/USER/REPO.git): \")\n",
        "\n",
        "branch = input(\"Target branch (usually main): \")\n",
        "if branch.strip() == \"\":\n",
        "    branch = \"main\"\n",
        "\n",
        "incoming_branch = input(\"Incoming branch to merge FROM: \")\n",
        "\n",
        "token = getpass.getpass(\"Paste GitHub Token (hidden): \")\n",
        "\n",
        "# -------- SETUP --------\n",
        "\n",
        "repo_name = repo_url.split(\"/\")[-1].replace(\".git\",\"\")\n",
        "\n",
        "auth_repo = repo_url.replace(\n",
        "    \"https://\",\n",
        "    f\"https://{token}@\"\n",
        ")\n",
        "\n",
        "print(\"\\nCloning or updating repo...\\n\")\n",
        "\n",
        "if os.path.exists(repo_name):\n",
        "    os.system(f\"rm -rf {repo_name}\")\n",
        "\n",
        "os.system(f\"git clone {auth_repo}\")\n",
        "\n",
        "os.chdir(repo_name)\n",
        "\n",
        "# -------- CONFIG --------\n",
        "\n",
        "os.system(\"git config user.email 'colab@gizmo.ai'\")\n",
        "os.system(\"git config user.name 'Colab Gizmo Bot'\")\n",
        "\n",
        "# -------- BACKUP --------\n",
        "\n",
        "print(\"\\nCreating backup branch...\\n\")\n",
        "\n",
        "os.system(f\"git checkout {branch}\")\n",
        "os.system(\"git checkout -b backup-before-merge\")\n",
        "\n",
        "# -------- MERGE --------\n",
        "\n",
        "print(\"\\nMerging incoming changes...\\n\")\n",
        "\n",
        "os.system(f\"git checkout {branch}\")\n",
        "os.system(\"git fetch origin\")\n",
        "\n",
        "merge_code = os.system(\n",
        "    f\"git merge -X theirs origin/{incoming_branch}\"\n",
        ")\n",
        "\n",
        "# -------- AUTO RESOLVE --------\n",
        "\n",
        "print(\"\\nResolving conflicts automatically...\\n\")\n",
        "\n",
        "os.system(\n",
        "    \"git diff --name-only --diff-filter=U | xargs -r git checkout --theirs --\"\n",
        ")\n",
        "\n",
        "os.system(\"git add -A\")\n",
        "\n",
        "os.system(\n",
        "    \"git commit -m 'Auto-resolve conflicts: accepted incoming changes'\"\n",
        ")\n",
        "\n",
        "# -------- PUSH --------\n",
        "\n",
        "print(\"\\nPushing to GitHub...\\n\")\n",
        "\n",
        "os.system(f\"git push origin {branch}\")\n",
        "\n",
        "# -------- DONE --------\n",
        "\n",
        "print(\"\\nSUCCESS!\")\n",
        "print(\"All incoming changes merged.\")\n",
        "print(\"Conflicts resolved automatically.\")\n",
        "print(\"Backup branch created: backup-before-merge\")"
      ],
      "metadata": {
        "id": "xz1cXI-NPY8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ RECOMMENDED MODELS (COPY EXACTLY)\n",
        "üîπ BEST GENERAL CHAT (START HERE)\n",
        "\n",
        "Llama-2-7B-Chat\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n",
        "\n",
        "üîπ FAST + LIGHT (LOW RAM)\n",
        "\n",
        "TinyLlama-1.1B-Chat\n",
        "\n",
        "Repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\n",
        "File: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
        "\n",
        "üîπ STRONG CHAT (BETTER THAN LLAMA-2)\n",
        "\n",
        "Mistral-7B-Instruct\n",
        "\n",
        "Repo: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
        "File: mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
        "\n",
        "üîπ CODING MODEL\n",
        "\n",
        "Code LLaMA-7B\n",
        "\n",
        "Repo: TheBloke/CodeLlama-7B-GGUF\n",
        "File: codellama-7b.Q4_K_M.gguf\n",
        "\n",
        "üîπ ROLEPLAY / STORY\n",
        "\n",
        "MythoMax-L2-13B (needs more RAM)\n",
        "\n",
        "Repo: TheBloke/MythoMax-L2-13B-GGUF\n",
        "File: mythomax-l2-13b.Q4_K_M.gguf\n",
        "\n",
        "üîπ VERY FAST / TEST MODEL\n",
        "\n",
        "Phi-2 (2.7B)\n",
        "\n",
        "Repo: TheBloke/phi-2-GGUF\n",
        "File: phi-2.Q4_K_M.gguf\n",
        "\n",
        "‚öôÔ∏è WHAT LOADER TO USE (IMPORTANT)\n",
        "\n",
        "For ALL models above:\n",
        "\n",
        "Loader: llama.cpp\n",
        "\n",
        "\n",
        "Repo: TheBloke/Llama-2-7B-Chat-GGUF\n",
        "File: llama-2-7b-chat.Q4_K_M.gguf\n"
      ],
      "metadata": {
        "id": "m8MjwwOyvJUh"
      }
    }
  ]
}