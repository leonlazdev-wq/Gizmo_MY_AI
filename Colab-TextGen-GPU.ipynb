{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch AI Gizmo with Llama 2 7B (Saves to Drive)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\033[1;32;1m\\n --> Mounting Google Drive...\\033[0;37;0m\\n\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup paths\n",
        "DRIVE_PATH = '/content/drive/MyDrive/MY-AI-Gizmo'\n",
        "WORK_DIR = '/content/text-generation-webui'  # Work from /content for speed\n",
        "\n",
        "os.environ.pop('PYTHONPATH', None)\n",
        "os.environ.pop('MPLBACKEND', None)\n",
        "\n",
        "# Check if we need to clone\n",
        "if Path(WORK_DIR).exists():\n",
        "    print(\"\\033[1;33;1m --> Removing old installation...\\033[0;37;0m\\n\")\n",
        "    shutil.rmtree(WORK_DIR)\n",
        "\n",
        "# Clone fresh\n",
        "print(\"\\033[1;32;1m --> Cloning AI Gizmo...\\033[0;37;0m\\n\")\n",
        "%cd /content\n",
        "!git clone https://github.com/gitleon8301/MY-AI-Gizmo-working.git text-generation-webui\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# Detect GPU\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    if has_gpu:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"\\033[1;32;1m ‚úì GPU: {gpu_name}\\033[0;37;0m\")\n",
        "    else:\n",
        "        print(\"\\033[1;33;1m ‚ö† CPU mode\\033[0;37;0m\")\n",
        "except:\n",
        "    has_gpu = False\n",
        "    print(\"\\033[1;33;1m ‚ö† CPU mode\\033[0;37;0m\")\n",
        "\n",
        "# Create Drive directories for persistence\n",
        "os.makedirs(f'{DRIVE_PATH}/models', exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_PATH}/characters', exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_PATH}/presets', exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_PATH}/logs', exist_ok=True)\n",
        "\n",
        "# Link models directory to Drive\n",
        "if Path('models').exists():\n",
        "    shutil.rmtree('models')\n",
        "os.symlink(f'{DRIVE_PATH}/models', 'models')\n",
        "\n",
        "# Link user_data directories to Drive\n",
        "for folder in ['characters', 'presets', 'prompts', 'logs']:\n",
        "    drive_folder = f'{DRIVE_PATH}/{folder}'\n",
        "    local_folder = f'user_data/{folder}'\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "    if Path(local_folder).exists():\n",
        "        shutil.rmtree(local_folder)\n",
        "    os.symlink(drive_folder, local_folder)\n",
        "\n",
        "# Install using the start_linux.sh script\n",
        "print(\"\\033[1;32;1m\\n --> Running installation...\\033[0;37;0m\\n\")\n",
        "os.environ['GPU_CHOICE'] = 'A' if has_gpu else 'C'  # A=NVIDIA, C=CPU\n",
        "os.environ['LAUNCH_AFTER_INSTALL'] = 'FALSE'\n",
        "os.environ['INSTALL_EXTENSIONS'] = 'FALSE'\n",
        "\n",
        "!bash start_linux.sh\n",
        "\n",
        "# Download Llama 2 7B\n",
        "model_name = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "model_repo = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
        "model_path = Path(f\"models/{model_name}\")\n",
        "\n",
        "if not model_path.exists():\n",
        "    print(f\"\\033[1;32;1m\\n --> Downloading Llama 2 7B...\\033[0;37;0m\\n\")\n",
        "    !python download-model.py {model_repo}\n",
        "\n",
        "    # The download-model.py downloads to a folder, we need to find the .gguf file\n",
        "    model_folder = Path(f\"models/TheBloke_Llama-2-7B-Chat-GGUF\")\n",
        "    if model_folder.exists():\n",
        "        gguf_files = list(model_folder.glob(\"*.gguf\"))\n",
        "        if gguf_files:\n",
        "            print(f\"\\033[1;32;1m ‚úì Model downloaded: {gguf_files[0].name}\\033[0;37;0m\\n\")\n",
        "            model_name = f\"TheBloke_Llama-2-7B-Chat-GGUF/{gguf_files[0].name}\"\n",
        "else:\n",
        "    print(f\"\\033[1;32;1m ‚úì Model exists\\033[0;37;0m\\n\")\n",
        "\n",
        "# Storage info\n",
        "print(\"\\n\" + \"\\033[1;36;1m\" + \"=\"*80)\n",
        "print(\"üíæ PERSISTENT STORAGE (Google Drive):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üìÅ Root:      {DRIVE_PATH}\")\n",
        "print(f\"ü§ñ Models:    {DRIVE_PATH}/models\")\n",
        "print(f\"üí¨ Chats:     {DRIVE_PATH}/characters\")\n",
        "print(f\"‚öôÔ∏è  Presets:   {DRIVE_PATH}/presets\")\n",
        "print(\"=\"*80 + \"\\033[0;37;0m\\n\")\n",
        "\n",
        "# Start server\n",
        "print(\"\\n\" + \"\\033[1;35;1m\" + \"=\"*80)\n",
        "print(\"üöÄ STARTING LLAMA 2 7B CHAT\")\n",
        "print(\"=\"*80 + \"\\033[0;37;0m\\n\")\n",
        "\n",
        "# Build command flags\n",
        "flags = [\n",
        "    \"--share\",\n",
        "    \"--listen\",\n",
        "    \"--api\",\n",
        "    f\"--model {model_name}\",\n",
        "]\n",
        "\n",
        "if has_gpu:\n",
        "    flags.append(\"--n-gpu-layers 35\")\n",
        "else:\n",
        "    flags.extend([\"--cpu\", \"--threads 2\"])\n",
        "\n",
        "cmd = f\"bash start_linux.sh {' '.join(flags)}\"\n",
        "\n",
        "# Run server\n",
        "process = subprocess.Popen(\n",
        "    cmd,\n",
        "    shell=True,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    universal_newlines=True,\n",
        "    bufsize=1\n",
        ")\n",
        "\n",
        "# Monitor for URLs\n",
        "local_url = None\n",
        "public_url = None\n",
        "urls_displayed = False\n",
        "\n",
        "for line in iter(process.stdout.readline, ''):\n",
        "    print(line, end='')\n",
        "\n",
        "    if not local_url and ('Running on local URL' in line or 'http://127.0.0.1' in line):\n",
        "        match = re.search(r'(http://[\\d\\.]+:\\d+)', line)\n",
        "        if match:\n",
        "            local_url = match.group(1)\n",
        "\n",
        "    if not public_url and ('Running on public URL' in line or 'gradio.live' in line):\n",
        "        match = re.search(r'(https://[a-z0-9\\-]+\\.gradio\\.live)', line)\n",
        "        if match:\n",
        "            public_url = match.group(1)\n",
        "\n",
        "    if local_url and public_url and not urls_displayed:\n",
        "        print(\"\\n\" + \"\\033[1;32;1m\" + \"=\"*80)\n",
        "        print(\"üéâ LLAMA 2 7B IS READY!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nüìç LOCAL:  {local_url}\")\n",
        "        print(f\"üåê PUBLIC: {public_url}\")\n",
        "        print(f\"\\nüí° Use PUBLIC URL to access from anywhere\")\n",
        "        print(f\"üíæ Everything saves to: {DRIVE_PATH}\")\n",
        "        print(\"=\"*80 + \"\\033[0;37;0m\\n\")\n",
        "        urls_displayed = True\n",
        "\n",
        "process.wait()"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}