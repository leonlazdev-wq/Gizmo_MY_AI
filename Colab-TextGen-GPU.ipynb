{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MY-AI-GIZMO COMPLETE WORKING LAUNCHER\n",
        "# Fixed all dependency issues + Persistent Google Drive storage\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "print(\"üöÄ MY-AI-GIZMO COMPLETE INSTALLER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- STEP 1: Mount Google Drive ---\n",
        "print(\"\\nüìÇ Step 1: Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"   ‚úÖ Mounted\")\n",
        "\n",
        "# --- STEP 2: Setup Directories ---\n",
        "print(\"\\nüìÅ Step 2: Setting up directories...\")\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    d.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"   ‚úÖ Base: {DRIVE_BASE}\")\n",
        "\n",
        "# --- STEP 3: Clone Repository ---\n",
        "if not (REPO_DIR / \"server.py\").exists():\n",
        "    print(\"\\nüì• Step 3: Cloning repository...\")\n",
        "    if REPO_DIR.exists():\n",
        "        shutil.rmtree(REPO_DIR)\n",
        "    os.chdir(DRIVE_BASE)\n",
        "    subprocess.run([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"], check=True)\n",
        "    print(\"   ‚úÖ Cloned\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Step 3: Repository exists\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- STEP 4: Link User Data ---\n",
        "print(\"\\nüîó Step 4: Linking user data to Drive...\")\n",
        "\n",
        "local_user_data = REPO_DIR / \"user_data\"\n",
        "if local_user_data.exists() and not local_user_data.is_symlink():\n",
        "    for item in local_user_data.rglob(\"*\"):\n",
        "        if item.is_file():\n",
        "            rel = item.relative_to(local_user_data)\n",
        "            dest = USER_DATA_DIR / rel\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(item, dest)\n",
        "    shutil.rmtree(local_user_data)\n",
        "\n",
        "if not local_user_data.exists():\n",
        "    local_user_data.symlink_to(USER_DATA_DIR)\n",
        "    print(\"   ‚úÖ Linked\")\n",
        "\n",
        "(USER_DATA_DIR / \"logs\").mkdir(exist_ok=True)\n",
        "\n",
        "# --- STEP 5: Environment Variables ---\n",
        "print(\"\\nüîß Step 5: Environment setup...\")\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
        "os.environ['CMAKE_ARGS'] = '-DLLAMA_CUBLAS=off'  # Force CPU build\n",
        "print(\"   ‚úÖ Environment configured\")\n",
        "\n",
        "# --- STEP 6: Install Core Dependencies ---\n",
        "print(\"\\nüì¶ Step 6: Installing dependencies...\")\n",
        "print(\"   This may take 2-3 minutes on first run...\")\n",
        "\n",
        "# Upgrade pip\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\"],\n",
        "               check=False, stdout=subprocess.DEVNULL)\n",
        "\n",
        "# Install essential packages first\n",
        "print(\"   Installing core packages...\")\n",
        "essential = [\n",
        "    \"wheel\",\n",
        "    \"setuptools\",\n",
        "    \"numpy\",\n",
        "    \"requests\",\n",
        "    \"tqdm\",\n",
        "    \"pyyaml\"\n",
        "]\n",
        "\n",
        "for pkg in essential:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                  check=False, stdout=subprocess.DEVNULL)\n",
        "\n",
        "# Install llama-cpp-python for CPU\n",
        "print(\"   Installing llama-cpp-python (CPU)...\")\n",
        "subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"install\",\n",
        "    \"llama-cpp-python\", \"--no-cache-dir\",\n",
        "    \"--force-reinstall\",\n",
        "    \"-q\"\n",
        "], check=False, env=dict(os.environ, CMAKE_ARGS='-DLLAMA_CUBLAS=off'))\n",
        "\n",
        "# Install llama_cpp_binaries (the missing module)\n",
        "print(\"   Installing llama_cpp_binaries...\")\n",
        "subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "    \"llama-cpp-binaries\"\n",
        "], check=False)\n",
        "\n",
        "# Try requirements files\n",
        "installed_from_req = False\n",
        "for req in [\"requirements_cpu_only.txt\", \"requirements.txt\"]:\n",
        "    if Path(req).exists():\n",
        "        print(f\"   Installing from {req}...\")\n",
        "        result = subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\",\n",
        "            \"-r\", req, \"-q\"\n",
        "        ], capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"   ‚úÖ Installed from {req}\")\n",
        "            installed_from_req = True\n",
        "            break\n",
        "\n",
        "# Install remaining common packages\n",
        "print(\"   Installing additional packages...\")\n",
        "additional = [\n",
        "    \"torch\",\n",
        "    \"transformers\",\n",
        "    \"gradio>=3.50.0\",\n",
        "    \"accelerate\",\n",
        "    \"markdown\",\n",
        "    \"Pillow\",\n",
        "    \"safetensors\",\n",
        "    \"sentencepiece\",\n",
        "    \"protobuf\"\n",
        "]\n",
        "\n",
        "for pkg in additional:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                  check=False, stdout=subprocess.DEVNULL)\n",
        "\n",
        "print(\"   ‚úÖ All dependencies installed\")\n",
        "\n",
        "# --- STEP 7: Fix UI Theme ---\n",
        "print(\"\\nüîß Step 7: Patching UI...\")\n",
        "\n",
        "ui_file = Path(\"modules/ui.py\")\n",
        "if ui_file.exists():\n",
        "    with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    if 'button_shadow_hover' in content and 'if False:' not in content:\n",
        "        content = content.replace(\n",
        "            'if not shared.args.old_colors:',\n",
        "            'if False:  # Disabled for Colab'\n",
        "        )\n",
        "        with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "        print(\"   ‚úÖ UI patched\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ UI compatible\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  UI file not found\")\n",
        "\n",
        "# --- STEP 8: Download Model ---\n",
        "print(\"\\n‚¨áÔ∏è  Step 8: Checking model...\")\n",
        "\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "if MODEL_FILE.exists():\n",
        "    size_gb = MODEL_FILE.stat().st_size / (1024**3)\n",
        "    print(f\"   ‚úÖ Model exists ({size_gb:.2f} GB)\")\n",
        "else:\n",
        "    print(\"   üì• Downloading model (3.8 GB - ONE TIME ONLY)...\")\n",
        "    print(\"   This will take 3-5 minutes...\")\n",
        "    url = \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "    result = subprocess.run([\n",
        "        \"wget\", \"-q\", \"--show-progress\",\n",
        "        \"-O\", str(MODEL_FILE), url\n",
        "    ], check=True)\n",
        "\n",
        "    if MODEL_FILE.exists():\n",
        "        print(f\"   ‚úÖ Downloaded ({MODEL_FILE.stat().st_size / (1024**3):.2f} GB)\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Download failed!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# --- STEP 9: Link Models Directory ---\n",
        "print(\"\\nüîó Step 9: Linking models...\")\n",
        "\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "    print(\"   ‚úÖ Models linked\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Already linked\")\n",
        "\n",
        "# --- STEP 10: Verify Installation ---\n",
        "print(\"\\nüîç Step 10: Verifying installation...\")\n",
        "\n",
        "checks = {\n",
        "    \"server.py\": REPO_DIR / \"server.py\",\n",
        "    \"Model file\": MODEL_FILE,\n",
        "    \"User data\": USER_DATA_DIR,\n",
        "    \"Modules dir\": REPO_DIR / \"modules\"\n",
        "}\n",
        "\n",
        "all_good = True\n",
        "for name, path in checks.items():\n",
        "    if path.exists():\n",
        "        print(f\"   ‚úÖ {name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {name} missing!\")\n",
        "        all_good = False\n",
        "\n",
        "if not all_good:\n",
        "    print(\"\\n‚ùå Installation incomplete!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- STEP 11: Interactive Configuration ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚öôÔ∏è  CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hardware\n",
        "print(\"\\nüñ•Ô∏è  Hardware Mode:\")\n",
        "print(\"1. CPU only (Recommended for Colab)\")\n",
        "print(\"2. GPU (Requires GPU runtime)\")\n",
        "print(\"3. Auto-detect\")\n",
        "\n",
        "while True:\n",
        "    hw = input(\"\\nChoice (1-3): \").strip()\n",
        "    if hw in ['1', '2', '3']:\n",
        "        break\n",
        "    print(\"‚ùå Enter 1, 2, or 3\")\n",
        "\n",
        "if hw == '1':\n",
        "    use_cpu = True\n",
        "    hw_name = \"CPU\"\n",
        "elif hw == '2':\n",
        "    use_cpu = False\n",
        "    hw_name = \"GPU\"\n",
        "else:\n",
        "    try:\n",
        "        has_gpu = subprocess.run(['nvidia-smi'], capture_output=True).returncode == 0\n",
        "        use_cpu = not has_gpu\n",
        "        hw_name = \"GPU (detected)\" if has_gpu else \"CPU (no GPU)\"\n",
        "    except:\n",
        "        use_cpu = True\n",
        "        hw_name = \"CPU (fallback)\"\n",
        "\n",
        "print(f\"‚úÖ Using: {hw_name}\")\n",
        "\n",
        "# Options\n",
        "print(\"\\nüîß Additional Options:\")\n",
        "print(\"1. Enable API\")\n",
        "print(\"2. Verbose logging\")\n",
        "print(\"3. Both API + Verbose\")\n",
        "print(\"4. None (minimal)\")\n",
        "\n",
        "while True:\n",
        "    opts = input(\"\\nChoice (1-4): \").strip()\n",
        "    if opts in ['1', '2', '3', '4']:\n",
        "        break\n",
        "    print(\"‚ùå Enter 1, 2, 3, or 4\")\n",
        "\n",
        "enable_api = opts in ['1', '3']\n",
        "enable_verbose = opts in ['2', '3']\n",
        "\n",
        "# --- STEP 12: Build Launch Command ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ LAUNCHING MY-AI-GIZMO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cmd = [sys.executable, \"server.py\"]\n",
        "\n",
        "# Hardware flags\n",
        "if use_cpu:\n",
        "    cmd.extend([\"--cpu\", \"--threads\", \"4\"])\n",
        "\n",
        "# Core flags\n",
        "cmd.extend([\n",
        "    \"--listen\",\n",
        "    \"--share\",\n",
        "    \"--model\", str(MODEL_FILE),\n",
        "    \"--loader\", \"llama.cpp\"  # Force llama.cpp loader\n",
        "])\n",
        "\n",
        "# Optional flags\n",
        "if enable_api:\n",
        "    cmd.extend([\"--api\", \"--public-api\"])\n",
        "if enable_verbose:\n",
        "    cmd.append(\"--verbose\")\n",
        "\n",
        "# Display config\n",
        "print(f\"\\nüìù Launch Configuration:\")\n",
        "print(f\"   Hardware: {hw_name}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "print(f\"   Loader: llama.cpp\")\n",
        "print(f\"   API: {'Enabled' if enable_api else 'Disabled'}\")\n",
        "print(f\"   Verbose: {'Yes' if enable_verbose else 'No'}\")\n",
        "print(f\"   Chats save to: {USER_DATA_DIR / 'logs'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚è≥ Starting server...\")\n",
        "print(\"üîó WATCH FOR GRADIO LINK BELOW ‚¨áÔ∏è\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# --- STEP 13: Launch Server ---\n",
        "try:\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    found_link = False\n",
        "    error_lines = []\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "        # Capture errors\n",
        "        if 'error' in line.lower() or 'traceback' in line.lower():\n",
        "            error_lines.append(line)\n",
        "\n",
        "        # Highlight Gradio link\n",
        "        if ('gradio.live' in line.lower() or\n",
        "            ('running on' in line.lower() and 'http' in line)):\n",
        "            if not found_link:\n",
        "                print(\"\\n\" + \"üéâ\"*35)\n",
        "                print(\"‚úÖ SERVER RUNNING! CLICK THE LINK ABOVE ‚òùÔ∏è\")\n",
        "                print(\"üéâ\"*35)\n",
        "                print(\"\\nüí° USAGE TIPS:\")\n",
        "                print(\"   ‚Ä¢ Select mode in the web UI (Chat/Default/Notebook)\")\n",
        "                print(\"   ‚Ä¢ Conversations auto-save to Google Drive\")\n",
        "                print(\"   ‚Ä¢ Use 'Session' tab to manage saved chats\")\n",
        "                print(\"   ‚Ä¢ Press Ctrl+C here to stop server\")\n",
        "                print(\"=\"*70 + \"\\n\")\n",
        "                found_link = True\n",
        "\n",
        "    return_code = process.wait()\n",
        "\n",
        "    if return_code != 0 and not found_link:\n",
        "        print(\"\\n‚ùå Server exited with error\")\n",
        "        if error_lines:\n",
        "            print(\"\\nüîç Error details:\")\n",
        "            for line in error_lines[-10:]:\n",
        "                print(line, end='')\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n‚èπÔ∏è  Server stopped by user\")\n",
        "    print(f\"üíæ All chats saved in: {USER_DATA_DIR / 'logs'}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\n‚ùå ERROR: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(f\"   1. Check if model exists: {MODEL_FILE.exists()}\")\n",
        "    print(f\"   2. Check working directory: {os.getcwd()}\")\n",
        "    print(f\"   3. Try running manually:\")\n",
        "    print(f\"      cd {REPO_DIR}\")\n",
        "    print(f\"      python server.py --cpu --model {MODEL_FILE}\")\n",
        "\n",
        "    # Show command that failed\n",
        "    print(f\"\\nüìã Command that was run:\")\n",
        "    print(f\"   {' '.join(str(x) for x in cmd)}\")\n",
        "\n",
        "    # Try to get more error details\n",
        "    print(\"\\nüîç Attempting to run with error capture...\")\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if result.stderr:\n",
        "        print(\"STDERR:\")\n",
        "        print(result.stderr[:1000])"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}