{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MY-AI-GIZMO WORKING INSTALLER - ALL FIXES APPLIED\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸš€ MY-AI-GIZMO WORKING INSTALLER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# HELPERS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def run_cmd(cmd, check=False, quiet=True, timeout=None):\n",
        "    try:\n",
        "        if quiet:\n",
        "            return subprocess.run(cmd, check=check, capture_output=True, text=True, timeout=timeout)\n",
        "        else:\n",
        "            return subprocess.run(cmd, check=check, timeout=timeout)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def safe_pip(packages, show_output=False):\n",
        "    if isinstance(packages, str):\n",
        "        packages = [packages]\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            if show_output:\n",
        "                print(f\"      Installing {pkg}...\")\n",
        "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                             check=False, timeout=300)\n",
        "            else:\n",
        "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                             check=False, timeout=300, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def extract_url(text):\n",
        "    match = re.search(r'https://[a-z0-9]+\\.gradio\\.live', text, re.IGNORECASE)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "def print_url_box(url, hw):\n",
        "    box = \"=\"*70\n",
        "    print(\"\\n\" + box)\n",
        "    print(\"ğŸ‰\"*35)\n",
        "    print(box)\n",
        "    print(\"âœ… SERVER IS RUNNING!\")\n",
        "    print(box)\n",
        "    print(f\"\\nğŸŒ PUBLIC URL (CLICK TO OPEN):\\n\")\n",
        "    print(f\"   {url}\\n\")\n",
        "    print(box)\n",
        "    print(f\"ğŸ’¾ All chats save to Google Drive\")\n",
        "    print(f\"ğŸ–¥ï¸  Hardware: {hw}\")\n",
        "    print(f\"â¹ï¸  Press Ctrl+C to stop\")\n",
        "    print(box + \"\\n\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PATHS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 1: MOUNT DRIVE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“‚ Step 1: Google Drive...\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    mydrive = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "    if mydrive.exists() and mydrive.is_dir():\n",
        "        print(\"   âœ… Mounted\")\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"   âœ… Mounted\")\n",
        "\n",
        "    if not mydrive.exists():\n",
        "        raise Exception(\"Mount failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   âŒ Error: {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 2-3: SYSTEM + REPO\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“¦ Step 2: System...\")\n",
        "run_cmd([\"apt-get\", \"update\", \"-qq\"])\n",
        "run_cmd([\"apt-get\", \"install\", \"-y\", \"-qq\", \"build-essential\", \"cmake\", \"git\", \"wget\"])\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "print(\"\\nğŸ“¥ Step 3: Repository...\")\n",
        "\n",
        "if (REPO_DIR / \"server.py\").exists():\n",
        "    print(\"   âœ… Exists\")\n",
        "    os.chdir(REPO_DIR)\n",
        "else:\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    os.chdir(REPO_DIR.parent)\n",
        "    run_cmd([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"])\n",
        "    print(\"   âœ… Cloned\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 4: PACKAGES - COMPREHENSIVE INSTALL\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ Step 4: Installing packages...\")\n",
        "\n",
        "# Core packages\n",
        "print(\"   ğŸ“¦ Core packages...\")\n",
        "safe_pip([\"setuptools\", \"wheel\", \"numpy\", \"requests\", \"tqdm\", \"pyyaml\"])\n",
        "\n",
        "# Install llama-cpp-python with server\n",
        "print(\"   ğŸ“¦ llama-cpp-python (this may take a few minutes)...\")\n",
        "env = dict(os.environ)\n",
        "env[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=off -DLLAMA_BUILD_SERVER=ON\"\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"--no-cache-dir\",\n",
        "               \"llama-cpp-python[server]\"],\n",
        "              check=False, env=env, timeout=600, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Main packages\n",
        "print(\"   ğŸ“¦ Main packages...\")\n",
        "safe_pip([\"torch\", \"transformers\", \"gradio>=3.50.0\", \"accelerate\", \"markdown\",\n",
        "         \"Pillow\", \"safetensors\", \"sentencepiece\", \"protobuf\"])\n",
        "\n",
        "# Extension requirements\n",
        "print(\"   ğŸ“¦ Extension dependencies...\")\n",
        "safe_pip([\"flask_cloudflared\", \"pydub\", \"SpeechRecognition\", \"openai-whisper\"])\n",
        "\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 5: FIND LLAMA-SERVER\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ” Step 5: Locating llama-server...\")\n",
        "\n",
        "llama_server_path = None\n",
        "\n",
        "# Check common locations\n",
        "check_paths = [\n",
        "    Path(sys.executable).parent / \"llama-server\",\n",
        "    Path(sys.prefix) / \"bin\" / \"llama-server\",\n",
        "    Path(\"/usr/local/bin/llama-server\"),\n",
        "    Path.home() / \".local\" / \"bin\" / \"llama-server\"\n",
        "]\n",
        "\n",
        "# Also check in site-packages\n",
        "try:\n",
        "    import llama_cpp\n",
        "    llama_cpp_dir = Path(llama_cpp.__file__).parent\n",
        "    check_paths.insert(0, llama_cpp_dir / \"server\" / \"llama-server\")\n",
        "    check_paths.insert(1, llama_cpp_dir / \"llama-server\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "for path in check_paths:\n",
        "    if path.exists():\n",
        "        llama_server_path = str(path)\n",
        "        print(f\"   âœ… Found: {llama_server_path}\")\n",
        "        break\n",
        "\n",
        "if not llama_server_path:\n",
        "    # Try using 'which' command\n",
        "    result = run_cmd(['which', 'llama-server'])\n",
        "    if result and result.returncode == 0 and result.stdout.strip():\n",
        "        llama_server_path = result.stdout.strip()\n",
        "        print(f\"   âœ… Found: {llama_server_path}\")\n",
        "    else:\n",
        "        print(\"   âš ï¸  llama-server not found, will use default\")\n",
        "        llama_server_path = \"llama-server\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 6: FIX UI\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”§ Step 6: Fixing UI...\")\n",
        "\n",
        "ui_file = REPO_DIR / \"modules\" / \"ui.py\"\n",
        "\n",
        "if ui_file.exists():\n",
        "    with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    if '# COLAB_FIX_APPLIED' in content:\n",
        "        print(\"   âœ… Already fixed\")\n",
        "    else:\n",
        "        lines = content.split('\\n')\n",
        "        fixed_lines = []\n",
        "        in_theme_block = False\n",
        "        theme_block_indent = 0\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'if not shared.args.old_colors:' in line and not in_theme_block:\n",
        "                fixed_lines.append('    # COLAB_FIX_APPLIED - Theme disabled for compatibility')\n",
        "                fixed_lines.append('    # ' + line.strip())\n",
        "                in_theme_block = True\n",
        "                theme_block_indent = len(line) - len(line.lstrip())\n",
        "                continue\n",
        "\n",
        "            if in_theme_block:\n",
        "                current_indent = len(line) - len(line.lstrip()) if line.strip() else 999\n",
        "\n",
        "                if line.strip() and current_indent <= theme_block_indent:\n",
        "                    in_theme_block = False\n",
        "                    fixed_lines.append(line)\n",
        "                else:\n",
        "                    if line.strip():\n",
        "                        fixed_lines.append('    # ' + line.strip())\n",
        "                    else:\n",
        "                        fixed_lines.append(line)\n",
        "                continue\n",
        "\n",
        "            fixed_lines.append(line)\n",
        "\n",
        "        with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(fixed_lines))\n",
        "\n",
        "        print(\"   âœ… Fixed\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 7: FIX LLAMA SERVER MODULE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”§ Step 7: Updating llama server module...\")\n",
        "\n",
        "LLAMA_CODE = f'''\"\"\"llama.cpp server with fixed path\"\"\"\n",
        "import json, torch, os, socket, subprocess, sys, threading, time, requests\n",
        "from pathlib import Path\n",
        "from modules import shared\n",
        "from modules.logging_colors import logger\n",
        "\n",
        "def get_llama_server_path():\n",
        "    # Use the path we found during installation\n",
        "    found_path = \"{llama_server_path}\"\n",
        "    if os.path.exists(found_path):\n",
        "        return found_path\n",
        "\n",
        "    # Fallback: search common locations\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        for p in [Path(llama_cpp.__file__).parent/\"server\"/\"llama-server\",\n",
        "                  Path(llama_cpp.__file__).parent/\"llama-server\",\n",
        "                  Path(sys.prefix)/\"bin\"/\"llama-server\"]:\n",
        "            if p.exists(): return str(p)\n",
        "        r = subprocess.run(['which','llama-server'], capture_output=True, text=True, timeout=5)\n",
        "        if r.returncode == 0 and r.stdout.strip():\n",
        "            return r.stdout.strip()\n",
        "    except:\n",
        "        pass\n",
        "    return \"llama-server\"\n",
        "\n",
        "class LlamaServer:\n",
        "    def __init__(self, model_path, server_path=None):\n",
        "        self.model_path = model_path\n",
        "        self.server_path = server_path or get_llama_server_path()\n",
        "        logger.info(f\"Using llama-server: {{self.server_path}}\")\n",
        "        self.port = self._find_port()\n",
        "        self.process = None\n",
        "        self.session = requests.Session()\n",
        "        self.vocabulary_size = None\n",
        "        self.bos_token = \"~~\"\n",
        "        self.last_prompt_token_count = 0\n",
        "        self._start_server()\n",
        "\n",
        "    def encode(self, text, add_bos_token=False, **kwargs):\n",
        "        if self.bos_token and text.startswith(self.bos_token): add_bos_token = False\n",
        "        r = self.session.post(f\"http://127.0.0.1:{{self.port}}/tokenize\", json={{\"content\":text,\"add_special\":add_bos_token}})\n",
        "        return r.json().get(\"tokens\",[])\n",
        "\n",
        "    def decode(self, token_ids, **kwargs):\n",
        "        r = self.session.post(f\"http://127.0.0.1:{{self.port}}/detokenize\", json={{\"tokens\":token_ids}})\n",
        "        return r.json().get(\"content\",\"\")\n",
        "\n",
        "    def prepare_payload(self, state):\n",
        "        temp = state[\"temperature\"]\n",
        "        if state.get(\"dynamic_temperature\"): temp = (state[\"dynatemp_low\"]+state[\"dynatemp_high\"])/2\n",
        "        return {{\"temperature\":temp,\"top_k\":state.get(\"top_k\",40),\"top_p\":state.get(\"top_p\",0.95),\n",
        "                \"min_p\":state.get(\"min_p\",0.05),\"repeat_penalty\":state.get(\"repetition_penalty\",1.1),\"seed\":state.get(\"seed\",-1)}}\n",
        "\n",
        "    def is_multimodal(self): return False\n",
        "\n",
        "    def generate_with_streaming(self, prompt, state):\n",
        "        payload = self.prepare_payload(state)\n",
        "        tokens = self.encode(prompt, add_bos_token=state.get(\"add_bos_token\",False))\n",
        "        self.last_prompt_token_count = len(tokens)\n",
        "        payload[\"prompt\"] = tokens\n",
        "        payload.update({{\"n_predict\":state.get('max_new_tokens',200),\"stream\":True,\"cache_prompt\":True}})\n",
        "        r = self.session.post(f\"http://127.0.0.1:{{self.port}}/completion\", json=payload, stream=True)\n",
        "        full = \"\"\n",
        "        try:\n",
        "            for line in r.iter_lines():\n",
        "                if not line: continue\n",
        "                try:\n",
        "                    line = line.decode('utf-8')\n",
        "                    if line.startswith('data: '): line = line[6:]\n",
        "                    data = json.loads(line)\n",
        "                    if data.get('content'): full += data['content']; yield full\n",
        "                    if data.get('stop'): break\n",
        "                except: continue\n",
        "        finally: r.close()\n",
        "\n",
        "    def generate(self, prompt, state):\n",
        "        out = \"\"\n",
        "        for out in self.generate_with_streaming(prompt, state): pass\n",
        "        return out\n",
        "\n",
        "    def _get_vocabulary_size(self):\n",
        "        try:\n",
        "            r = self.session.get(f\"http://127.0.0.1:{{self.port}}/v1/models\").json()\n",
        "            if r.get(\"data\") and r[\"data\"]:\n",
        "                meta = r[\"data\"][0].get(\"meta\",{{}})\n",
        "                if \"n_vocab\" in meta: self.vocabulary_size = meta[\"n_vocab\"]\n",
        "        except: pass\n",
        "\n",
        "    def _get_bos_token(self):\n",
        "        try:\n",
        "            r = self.session.get(f\"http://127.0.0.1:{{self.port}}/props\").json()\n",
        "            if \"bos_token\" in r: self.bos_token = r[\"bos_token\"]\n",
        "        except: pass\n",
        "\n",
        "    def _find_port(self):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.bind(('',0))\n",
        "            return s.getsockname()[1]\n",
        "\n",
        "    def _start_server(self):\n",
        "        has_gpu = torch.cuda.is_available()\n",
        "        gpu_layers = getattr(shared.args,'gpu_layers',0) if has_gpu else 0\n",
        "        if has_gpu: logger.info(f\"GPU: {{torch.cuda.get_device_name(0)}}\")\n",
        "        else: logger.info(\"CPU mode\")\n",
        "        cmd = [self.server_path,\"--model\",self.model_path,\"--ctx-size\",str(getattr(shared.args,'ctx_size',2048)),\n",
        "               \"--batch-size\",str(getattr(shared.args,'batch_size',512)),\"--port\",str(self.port),\"--no-webui\"]\n",
        "        if has_gpu and gpu_layers > 0: cmd += [\"--gpu-layers\",str(gpu_layers)]\n",
        "        threads = getattr(shared.args,'threads',0)\n",
        "        if threads > 0: cmd += [\"--threads\",str(threads)]\n",
        "        logger.info(f\"Starting llama.cpp on port {{self.port}}\")\n",
        "\n",
        "        try:\n",
        "            self.process = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE, bufsize=0)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"llama-server not found at: {{self.server_path}}\")\n",
        "            raise RuntimeError(f\"llama-server executable not found. Tried: {{self.server_path}}\")\n",
        "\n",
        "        threading.Thread(target=self._log_stderr, daemon=True).start()\n",
        "        health = f\"http://127.0.0.1:{{self.port}}/health\"\n",
        "        for _ in range(60):\n",
        "            if self.process.poll() is not None:\n",
        "                raise RuntimeError(f\"Server died with code: {{self.process.returncode}}\")\n",
        "            try:\n",
        "                if self.session.get(health, timeout=1).status_code == 200: break\n",
        "            except: pass\n",
        "            time.sleep(1)\n",
        "        self._get_vocabulary_size()\n",
        "        self._get_bos_token()\n",
        "        logger.info(\"Ready\")\n",
        "\n",
        "    def _log_stderr(self):\n",
        "        try:\n",
        "            for line in iter(self.process.stderr.readline, b''):\n",
        "                print(line.decode('utf-8',errors='replace').strip(), file=sys.stderr)\n",
        "        except: pass\n",
        "\n",
        "    def stop(self):\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            try: self.process.wait(timeout=5)\n",
        "            except: self.process.kill()\n",
        "\n",
        "    def __del__(self): self.stop()\n",
        "'''\n",
        "\n",
        "(REPO_DIR / \"modules\" / \"llama_cpp_server.py\").write_text(LLAMA_CODE, encoding='utf-8')\n",
        "print(\"   âœ… Fixed\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEPS 8-10: USER DATA, MODEL, LINKS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”— Step 8: User data...\")\n",
        "local_data = REPO_DIR / \"user_data\"\n",
        "if local_data.exists() and not local_data.is_symlink():\n",
        "    for item in local_data.rglob(\"*\"):\n",
        "        if item.is_file():\n",
        "            rel = item.relative_to(local_data)\n",
        "            dest = USER_DATA_DIR / rel\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(item, dest)\n",
        "    shutil.rmtree(local_data)\n",
        "if not local_data.exists():\n",
        "    local_data.symlink_to(USER_DATA_DIR)\n",
        "for sub in [\"logs\",\"logs/chat\",\"logs/instruct\",\"presets\",\"characters\"]:\n",
        "    (USER_DATA_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "print(\"\\nâ¬‡ï¸  Step 9: Model...\")\n",
        "if MODEL_FILE.exists():\n",
        "    print(f\"   âœ… Exists ({MODEL_FILE.stat().st_size/(1024**3):.2f} GB)\")\n",
        "else:\n",
        "    print(\"   ğŸ“¥ Downloading (this will take a few minutes)...\")\n",
        "    MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "    run_cmd([\"wget\",\"-q\",\"--show-progress\",\"-O\",str(MODEL_FILE),\n",
        "            \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"],\n",
        "            quiet=False, timeout=900)\n",
        "    print(f\"   âœ… Downloaded\")\n",
        "\n",
        "print(\"\\nğŸ”— Step 10: Models link...\")\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 11: HARDWARE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ–¥ï¸  Step 11: Hardware check...\")\n",
        "has_gpu = False\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    print(f\"   {'âœ… GPU: '+torch.cuda.get_device_name(0) if has_gpu else 'â„¹ï¸  CPU'}\")\n",
        "except:\n",
        "    print(\"   â„¹ï¸  CPU\")\n",
        "\n",
        "hw = \"GPU\" if has_gpu else \"CPU\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 12: LAUNCH\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nâš™ï¸  Step 12: Launching server...\")\n",
        "\n",
        "cmd = [sys.executable, \"server.py\"]\n",
        "if has_gpu:\n",
        "    cmd.extend([\"--gpu-layers\", \"35\"])\n",
        "else:\n",
        "    cmd.extend([\"--cpu\"])\n",
        "cmd.extend([\"--threads\",\"4\",\"--listen\",\"--listen-host\",\"0.0.0.0\",\"--share\",\n",
        "            \"--model\",str(MODEL_FILE),\"--loader\",\"llama.cpp\"])\n",
        "\n",
        "print(f\"   Hardware: {hw}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "print(f\"   Llama-server: {llama_server_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ STARTING SERVER\")\n",
        "print(\"=\"*70)\n",
        "print(\"â³ Please wait 30-90 seconds for startup...\")\n",
        "print(\"ğŸ” Watching for Gradio URL...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "found_url = [None]\n",
        "url_printed = [False]\n",
        "\n",
        "def url_reminder():\n",
        "    \"\"\"Print reminder every 2 minutes\"\"\"\n",
        "    while True:\n",
        "        time.sleep(120)\n",
        "        if not url_printed[0]:\n",
        "            print(\"\\n\" + \"â³\"*35)\n",
        "            print(\"Still starting up... Gradio URL will appear above when ready\")\n",
        "            print(\"â³\"*35 + \"\\n\")\n",
        "\n",
        "reminder_thread = threading.Thread(target=url_reminder, daemon=True)\n",
        "reminder_thread.start()\n",
        "\n",
        "try:\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                              universal_newlines=True, bufsize=1)\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "        if not url_printed[0]:\n",
        "            url = extract_url(line)\n",
        "            if url:\n",
        "                found_url[0] = url\n",
        "                url_printed[0] = True\n",
        "                print_url_box(url, hw)\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"â¹ï¸  Server stopped by user\")\n",
        "    if found_url[0]:\n",
        "        print(f\"ğŸ“ URL was: {found_url[0]}\")\n",
        "    print(f\"ğŸ’¾ Data saved to: {USER_DATA_DIR}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nâœ… Done\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}