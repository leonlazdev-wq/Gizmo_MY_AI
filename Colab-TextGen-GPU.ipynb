{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MY-AI-GIZMO SUPER ROBUST INSTALLER - CONTINUES DESPITE ERRORS\n",
        "# Fixed all dependency issues + Error-tolerant execution\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "print(\"üöÄ MY-AI-GIZMO SUPER ROBUST INSTALLER\")\n",
        "print(\"=\"*70)\n",
        "print(\"This version continues even if some things fail!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- STEP 1: Mount Google Drive ---\n",
        "print(\"\\nüìÇ Step 1: Mounting Google Drive...\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    print(\"   ‚úÖ Mounted\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: {e}\")\n",
        "    print(\"   Continuing anyway...\")\n",
        "\n",
        "# --- STEP 2: Setup Directories ---\n",
        "print(\"\\nüìÅ Step 2: Setting up directories...\")\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    try:\n",
        "        d.mkdir(exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Could not create {d}: {e}\")\n",
        "\n",
        "print(f\"   ‚úÖ Base: {DRIVE_BASE}\")\n",
        "\n",
        "# --- STEP 3: Clone Repository ---\n",
        "if not (REPO_DIR / \"server.py\").exists():\n",
        "    print(\"\\nüì• Step 3: Cloning repository...\")\n",
        "    try:\n",
        "        if REPO_DIR.exists():\n",
        "            shutil.rmtree(REPO_DIR)\n",
        "        os.chdir(DRIVE_BASE)\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"], check=True)\n",
        "        print(\"   ‚úÖ Cloned\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Clone failed: {e}\")\n",
        "        print(\"   Trying to continue with existing files...\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Step 3: Repository exists\")\n",
        "\n",
        "try:\n",
        "    os.chdir(REPO_DIR)\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Could not change to repo dir: {e}\")\n",
        "\n",
        "# --- STEP 4: Link User Data ---\n",
        "print(\"\\nüîó Step 4: Linking user data to Drive...\")\n",
        "\n",
        "try:\n",
        "    local_user_data = REPO_DIR / \"user_data\"\n",
        "    if local_user_data.exists() and not local_user_data.is_symlink():\n",
        "        for item in local_user_data.rglob(\"*\"):\n",
        "            if item.is_file():\n",
        "                rel = item.relative_to(local_user_data)\n",
        "                dest = USER_DATA_DIR / rel\n",
        "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(item, dest)\n",
        "        shutil.rmtree(local_user_data)\n",
        "\n",
        "    if not local_user_data.exists():\n",
        "        local_user_data.symlink_to(USER_DATA_DIR)\n",
        "        print(\"   ‚úÖ Linked\")\n",
        "\n",
        "    (USER_DATA_DIR / \"logs\").mkdir(exist_ok=True)\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Linking warning: {e}\")\n",
        "    print(\"   Continuing anyway...\")\n",
        "\n",
        "# --- STEP 5: Environment Variables ---\n",
        "print(\"\\nüîß Step 5: Environment setup...\")\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
        "os.environ['CMAKE_ARGS'] = '-DLLAMA_CUBLAS=off'\n",
        "print(\"   ‚úÖ Environment configured\")\n",
        "\n",
        "# --- STEP 6: Install Core Dependencies ---\n",
        "print(\"\\nüì¶ Step 6: Installing dependencies (ROBUST)...\")\n",
        "print(\"   This may take 2-3 minutes on first run...\")\n",
        "\n",
        "def safe_install(packages, description=\"packages\"):\n",
        "    \"\"\"Install packages but don't fail if some don't work\"\"\"\n",
        "    print(f\"   Installing {description}...\")\n",
        "    for pkg in (packages if isinstance(packages, list) else [packages]):\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                          check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
        "                          timeout=120)\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  {pkg} failed: {e}\")\n",
        "    return True\n",
        "\n",
        "# Upgrade pip\n",
        "safe_install(\"pip --upgrade\", \"pip upgrade\")\n",
        "\n",
        "# Essential packages\n",
        "essential = [\"wheel\", \"setuptools\", \"numpy\", \"requests\", \"tqdm\", \"pyyaml\"]\n",
        "safe_install(essential, \"core packages\")\n",
        "\n",
        "# Install llama-cpp-python\n",
        "print(\"   Installing llama-cpp-python (CPU)...\")\n",
        "try:\n",
        "    subprocess.run([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\",\n",
        "        \"llama-cpp-python\", \"--no-cache-dir\",\n",
        "        \"--force-reinstall\", \"-q\"\n",
        "    ], check=False, env=dict(os.environ, CMAKE_ARGS='-DLLAMA_CUBLAS=off'),\n",
        "       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, timeout=300)\n",
        "    print(\"   ‚úÖ llama-cpp-python installed\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  llama-cpp-python warning: {e}\")\n",
        "\n",
        "# Additional packages\n",
        "additional = [\n",
        "    \"torch\", \"transformers\", \"gradio>=3.50.0\", \"accelerate\",\n",
        "    \"markdown\", \"Pillow\", \"safetensors\", \"sentencepiece\", \"protobuf\",\n",
        "    \"flask\", \"flask-cloudflared\"  # Add flask-cloudflared for openai extension\n",
        "]\n",
        "safe_install(additional, \"additional packages\")\n",
        "\n",
        "print(\"   ‚úÖ All dependencies installed (some may have warnings)\")\n",
        "\n",
        "# --- STEP 7: ADVANCED FIX for llama_cpp_server.py ---\n",
        "print(\"\\nüîß Step 7: Advanced patching of llama_cpp_server.py...\")\n",
        "\n",
        "llama_server_file = REPO_DIR / \"modules\" / \"llama_cpp_server.py\"\n",
        "if llama_server_file.exists():\n",
        "    try:\n",
        "        with open(llama_server_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        original_content = content\n",
        "\n",
        "        # Fix 1: Comment out the import\n",
        "        content = content.replace(\n",
        "            'import llama_cpp_binaries',\n",
        "            '# import llama_cpp_binaries  # Removed - not needed'\n",
        "        )\n",
        "\n",
        "        # Fix 2: Remove or replace any usage of llama_cpp_binaries\n",
        "        # Common patterns where it might be used:\n",
        "        content = content.replace(\n",
        "            'llama_cpp_binaries.',\n",
        "            '# llama_cpp_binaries.  # Removed - '\n",
        "        )\n",
        "\n",
        "        # Fix 3: If there's a conditional check for llama_cpp_binaries\n",
        "        content = content.replace(\n",
        "            'if llama_cpp_binaries',\n",
        "            'if False  # llama_cpp_binaries removed'\n",
        "        )\n",
        "\n",
        "        content = content.replace(\n",
        "            'if hasattr(llama_cpp_binaries',\n",
        "            'if False and hasattr(llama_cpp_binaries'\n",
        "        )\n",
        "\n",
        "        # Save the patched file\n",
        "        if content != original_content:\n",
        "            with open(llama_server_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "            print(\"   ‚úÖ Patched llama_cpp_server.py completely\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ llama_cpp_server.py already compatible\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Patching warning: {e}\")\n",
        "        print(\"   Continuing anyway...\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  llama_cpp_server.py not found - may be OK\")\n",
        "\n",
        "# --- STEP 8: Disable problematic extensions ---\n",
        "print(\"\\nüîß Step 8: Configuring extensions...\")\n",
        "\n",
        "try:\n",
        "    extensions_dir = REPO_DIR / \"extensions\"\n",
        "    if extensions_dir.exists():\n",
        "        # Disable openai extension if it causes issues\n",
        "        openai_script = extensions_dir / \"openai\" / \"script.py\"\n",
        "        if openai_script.exists():\n",
        "            with open(openai_script, 'r', encoding='utf-8') as f:\n",
        "                ext_content = f.read()\n",
        "\n",
        "            # Make cloudflared optional\n",
        "            ext_content = ext_content.replace(\n",
        "                'from flask_cloudflared import _run_cloudflared',\n",
        "                'try:\\n    from flask_cloudflared import _run_cloudflared\\nexcept:\\n    _run_cloudflared = None  # Optional'\n",
        "            )\n",
        "\n",
        "            ext_content = ext_content.replace(\n",
        "                'raise Exception(',\n",
        "                'print(\"Warning: flask_cloudflared not available\"); return  # '\n",
        "            )\n",
        "\n",
        "            with open(openai_script, 'w', encoding='utf-8') as f:\n",
        "                f.write(ext_content)\n",
        "            print(\"   ‚úÖ Made extensions error-tolerant\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ No extensions to configure\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Extension config warning: {e}\")\n",
        "\n",
        "# --- STEP 9: Fix UI Theme ---\n",
        "print(\"\\nüîß Step 9: Patching UI...\")\n",
        "\n",
        "try:\n",
        "    ui_file = REPO_DIR / \"modules\" / \"ui.py\"\n",
        "    if ui_file.exists():\n",
        "        with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        if 'button_shadow_hover' in content and 'if False:' not in content:\n",
        "            content = content.replace(\n",
        "                'if not shared.args.old_colors:',\n",
        "                'if False:  # Disabled for Colab'\n",
        "            )\n",
        "            with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "            print(\"   ‚úÖ UI patched\")\n",
        "        else:\n",
        "            print(\"   ‚úÖ UI compatible\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  UI file not found\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  UI patch warning: {e}\")\n",
        "\n",
        "# --- STEP 10: Download Model ---\n",
        "print(\"\\n‚¨áÔ∏è  Step 10: Checking model...\")\n",
        "\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "try:\n",
        "    if MODEL_FILE.exists():\n",
        "        size_gb = MODEL_FILE.stat().st_size / (1024**3)\n",
        "        print(f\"   ‚úÖ Model exists ({size_gb:.2f} GB)\")\n",
        "    else:\n",
        "        print(\"   üì• Downloading model (3.8 GB - ONE TIME ONLY)...\")\n",
        "        print(\"   This will take 3-5 minutes...\")\n",
        "        url = \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "        subprocess.run([\n",
        "            \"wget\", \"-q\", \"--show-progress\",\n",
        "            \"-O\", str(MODEL_FILE), url\n",
        "        ], check=True, timeout=600)\n",
        "\n",
        "        if MODEL_FILE.exists():\n",
        "            print(f\"   ‚úÖ Downloaded ({MODEL_FILE.stat().st_size / (1024**3):.2f} GB)\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  Download may have failed - check manually\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Model download warning: {e}\")\n",
        "    print(\"   You may need to download manually\")\n",
        "\n",
        "# --- STEP 11: Link Models Directory ---\n",
        "print(\"\\nüîó Step 11: Linking models...\")\n",
        "\n",
        "try:\n",
        "    repo_models = REPO_DIR / \"models\"\n",
        "    if not repo_models.is_symlink():\n",
        "        if repo_models.exists():\n",
        "            shutil.rmtree(repo_models)\n",
        "        repo_models.symlink_to(MODELS_DIR)\n",
        "        print(\"   ‚úÖ Models linked\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ Already linked\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è  Linking warning: {e}\")\n",
        "\n",
        "# --- STEP 12: Verify Installation ---\n",
        "print(\"\\nüîç Step 12: Verifying installation...\")\n",
        "\n",
        "checks = {\n",
        "    \"server.py\": REPO_DIR / \"server.py\",\n",
        "    \"Model file\": MODEL_FILE,\n",
        "    \"User data\": USER_DATA_DIR,\n",
        "    \"Modules dir\": REPO_DIR / \"modules\"\n",
        "}\n",
        "\n",
        "warnings = []\n",
        "for name, path in checks.items():\n",
        "    if path.exists():\n",
        "        print(f\"   ‚úÖ {name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  {name} missing!\")\n",
        "        warnings.append(name)\n",
        "\n",
        "if warnings:\n",
        "    print(f\"\\n‚ö†Ô∏è  Some items missing: {', '.join(warnings)}\")\n",
        "    print(\"   Attempting to continue anyway...\")\n",
        "\n",
        "# --- STEP 13: Interactive Configuration ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚öôÔ∏è  CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hardware\n",
        "print(\"\\nüñ•Ô∏è  Hardware Mode:\")\n",
        "print(\"1. CPU only (Recommended for Colab)\")\n",
        "print(\"2. GPU (Requires GPU runtime)\")\n",
        "print(\"3. Auto-detect\")\n",
        "\n",
        "hw = input(\"\\nChoice (1-3) [default: 1]: \").strip() or \"1\"\n",
        "\n",
        "if hw == '1':\n",
        "    use_cpu = True\n",
        "    hw_name = \"CPU\"\n",
        "elif hw == '2':\n",
        "    use_cpu = False\n",
        "    hw_name = \"GPU\"\n",
        "else:\n",
        "    try:\n",
        "        has_gpu = subprocess.run(['nvidia-smi'], capture_output=True, timeout=5).returncode == 0\n",
        "        use_cpu = not has_gpu\n",
        "        hw_name = \"GPU (detected)\" if has_gpu else \"CPU (no GPU)\"\n",
        "    except:\n",
        "        use_cpu = True\n",
        "        hw_name = \"CPU (fallback)\"\n",
        "\n",
        "print(f\"‚úÖ Using: {hw_name}\")\n",
        "\n",
        "# Options\n",
        "print(\"\\nüîß Additional Options:\")\n",
        "print(\"1. Enable API\")\n",
        "print(\"2. Verbose logging\")\n",
        "print(\"3. Both API + Verbose\")\n",
        "print(\"4. None (minimal)\")\n",
        "\n",
        "opts = input(\"\\nChoice (1-4) [default: 4]: \").strip() or \"4\"\n",
        "\n",
        "enable_api = opts in ['1', '3']\n",
        "enable_verbose = opts in ['2', '3']\n",
        "\n",
        "# --- STEP 14: Build Launch Command ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ LAUNCHING MY-AI-GIZMO\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cmd = [sys.executable, \"server.py\"]\n",
        "\n",
        "# Hardware flags\n",
        "if use_cpu:\n",
        "    cmd.extend([\"--cpu\", \"--threads\", \"4\"])\n",
        "\n",
        "# Core flags\n",
        "cmd.extend([\n",
        "    \"--listen\",\n",
        "    \"--share\",\n",
        "    \"--model\", str(MODEL_FILE),\n",
        "    \"--loader\", \"llama.cpp\"\n",
        "])\n",
        "\n",
        "# Optional flags\n",
        "if enable_api:\n",
        "    cmd.extend([\"--api\", \"--public-api\"])\n",
        "if enable_verbose:\n",
        "    cmd.append(\"--verbose\")\n",
        "\n",
        "# Disable problematic extensions\n",
        "cmd.extend([\"--extensions\", \"\"])  # Don't load any extensions by default\n",
        "\n",
        "# Display config\n",
        "print(f\"\\nüìù Launch Configuration:\")\n",
        "print(f\"   Hardware: {hw_name}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "print(f\"   Loader: llama.cpp\")\n",
        "print(f\"   API: {'Enabled' if enable_api else 'Disabled'}\")\n",
        "print(f\"   Verbose: {'Yes' if enable_verbose else 'No'}\")\n",
        "print(f\"   Extensions: Disabled (for stability)\")\n",
        "print(f\"   Chats save to: {USER_DATA_DIR / 'logs'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚è≥ Starting server...\")\n",
        "print(\"üîó WATCH FOR GRADIO LINK BELOW ‚¨áÔ∏è\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# --- STEP 15: Launch Server with Error Recovery ---\n",
        "try:\n",
        "    # Change to repo directory\n",
        "    os.chdir(REPO_DIR)\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    found_link = False\n",
        "    error_count = 0\n",
        "    max_errors = 50  # Continue even with many errors\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "        # Count errors but don't stop\n",
        "        if 'error' in line.lower() and 'deprecation' not in line.lower():\n",
        "            error_count += 1\n",
        "            if error_count <= 5:  # Only show first few\n",
        "                print(f\"   [Error #{error_count} - continuing...]\")\n",
        "\n",
        "        # Highlight Gradio link\n",
        "        if ('gradio.live' in line.lower() or\n",
        "            ('running on' in line.lower() and 'http' in line)):\n",
        "            if not found_link:\n",
        "                print(\"\\n\" + \"üéâ\"*35)\n",
        "                print(\"‚úÖ SERVER IS RUNNING! CLICK THE LINK ABOVE ‚òùÔ∏è\")\n",
        "                print(\"üéâ\"*35)\n",
        "                print(f\"\\n‚ö†Ô∏è  Note: {error_count} errors occurred but server started anyway\")\n",
        "                print(\"\\nüí° USAGE TIPS:\")\n",
        "                print(\"   ‚Ä¢ Click the Gradio link above\")\n",
        "                print(\"   ‚Ä¢ Select Chat mode in the web UI\")\n",
        "                print(\"   ‚Ä¢ Conversations auto-save to Google Drive\")\n",
        "                print(\"   ‚Ä¢ Press Ctrl+C here to stop server\")\n",
        "                print(\"=\"*70 + \"\\n\")\n",
        "                found_link = True\n",
        "\n",
        "    return_code = process.wait()\n",
        "\n",
        "    if return_code != 0 and not found_link:\n",
        "        print(\"\\n‚ö†Ô∏è  Server exited (this might be OK if you stopped it)\")\n",
        "        print(f\"   Total errors encountered: {error_count}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n‚èπÔ∏è  Server stopped by user\")\n",
        "    print(f\"üíæ All chats saved in: {USER_DATA_DIR / 'logs'}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\n‚ö†Ô∏è  Launch issue: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(f\"   1. Check if model exists: {MODEL_FILE.exists() if MODEL_FILE.exists() else 'Unknown'}\")\n",
        "    print(f\"   2. Try running manually:\")\n",
        "    print(f\"      cd {REPO_DIR}\")\n",
        "    print(f\"      python server.py --cpu --model {MODEL_FILE} --listen --share\")\n",
        "    print(\"\\n   3. Check the command that was attempted:\")\n",
        "    print(f\"      {' '.join(str(x) for x in cmd)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Installation/Launch attempt complete!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}