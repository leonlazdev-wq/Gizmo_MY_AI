{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# MY-AI-Gizmo Complete Setup Script for Google Colab\n",
        "# Repository: https://github.com/gitleon8301/MY-AI-Gizmo-working\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "print(\"üöÄ MY-AI-GIZMO INSTALLER FOR GOOGLE COLAB\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --- STEP 1: Clean and Clone ---\n",
        "print(\"\\nüì• Step 1: Cloning repository...\")\n",
        "os.chdir('/content')\n",
        "\n",
        "if Path(\"MY-AI-Gizmo-working\").exists():\n",
        "    print(\"   Removing existing directory...\")\n",
        "    shutil.rmtree(\"MY-AI-Gizmo-working\")\n",
        "\n",
        "subprocess.run([\n",
        "    \"git\", \"clone\",\n",
        "    \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"\n",
        "], check=True)\n",
        "\n",
        "os.chdir(\"MY-AI-Gizmo-working\")\n",
        "print(\"   ‚úÖ Repository cloned\")\n",
        "\n",
        "# --- STEP 2: Environment Setup ---\n",
        "print(\"\\nüîß Step 2: Setting up environment...\")\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "print(\"   ‚úÖ Environment configured\")\n",
        "\n",
        "# --- STEP 3: Explore Repository Structure ---\n",
        "print(\"\\nüîç Step 3: Analyzing repository structure...\")\n",
        "\n",
        "# Check what files exist\n",
        "key_files = {\n",
        "    \"requirements.txt\": Path(\"requirements.txt\"),\n",
        "    \"requirements_cpu_only.txt\": Path(\"requirements_cpu_only.txt\"),\n",
        "    \"requirements_cpu_only_windows.txt\": Path(\"requirements_cpu_only_windows.txt\"),\n",
        "    \"server.py\": Path(\"server.py\"),\n",
        "    \"download-model.py\": Path(\"download-model.py\"),\n",
        "    \"start_linux.sh\": Path(\"start_linux.sh\"),\n",
        "    \"start_windows.bat\": Path(\"start_windows.bat\"),\n",
        "    \"installer_files/requirements.txt\": Path(\"installer_files/requirements.txt\"),\n",
        "}\n",
        "\n",
        "print(\"\\n   Files found:\")\n",
        "for name, path in key_files.items():\n",
        "    if path.exists():\n",
        "        print(f\"   ‚úÖ {name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {name}\")\n",
        "\n",
        "# --- STEP 4: Install Dependencies ---\n",
        "print(\"\\nüì¶ Step 4: Installing dependencies...\")\n",
        "\n",
        "# Upgrade pip first\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n",
        "               check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Try to find and use the correct requirements file\n",
        "req_priority = [\n",
        "    \"requirements_cpu_only.txt\",\n",
        "    \"requirements.txt\",\n",
        "    \"installer_files/requirements.txt\"\n",
        "]\n",
        "\n",
        "requirements_installed = False\n",
        "for req_file in req_priority:\n",
        "    if Path(req_file).exists():\n",
        "        print(f\"   Installing from {req_file}...\")\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                \"-r\", req_file, \"--quiet\"\n",
        "            ], check=True, timeout=300)\n",
        "            print(f\"   ‚úÖ Installed from {req_file}\")\n",
        "            requirements_installed = True\n",
        "            break\n",
        "        except (subprocess.CalledProcessError, subprocess.TimeoutExpired) as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Failed with {req_file}, trying next...\")\n",
        "            continue\n",
        "\n",
        "# Fallback: Install essential packages manually\n",
        "if not requirements_installed:\n",
        "    print(\"   üì¶ Installing core packages manually...\")\n",
        "    core_packages = [\n",
        "        \"torch\",\n",
        "        \"torchvision\",\n",
        "        \"torchaudio\",\n",
        "        \"transformers>=4.30.0\",\n",
        "        \"gradio>=3.50.0\",\n",
        "        \"accelerate\",\n",
        "        \"peft\",\n",
        "        \"markdown\",\n",
        "        \"numpy\",\n",
        "        \"Pillow>=9.5.0\",\n",
        "        \"pyyaml\",\n",
        "        \"requests\",\n",
        "        \"safetensors>=0.3.1\",\n",
        "        \"sentencepiece\",\n",
        "        \"tqdm\",\n",
        "        \"protobuf\",\n",
        "        \"llama-cpp-python\"\n",
        "    ]\n",
        "\n",
        "    for package in core_packages:\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                package, \"--quiet\"\n",
        "            ], check=True, timeout=60)\n",
        "            print(f\"   ‚úÖ {package}\")\n",
        "        except:\n",
        "            print(f\"   ‚ö†Ô∏è  {package} (skipped)\")\n",
        "\n",
        "# --- STEP 5: Fix Gradio Theme Issue ---\n",
        "print(\"\\nüîß Step 5: Patching UI compatibility...\")\n",
        "\n",
        "ui_file = Path(\"modules/ui.py\")\n",
        "if ui_file.exists():\n",
        "    try:\n",
        "        with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Check if the problematic code exists\n",
        "        if 'button_shadow_hover' in content or 'theme.set(' in content:\n",
        "            # Create a backup\n",
        "            shutil.copy(ui_file, ui_file.with_suffix('.py.bak'))\n",
        "\n",
        "            # Replace the theme configuration\n",
        "            content = content.replace(\n",
        "                'if not shared.args.old_colors:',\n",
        "                'if False:  # Theme disabled for Colab - was: if not shared.args.old_colors:'\n",
        "            )\n",
        "\n",
        "            with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(content)\n",
        "\n",
        "            print(\"   ‚úÖ UI theme patched\")\n",
        "        else:\n",
        "            print(\"   ‚ÑπÔ∏è  No theme issues detected\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Could not patch UI: {e}\")\n",
        "else:\n",
        "    print(\"   ‚ÑπÔ∏è  No UI file found to patch\")\n",
        "\n",
        "# --- STEP 6: Setup Models Directory ---\n",
        "print(\"\\nüìÅ Step 6: Creating models directory...\")\n",
        "\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "print(f\"   ‚úÖ Models directory: {models_dir.absolute()}\")\n",
        "\n",
        "# --- STEP 7: Download Model ---\n",
        "print(\"\\n‚¨áÔ∏è  Step 7: Downloading model...\")\n",
        "print(\"   This may take 3-5 minutes...\")\n",
        "\n",
        "model_name = \"llama-2-7b.Q4_K_M.gguf\"\n",
        "model_path = models_dir / model_name\n",
        "\n",
        "if model_path.exists():\n",
        "    print(f\"   ‚ÑπÔ∏è  Model already exists: {model_path}\")\n",
        "else:\n",
        "    # Try the download script first\n",
        "    if Path(\"download-model.py\").exists():\n",
        "        print(\"   Trying download-model.py...\")\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                sys.executable, \"download-model.py\",\n",
        "                \"TheBloke/Llama-2-7B-GGUF\"\n",
        "            ], check=True, timeout=600)\n",
        "            print(\"   ‚úÖ Model downloaded via script\")\n",
        "        except:\n",
        "            print(\"   ‚ö†Ô∏è  Script failed, trying direct download...\")\n",
        "\n",
        "    # Fallback: Direct download\n",
        "    if not model_path.exists():\n",
        "        print(\"   Direct download from Hugging Face...\")\n",
        "        model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                \"wget\", \"-q\", \"--show-progress\",\n",
        "                \"-O\", str(model_path),\n",
        "                model_url\n",
        "            ], check=True, timeout=600)\n",
        "            print(f\"   ‚úÖ Model downloaded: {model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Download failed: {e}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "# Verify model exists\n",
        "if not model_path.exists():\n",
        "    # Try to find any GGUF file\n",
        "    gguf_files = list(Path(\".\").rglob(\"*.gguf\"))\n",
        "    if gguf_files:\n",
        "        model_path = gguf_files[0]\n",
        "        print(f\"   ‚úÖ Found model: {model_path}\")\n",
        "    else:\n",
        "        print(\"   ‚ùå No model file found!\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# --- STEP 8: Launch Configuration ---\n",
        "print(\"\\n‚öôÔ∏è  Step 8: Preparing launch...\")\n",
        "\n",
        "# Determine the correct launch script\n",
        "if Path(\"server.py\").exists():\n",
        "    launch_cmd = [sys.executable, \"server.py\"]\n",
        "    print(\"   Using: server.py\")\n",
        "elif Path(\"start_linux.sh\").exists():\n",
        "    subprocess.run([\"chmod\", \"+x\", \"start_linux.sh\"], check=False)\n",
        "    launch_cmd = [\"bash\", \"start_linux.sh\"]\n",
        "    print(\"   Using: start_linux.sh\")\n",
        "else:\n",
        "    print(\"   ‚ùå No launch script found!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Add arguments\n",
        "launch_args = [\n",
        "    \"--cpu\",  # Force CPU mode\n",
        "    \"--listen\",  # Listen on all interfaces\n",
        "    \"--share\",  # Create public Gradio link\n",
        "    \"--model\", str(model_path),  # Model file\n",
        "    \"--chat\",  # Enable chat mode\n",
        "    \"--verbose\"  # Verbose output\n",
        "]\n",
        "\n",
        "full_command = launch_cmd + launch_args\n",
        "\n",
        "# --- STEP 9: Launch Server ---\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üöÄ LAUNCHING MY-AI-GIZMO SERVER\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {model_path}\")\n",
        "print(f\"Mode: CPU-only\")\n",
        "print(f\"Interface: Gradio Web UI\")\n",
        "print(f\"\\nCommand: {' '.join(full_command)}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚è≥ Starting server (this may take 1-2 minutes)...\")\n",
        "print(\"üîó Look for the Gradio link below (https://xxxxx.gradio.live)\\n\")\n",
        "\n",
        "try:\n",
        "    subprocess.run(full_command, check=True)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n‚èπÔ∏è  Server stopped by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\\n‚ùå Server error: {e}\")\n",
        "    print(\"\\nüîç Debugging information:\")\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "    print(f\"Python: {sys.executable}\")\n",
        "    print(f\"Model exists: {model_path.exists()}\")\n",
        "\n",
        "    # Show recent files\n",
        "    print(\"\\nüìÇ Recent files in directory:\")\n",
        "    subprocess.run([\"ls\", \"-lah\"], check=False)"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}