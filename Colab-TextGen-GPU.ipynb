{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MY-AI-GIZMO - NO UI MODIFICATIONS (CLEAN INSTALL)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸš€ MY-AI-GIZMO - CLEAN INSTALLER (NO UI MODS)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# HELPERS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def run_cmd(cmd, check=False, quiet=True, timeout=None):\n",
        "    try:\n",
        "        if quiet:\n",
        "            return subprocess.run(cmd, check=check, capture_output=True, text=True, timeout=timeout)\n",
        "        else:\n",
        "            return subprocess.run(cmd, check=check, timeout=timeout)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def extract_urls(text):\n",
        "    urls = {}\n",
        "    public_match = re.search(r'https://[a-z0-9]+\\.gradio\\.live', text, re.IGNORECASE)\n",
        "    if public_match:\n",
        "        urls['public'] = public_match.group(0)\n",
        "    local_match = re.search(r'http://127\\.0\\.0\\.1:\\d+', text)\n",
        "    if local_match:\n",
        "        urls['local'] = local_match.group(0)\n",
        "    return urls\n",
        "\n",
        "def print_url_box(local_url, public_url, hw):\n",
        "    box = \"=\"*70\n",
        "    print(\"\\n\" + box)\n",
        "    print(\"ğŸ‰\"*35)\n",
        "    print(box)\n",
        "    print(\"âœ… SERVER IS RUNNING!\")\n",
        "    print(box)\n",
        "\n",
        "    if public_url:\n",
        "        print(f\"\\nğŸŒ PUBLIC URL (CLICK THIS):\\n\")\n",
        "        print(f\"   {public_url}\\n\")\n",
        "\n",
        "    if local_url:\n",
        "        print(f\"ğŸ  LOCAL URL:\\n\")\n",
        "        print(f\"   {local_url}\\n\")\n",
        "\n",
        "    print(box)\n",
        "    print(f\"ğŸ’¾ Chats saved to Google Drive\")\n",
        "    print(f\"ğŸ–¥ï¸  Hardware: {hw}\")\n",
        "    print(f\"â¹ï¸  Press Ctrl+C to stop\")\n",
        "    print(box + \"\\n\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PATHS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 1: MOUNT DRIVE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“‚ Step 1: Mounting Google Drive...\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    mydrive = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "    if mydrive.exists() and mydrive.is_dir():\n",
        "        print(\"   âœ… Already mounted\")\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"   âœ… Mounted\")\n",
        "\n",
        "    if not mydrive.exists():\n",
        "        raise Exception(\"Mount failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   âŒ Error: {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 2-3: SYSTEM + REPO\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“¦ Step 2: System packages...\")\n",
        "run_cmd([\"apt-get\", \"update\", \"-qq\"])\n",
        "run_cmd([\"apt-get\", \"install\", \"-y\", \"-qq\", \"build-essential\", \"cmake\", \"git\", \"wget\"])\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "print(\"\\nğŸ“¥ Step 3: Repository...\")\n",
        "\n",
        "# Clean install if UI was previously broken\n",
        "if (REPO_DIR / \"modules\" / \"ui.py\").exists():\n",
        "    with open(REPO_DIR / \"modules\" / \"ui.py\", 'r', encoding='utf-8') as f:\n",
        "        if '# COLAB_FIX' in f.read():\n",
        "            print(\"   ğŸ”„ Removing old broken UI, fresh clone needed...\")\n",
        "            if REPO_DIR.exists():\n",
        "                shutil.rmtree(REPO_DIR)\n",
        "\n",
        "if (REPO_DIR / \"server.py\").exists():\n",
        "    print(\"   âœ… Already exists (clean)\")\n",
        "else:\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    os.chdir(REPO_DIR.parent)\n",
        "    run_cmd([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"])\n",
        "    print(\"   âœ… Cloned fresh\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 4: PACKAGES - FAST WITH PRE-BUILT WHEELS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ Step 4: Installing packages...\")\n",
        "print(\"   (Using pre-built wheels - fast!)\")\n",
        "\n",
        "# Core\n",
        "print(\"   ğŸ“¦ Core packages...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "               \"setuptools\", \"wheel\", \"numpy\", \"requests\", \"tqdm\", \"pyyaml\"],\n",
        "              check=False, timeout=120)\n",
        "\n",
        "# llama-cpp-python PRE-BUILT (CPU)\n",
        "print(\"   ğŸ“¦ llama-cpp-python (pre-built, ~30s)...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\",\n",
        "               \"llama-cpp-python[server]\",\n",
        "               \"--extra-index-url\", \"https://abetlen.github.io/llama-cpp-python/whl/cpu\"],\n",
        "              check=False, timeout=180)\n",
        "\n",
        "# Main packages\n",
        "print(\"   ğŸ“¦ AI packages...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "               \"torch\", \"transformers\", \"gradio>=3.50.0\", \"accelerate\",\n",
        "               \"markdown\", \"Pillow\", \"safetensors\", \"sentencepiece\", \"protobuf\"],\n",
        "              check=False, timeout=300)\n",
        "\n",
        "# Extensions\n",
        "print(\"   ğŸ“¦ Extension packages...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "               \"flask_cloudflared\"],\n",
        "              check=False, timeout=60)\n",
        "\n",
        "print(\"   âœ… All packages installed!\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 5: NO UI MODIFICATIONS - LEAVE IT CLEAN!\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nâœ… Step 5: Skipping UI modifications (using GitHub version as-is)\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 6: DATA SETUP\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”— Step 6: Linking user data...\")\n",
        "local_data = REPO_DIR / \"user_data\"\n",
        "if local_data.exists() and not local_data.is_symlink():\n",
        "    for item in local_data.rglob(\"*\"):\n",
        "        if item.is_file():\n",
        "            rel = item.relative_to(local_data)\n",
        "            dest = USER_DATA_DIR / rel\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(item, dest)\n",
        "    shutil.rmtree(local_data)\n",
        "if not local_data.exists():\n",
        "    local_data.symlink_to(USER_DATA_DIR)\n",
        "for sub in [\"logs\",\"logs/chat\",\"logs/instruct\",\"presets\",\"characters\"]:\n",
        "    (USER_DATA_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "print(\"\\nâ¬‡ï¸  Step 7: Model check...\")\n",
        "if MODEL_FILE.exists():\n",
        "    print(f\"   âœ… Found ({MODEL_FILE.stat().st_size/(1024**3):.2f} GB)\")\n",
        "else:\n",
        "    print(\"   ğŸ“¥ Downloading model...\")\n",
        "    MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "    subprocess.run([\"wget\",\"-q\",\"--show-progress\",\"-O\",str(MODEL_FILE),\n",
        "                   \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"],\n",
        "                  timeout=900)\n",
        "    print(\"   âœ… Downloaded\")\n",
        "\n",
        "print(\"\\nğŸ”— Step 8: Linking models...\")\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "print(\"\\nğŸ–¥ï¸  Step 9: Hardware check...\")\n",
        "has_gpu = False\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    if has_gpu:\n",
        "        print(f\"   âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"   â„¹ï¸  CPU mode\")\n",
        "except:\n",
        "    print(\"   â„¹ï¸  CPU mode\")\n",
        "\n",
        "hw = \"GPU\" if has_gpu else \"CPU\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 10: LAUNCH - ADD --old-colors FLAG TO USE LEGACY GRADIO THEME\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸš€ Step 10: Launching server...\")\n",
        "print(f\"   Hardware: {hw}\")\n",
        "print(f\"   Model: {MODEL_FILE.name}\")\n",
        "\n",
        "cmd = [sys.executable, \"server.py\"]\n",
        "if has_gpu:\n",
        "    cmd.extend([\"--gpu-layers\", \"35\"])\n",
        "cmd.extend([\"--cpu\", \"--threads\",\"4\",\"--listen\",\"--listen-host\",\"0.0.0.0\",\n",
        "            \"--share\",\"--model\",str(MODEL_FILE),\"--loader\",\"llama.cpp\",\n",
        "            \"--old-colors\"])  # Use old Gradio theme - avoids the theme code issues!\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸŒ STARTING GRADIO SERVER\")\n",
        "print(\"=\"*70)\n",
        "print(\"Using --old-colors flag to avoid theme issues\")\n",
        "print(\"Watching for URLs...\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "found_urls = {'local': None, 'public': None}\n",
        "url_box_printed = False\n",
        "\n",
        "try:\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                              universal_newlines=True, bufsize=1)\n",
        "\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "\n",
        "        line_urls = extract_urls(line)\n",
        "\n",
        "        if 'local' in line_urls and not found_urls['local']:\n",
        "            found_urls['local'] = line_urls['local']\n",
        "            print(f\"\\nâœ… LOCAL URL: {found_urls['local']}\")\n",
        "\n",
        "        if 'public' in line_urls and not found_urls['public']:\n",
        "            found_urls['public'] = line_urls['public']\n",
        "            print(f\"\\nğŸŒ PUBLIC URL: {found_urls['public']}\")\n",
        "\n",
        "        if found_urls['public'] and not url_box_printed:\n",
        "            url_box_printed = True\n",
        "            print_url_box(found_urls['local'], found_urls['public'], hw)\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"â¹ï¸  Server stopped\")\n",
        "    if found_urls['public']:\n",
        "        print(f\"ğŸ“ Public URL: {found_urls['public']}\")\n",
        "    if found_urls['local']:\n",
        "        print(f\"ğŸ“ Local URL: {found_urls['local']}\")\n",
        "    print(f\"ğŸ’¾ Data: {USER_DATA_DIR}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ ERROR: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nâœ… Complete!\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}