{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitleon8301/MY-AI-Gizmo-working/blob/main/Colab-TextGen-GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# oobabooga/text-generation-webui\n",
        "\n",
        "After running both cells, a public gradio URL will appear at the bottom in around 10 minutes. You can optionally generate an API link.\n",
        "\n",
        "* Project page: https://github.com/oobabooga/text-generation-webui\n",
        "* Gradio server status: https://status.gradio.app/"
      ],
      "metadata": {
        "id": "MFQl6-FjSYtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown Press play on the music player that will appear below:\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "f7TVVj_z4flw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MY-AI-GIZMO AGGRESSIVE URL CAPTURE - GUARANTEED LINK\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸš€ MY-AI-GIZMO - GUARANTEED GRADIO LINK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# HELPERS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def run_cmd(cmd, check=False, quiet=True, timeout=None):\n",
        "    try:\n",
        "        if quiet:\n",
        "            return subprocess.run(cmd, check=check, capture_output=True, text=True, timeout=timeout)\n",
        "        else:\n",
        "            return subprocess.run(cmd, check=check, timeout=timeout)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def safe_pip(packages):\n",
        "    if isinstance(packages, str):\n",
        "        packages = [packages]\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n",
        "                         check=False, timeout=300, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def extract_urls(text):\n",
        "    \"\"\"Extract both local and public URLs\"\"\"\n",
        "    urls = {}\n",
        "\n",
        "    # Pattern for Gradio public URLs\n",
        "    public_match = re.search(r'https://[a-z0-9]+\\.gradio\\.live', text, re.IGNORECASE)\n",
        "    if public_match:\n",
        "        urls['public'] = public_match.group(0)\n",
        "\n",
        "    # Pattern for local URLs\n",
        "    local_patterns = [\n",
        "        r'http://127\\.0\\.0\\.1:\\d+',\n",
        "        r'http://localhost:\\d+',\n",
        "        r'Running on local URL:\\s+(http://[^\\s]+)',\n",
        "        r'Running on public URL:\\s+(https://[^\\s]+)'\n",
        "    ]\n",
        "\n",
        "    for pattern in local_patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            if 'local URL' in pattern or '127.0.0.1' in pattern or 'localhost' in pattern:\n",
        "                urls['local'] = match.group(1) if match.lastindex else match.group(0)\n",
        "            elif 'public URL' in pattern:\n",
        "                urls['public'] = match.group(1) if match.lastindex else match.group(0)\n",
        "\n",
        "    return urls\n",
        "\n",
        "def print_url_box(local_url, public_url, hw):\n",
        "    box = \"=\"*70\n",
        "    print(\"\\n\" + box)\n",
        "    print(\"ğŸ‰\"*35)\n",
        "    print(box)\n",
        "    print(\"âœ… SERVER IS RUNNING!\")\n",
        "    print(box)\n",
        "\n",
        "    if public_url:\n",
        "        print(f\"\\nğŸŒ PUBLIC URL (SHARE THIS):\\n\")\n",
        "        print(f\"   {public_url}\\n\")\n",
        "\n",
        "    if local_url:\n",
        "        print(f\"ğŸ  LOCAL URL:\\n\")\n",
        "        print(f\"   {local_url}\\n\")\n",
        "\n",
        "    print(box)\n",
        "    print(f\"ğŸ’¾ All chats save to Google Drive\")\n",
        "    print(f\"ğŸ–¥ï¸  Hardware: {hw}\")\n",
        "    print(f\"â¹ï¸  Press Ctrl+C to stop\")\n",
        "    print(box + \"\\n\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PATHS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive/MY-AI-Gizmo\")\n",
        "REPO_DIR = DRIVE_BASE / \"MY-AI-Gizmo-working\"\n",
        "MODELS_DIR = DRIVE_BASE / \"models\"\n",
        "USER_DATA_DIR = DRIVE_BASE / \"user_data\"\n",
        "MODEL_FILE = MODELS_DIR / \"llama-2-7b.Q4_K_M.gguf\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 1: MOUNT DRIVE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“‚ Step 1: Google Drive...\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    mydrive = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "    if mydrive.exists() and mydrive.is_dir():\n",
        "        print(\"   âœ… Mounted\")\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"   âœ… Mounted\")\n",
        "\n",
        "    if not mydrive.exists():\n",
        "        raise Exception(\"Mount failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   âŒ Error: {e}\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "for d in [DRIVE_BASE, MODELS_DIR, USER_DATA_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 2-3: SYSTEM + REPO\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ“¦ Step 2: System...\")\n",
        "run_cmd([\"apt-get\", \"update\", \"-qq\"])\n",
        "run_cmd([\"apt-get\", \"install\", \"-y\", \"-qq\", \"build-essential\", \"cmake\", \"git\", \"wget\"])\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "print(\"\\nğŸ“¥ Step 3: Repository...\")\n",
        "\n",
        "if (REPO_DIR / \"server.py\").exists():\n",
        "    print(\"   âœ… Exists\")\n",
        "else:\n",
        "    REPO_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
        "    os.chdir(REPO_DIR.parent)\n",
        "    run_cmd([\"git\", \"clone\", \"https://github.com/gitleon8301/MY-AI-Gizmo-working.git\"])\n",
        "    print(\"   âœ… Cloned\")\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 4: PACKAGES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ Step 4: Packages (quick install)...\")\n",
        "\n",
        "safe_pip([\"setuptools\", \"wheel\", \"numpy\", \"requests\", \"tqdm\", \"pyyaml\"])\n",
        "\n",
        "env = dict(os.environ)\n",
        "env[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=off -DLLAMA_BUILD_SERVER=ON\"\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--force-reinstall\", \"--no-cache-dir\", \"-q\",\n",
        "               \"llama-cpp-python[server]\"],\n",
        "              check=False, env=env, timeout=600, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "safe_pip([\"torch\", \"transformers\", \"gradio>=3.50.0\", \"accelerate\", \"markdown\",\n",
        "         \"Pillow\", \"safetensors\", \"sentencepiece\", \"protobuf\", \"flask_cloudflared\"])\n",
        "\n",
        "print(\"   âœ… Done\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 5: FIND LLAMA-SERVER\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ” Step 5: Finding llama-server...\")\n",
        "\n",
        "llama_server_path = None\n",
        "check_paths = [\n",
        "    Path(sys.executable).parent / \"llama-server\",\n",
        "    Path(sys.prefix) / \"bin\" / \"llama-server\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    import llama_cpp\n",
        "    llama_cpp_dir = Path(llama_cpp.__file__).parent\n",
        "    check_paths.insert(0, llama_cpp_dir / \"server\" / \"llama-server\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "for path in check_paths:\n",
        "    if path.exists():\n",
        "        llama_server_path = str(path)\n",
        "        print(f\"   âœ… Found: {llama_server_path}\")\n",
        "        break\n",
        "\n",
        "if not llama_server_path:\n",
        "    result = run_cmd(['which', 'llama-server'])\n",
        "    if result and result.returncode == 0 and result.stdout.strip():\n",
        "        llama_server_path = result.stdout.strip()\n",
        "        print(f\"   âœ… Found: {llama_server_path}\")\n",
        "    else:\n",
        "        llama_server_path = \"llama-server\"\n",
        "        print(f\"   âš ï¸  Using default: {llama_server_path}\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 6-7: FIX UI AND LLAMA\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”§ Step 6: Fixing UI...\")\n",
        "\n",
        "ui_file = REPO_DIR / \"modules\" / \"ui.py\"\n",
        "if ui_file.exists():\n",
        "    with open(ui_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    if '# COLAB_FIX_APPLIED' not in content:\n",
        "        lines = content.split('\\n')\n",
        "        fixed_lines = []\n",
        "        in_theme_block = False\n",
        "        theme_block_indent = 0\n",
        "\n",
        "        for line in lines:\n",
        "            if 'if not shared.args.old_colors:' in line and not in_theme_block:\n",
        "                fixed_lines.append('    # COLAB_FIX_APPLIED')\n",
        "                fixed_lines.append('    # ' + line.strip())\n",
        "                in_theme_block = True\n",
        "                theme_block_indent = len(line) - len(line.lstrip())\n",
        "                continue\n",
        "\n",
        "            if in_theme_block:\n",
        "                current_indent = len(line) - len(line.lstrip()) if line.strip() else 999\n",
        "                if line.strip() and current_indent <= theme_block_indent:\n",
        "                    in_theme_block = False\n",
        "                    fixed_lines.append(line)\n",
        "                else:\n",
        "                    if line.strip():\n",
        "                        fixed_lines.append('    # ' + line.strip())\n",
        "                    else:\n",
        "                        fixed_lines.append(line)\n",
        "                continue\n",
        "\n",
        "            fixed_lines.append(line)\n",
        "\n",
        "        with open(ui_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(fixed_lines))\n",
        "    print(\"   âœ… Fixed\")\n",
        "\n",
        "print(\"\\nğŸ”§ Step 7: Updating llama module...\")\n",
        "\n",
        "LLAMA_CODE = f'''\"\"\"llama.cpp server\"\"\"\n",
        "import json, torch, os, socket, subprocess, sys, threading, time, requests\n",
        "from pathlib import Path\n",
        "from modules import shared\n",
        "from modules.logging_colors import logger\n",
        "\n",
        "def get_llama_server_path():\n",
        "    found = \"{llama_server_path}\"\n",
        "    if os.path.exists(found): return found\n",
        "    try:\n",
        "        import llama_cpp\n",
        "        for p in [Path(llama_cpp.__file__).parent/\"server\"/\"llama-server\", Path(sys.prefix)/\"bin\"/\"llama-server\"]:\n",
        "            if p.exists(): return str(p)\n",
        "        r = subprocess.run(['which','llama-server'], capture_output=True, text=True, timeout=5)\n",
        "        if r.returncode == 0 and r.stdout.strip(): return r.stdout.strip()\n",
        "    except: pass\n",
        "    return \"llama-server\"\n",
        "\n",
        "class LlamaServer:\n",
        "    def __init__(self, model_path, server_path=None):\n",
        "        self.model_path = model_path\n",
        "        self.server_path = server_path or get_llama_server_path()\n",
        "        logger.info(f\"Llama-server: {{self.server_path}}\")\n",
        "        self.port = self._find_port()\n",
        "        self.process = None\n",
        "        self.session = requests.Session()\n",
        "        self.vocabulary_size = None\n",
        "        self.bos_token = \"~~\"\n",
        "        self.last_prompt_token_count = 0\n",
        "        self._start_server()\n",
        "\n",
        "    def encode(self, text, add_bos_token=False, **kwargs):\n",
        "        if self.bos_token and text.startswith(self.bos_token): add_bos_token = False\n",
        "        r = self.session.post(f\"http://127.0.0.1:{{self.port}}/tokenize\", json={{\"content\":text,\"add_special\":add_bos_token}})\n",
        "        return r.json().get(\"tokens\",[])\n",
        "\n",
        "    def decode(self, token_ids, **kwargs):\n",
        "        r = self.session.post(f\"http://127.0.0.1:{{self.port}}/detokenize\", json={{\"tokens\":token_ids}})\n",
        "        return r.json().get(\"content\",\"\")\n",
        "\n",
        "    def prepare_payload(self, state):\n",
        "        temp = state[\"temperature\"]\n",
        "        if state.get(\"dynamic_temperature\"): temp = (state[\"dynatemp_low\"]+state[\"dynatemp_high\"])/2\n",
        "        return {{\"temperature\":temp,\"top_k\":state.get(\"top_k\",40),\"top_p\":state.get(\"top_p\",0.95),\n",
        "                \"min_p\":state.get(\"min_p\",0.05),\"repeat_penalty\":state.get(\"repetition_penalty\",1.1),\"seed\":state.get(\"seed\",-1)}}\n",
        "\n",
        "    def is_multimodal(self): return False\n",
        "\n",
        "    def generate_with_streaming(self, prompt, state):\n",
        "        payload = self.prepare_payload(state)\n",
        "        tokens = self.encode(prompt, add_bos_token=state.get(\"add_bos_token\",False))\n",
        "        self.last_prompt_token_count = len(tokens)\n",
        "        payload[\"prompt\"] = tokens\n",
        "        payload.update({{\"n_predict\":state.get('max_new_tokens',200),\"stream\":True,\"cache_prompt\":True}})\n",
        "        r = self.session.post(f\"http://127.0.0.1:{{self.port}}/completion\", json=payload, stream=True)\n",
        "        full = \"\"\n",
        "        try:\n",
        "            for line in r.iter_lines():\n",
        "                if not line: continue\n",
        "                try:\n",
        "                    line = line.decode('utf-8')\n",
        "                    if line.startswith('data: '): line = line[6:]\n",
        "                    data = json.loads(line)\n",
        "                    if data.get('content'): full += data['content']; yield full\n",
        "                    if data.get('stop'): break\n",
        "                except: continue\n",
        "        finally: r.close()\n",
        "\n",
        "    def generate(self, prompt, state):\n",
        "        out = \"\"\n",
        "        for out in self.generate_with_streaming(prompt, state): pass\n",
        "        return out\n",
        "\n",
        "    def _get_vocabulary_size(self):\n",
        "        try:\n",
        "            r = self.session.get(f\"http://127.0.0.1:{{self.port}}/v1/models\").json()\n",
        "            if r.get(\"data\") and r[\"data\"]:\n",
        "                meta = r[\"data\"][0].get(\"meta\",{{}})\n",
        "                if \"n_vocab\" in meta: self.vocabulary_size = meta[\"n_vocab\"]\n",
        "        except: pass\n",
        "\n",
        "    def _get_bos_token(self):\n",
        "        try:\n",
        "            r = self.session.get(f\"http://127.0.0.1:{{self.port}}/props\").json()\n",
        "            if \"bos_token\" in r: self.bos_token = r[\"bos_token\"]\n",
        "        except: pass\n",
        "\n",
        "    def _find_port(self):\n",
        "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "            s.bind(('',0))\n",
        "            return s.getsockname()[1]\n",
        "\n",
        "    def _start_server(self):\n",
        "        has_gpu = torch.cuda.is_available()\n",
        "        gpu_layers = getattr(shared.args,'gpu_layers',0) if has_gpu else 0\n",
        "        if has_gpu: logger.info(f\"GPU: {{torch.cuda.get_device_name(0)}}\")\n",
        "        else: logger.info(\"CPU mode\")\n",
        "        cmd = [self.server_path,\"--model\",self.model_path,\"--ctx-size\",str(getattr(shared.args,'ctx_size',2048)),\n",
        "               \"--batch-size\",str(getattr(shared.args,'batch_size',512)),\"--port\",str(self.port),\"--no-webui\"]\n",
        "        if has_gpu and gpu_layers > 0: cmd += [\"--gpu-layers\",str(gpu_layers)]\n",
        "        threads = getattr(shared.args,'threads',0)\n",
        "        if threads > 0: cmd += [\"--threads\",str(threads)]\n",
        "        logger.info(f\"Starting llama.cpp on port {{self.port}}\")\n",
        "        try:\n",
        "            self.process = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE, bufsize=0)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"llama-server not found: {{self.server_path}}\")\n",
        "            raise RuntimeError(f\"llama-server not found: {{self.server_path}}\")\n",
        "        threading.Thread(target=self._log_stderr, daemon=True).start()\n",
        "        health = f\"http://127.0.0.1:{{self.port}}/health\"\n",
        "        for _ in range(60):\n",
        "            if self.process.poll() is not None: raise RuntimeError(f\"Server died: {{self.process.returncode}}\")\n",
        "            try:\n",
        "                if self.session.get(health, timeout=1).status_code == 200: break\n",
        "            except: pass\n",
        "            time.sleep(1)\n",
        "        self._get_vocabulary_size()\n",
        "        self._get_bos_token()\n",
        "        logger.info(\"Ready\")\n",
        "\n",
        "    def _log_stderr(self):\n",
        "        try:\n",
        "            for line in iter(self.process.stderr.readline, b''):\n",
        "                print(line.decode('utf-8',errors='replace').strip(), file=sys.stderr)\n",
        "        except: pass\n",
        "\n",
        "    def stop(self):\n",
        "        if self.process:\n",
        "            self.process.terminate()\n",
        "            try: self.process.wait(timeout=5)\n",
        "            except: self.process.kill()\n",
        "\n",
        "    def __del__(self): self.stop()\n",
        "'''\n",
        "\n",
        "(REPO_DIR / \"modules\" / \"llama_cpp_server.py\").write_text(LLAMA_CODE, encoding='utf-8')\n",
        "print(\"   âœ… Fixed\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEPS 8-10: DATA, MODEL, LINKS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nğŸ”— Step 8: User data...\")\n",
        "local_data = REPO_DIR / \"user_data\"\n",
        "if local_data.exists() and not local_data.is_symlink():\n",
        "    for item in local_data.rglob(\"*\"):\n",
        "        if item.is_file():\n",
        "            rel = item.relative_to(local_data)\n",
        "            dest = USER_DATA_DIR / rel\n",
        "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(item, dest)\n",
        "    shutil.rmtree(local_data)\n",
        "if not local_data.exists():\n",
        "    local_data.symlink_to(USER_DATA_DIR)\n",
        "for sub in [\"logs\",\"logs/chat\",\"logs/instruct\",\"presets\",\"characters\"]:\n",
        "    (USER_DATA_DIR / sub).mkdir(parents=True, exist_ok=True)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "print(\"\\nâ¬‡ï¸  Step 9: Model...\")\n",
        "if MODEL_FILE.exists():\n",
        "    print(f\"   âœ… Exists ({MODEL_FILE.stat().st_size/(1024**3):.2f} GB)\")\n",
        "else:\n",
        "    print(\"   ğŸ“¥ Downloading...\")\n",
        "    MODEL_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "    run_cmd([\"wget\",\"-q\",\"--show-progress\",\"-O\",str(MODEL_FILE),\n",
        "            \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\"],\n",
        "            quiet=False, timeout=900)\n",
        "\n",
        "print(\"\\nğŸ”— Step 10: Models link...\")\n",
        "repo_models = REPO_DIR / \"models\"\n",
        "if not repo_models.is_symlink():\n",
        "    if repo_models.exists():\n",
        "        shutil.rmtree(repo_models)\n",
        "    repo_models.symlink_to(MODELS_DIR)\n",
        "print(\"   âœ… Linked\")\n",
        "\n",
        "print(\"\\nğŸ–¥ï¸  Step 11: Hardware...\")\n",
        "has_gpu = False\n",
        "try:\n",
        "    import torch\n",
        "    has_gpu = torch.cuda.is_available()\n",
        "    print(f\"   {'âœ… GPU: '+torch.cuda.get_device_name(0) if has_gpu else 'â„¹ï¸  CPU'}\")\n",
        "except:\n",
        "    print(\"   â„¹ï¸  CPU\")\n",
        "\n",
        "hw = \"GPU\" if has_gpu else \"CPU\"\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# STEP 12: LAUNCH WITH AGGRESSIVE URL CAPTURE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\nâš™ï¸  Step 12: Launching with URL capture...\")\n",
        "\n",
        "def launch_server(attempt=1, max_attempts=3):\n",
        "    \"\"\"Launch server and capture URLs with retry logic\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸš€ LAUNCH ATTEMPT {attempt}/{max_attempts}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    cmd = [sys.executable, \"server.py\"]\n",
        "    if has_gpu:\n",
        "        cmd.extend([\"--gpu-layers\", \"35\"])\n",
        "    else:\n",
        "        cmd.extend([\"--cpu\"])\n",
        "    cmd.extend([\"--threads\",\"4\",\"--listen\",\"--listen-host\",\"0.0.0.0\",\"--share\",\n",
        "                \"--model\",str(MODEL_FILE),\"--loader\",\"llama.cpp\"])\n",
        "\n",
        "    print(f\"Hardware: {hw} | Model: {MODEL_FILE.name}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    os.chdir(REPO_DIR)\n",
        "\n",
        "    found_urls = {'local': None, 'public': None}\n",
        "    urls_printed = [False]\n",
        "    start_time = time.time()\n",
        "    timeout = 180  # 3 minutes timeout\n",
        "\n",
        "    try:\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                                  universal_newlines=True, bufsize=1)\n",
        "\n",
        "        print(\"ğŸ“¡ Monitoring for Gradio URLs...\\n\")\n",
        "\n",
        "        for line in process.stdout:\n",
        "            # Print the line\n",
        "            print(line, end='')\n",
        "\n",
        "            # Extract URLs from this line\n",
        "            line_urls = extract_urls(line)\n",
        "\n",
        "            # Update our found URLs\n",
        "            if 'local' in line_urls and not found_urls['local']:\n",
        "                found_urls['local'] = line_urls['local']\n",
        "                print(f\"\\nâœ… LOCAL URL DETECTED: {found_urls['local']}\\n\")\n",
        "\n",
        "            if 'public' in line_urls and not found_urls['public']:\n",
        "                found_urls['public'] = line_urls['public']\n",
        "                print(f\"\\nğŸŒ PUBLIC URL DETECTED: {found_urls['public']}\\n\")\n",
        "\n",
        "            # If we have both URLs, print the box\n",
        "            if found_urls['public'] and not urls_printed[0]:\n",
        "                urls_printed[0] = True\n",
        "                print_url_box(found_urls['local'], found_urls['public'], hw)\n",
        "\n",
        "            # Check timeout\n",
        "            if time.time() - start_time > timeout:\n",
        "                print(f\"\\nâš ï¸  Timeout after {timeout}s\")\n",
        "                break\n",
        "\n",
        "        # Keep process running if URLs found\n",
        "        if urls_printed[0]:\n",
        "            print(\"\\nâœ… Server running! URLs displayed above.\")\n",
        "            print(\"Press Ctrl+C to stop...\\n\")\n",
        "            process.wait()\n",
        "        else:\n",
        "            print(f\"\\nâš ï¸  URLs not detected in attempt {attempt}\")\n",
        "            process.terminate()\n",
        "            return found_urls\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nâ¹ï¸  Stopped by user\")\n",
        "        if found_urls['public']:\n",
        "            print(f\"ğŸ“ Public URL was: {found_urls['public']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error in attempt {attempt}: {e}\")\n",
        "        return None\n",
        "\n",
        "    return found_urls\n",
        "\n",
        "# Try to launch\n",
        "result = None\n",
        "for attempt in range(1, 4):  # Try up to 3 times\n",
        "    result = launch_server(attempt, max_attempts=3)\n",
        "\n",
        "    if result and result.get('public'):\n",
        "        # Success! We got the public URL\n",
        "        break\n",
        "    elif attempt < 3:\n",
        "        print(f\"\\nğŸ”„ Retrying in 5 seconds...\")\n",
        "        time.sleep(5)\n",
        "    else:\n",
        "        print(f\"\\nâŒ Failed to get public URL after {attempt} attempts\")\n",
        "        print(\"Check the output above for errors\")\n",
        "\n",
        "print(\"\\nâœ… Done\")"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}